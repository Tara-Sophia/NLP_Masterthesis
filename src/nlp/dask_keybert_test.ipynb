{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-21 20:47:48.971649: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "\n",
    "from src.nlp.constants import (\n",
    "    MODEL_MLM_DIR,\n",
    "    MODEL_TC_DIR,\n",
    "    MIMIC_FINAL,\n",
    "    MIMIC_PROCESSED_CLEANED_DIR,\n",
    ")\n",
    "import pandas as pd\n",
    "from keybert import KeyBERT\n",
    "from transformers import AutoTokenizer, pipeline\n",
    "from tqdm import tqdm\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "tqdm.pandas()\n",
    "# dont show warnings\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# from dask.distributed import Client\n",
    "\n",
    "# client = Client(n_workers=4)\n",
    "import dask\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# run keybert on dask df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sentencetransformers\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keyword_extraction(x: str, model, nr_candidates: int, top_n: int) -> list[tuple]:\n",
    "    \"\"\"\n",
    "    This function extracts keywords from the input text.\n",
    "    Parameters\n",
    "    ----------\n",
    "    x : str\n",
    "        Input sentence.\n",
    "    model : str\n",
    "        Path to the model to use for keyword extraction\n",
    "    Returns\n",
    "    -------\n",
    "    list[str]\n",
    "        List of keywords. \n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model, model_max_lenght=512)\n",
    "    # Truncate all the text to 512 tokens\n",
    "\n",
    "#model and tokenizer for sentence transformer\n",
    "#force gpu              \n",
    "    hf_model = pipeline(\n",
    "        \"feature-extraction\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "    )\n",
    "    #use gpu for keybert\n",
    "    #device cuda means use gpu\n",
    "    \n",
    "    model = hf_model\n",
    "    device = 6\n",
    "    kw_model = KeyBERT(model=hf_model).to(device)\n",
    "\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        x,\n",
    "        # df[\"transcription\"],\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        use_maxsum=True,\n",
    "        nr_candidates=nr_candidates,\n",
    "        top_n=top_n,\n",
    "        use_mmr=True,\n",
    "        diversity=0.5,\n",
    "    )\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "def keywords_from_TC_model(df: pd.DataFrame, model: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract keywords from the input text using the TC model\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with the medical transcription text to extract keywords from\n",
    "    model : str\n",
    "        Path to the model to use for keyword extraction\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with the keywords and weights extracted from the input text\n",
    "    \"\"\"\n",
    "    #progress apply for dask df on keyword_extraction\n",
    "    \n",
    "    df[\"keywords_outcome_weights_TC\"] = df.progress_apply(\n",
    "        lambda x: keyword_extraction(\n",
    "            x[\"TEXT_final_cleaned\"], model, x[\"nr_candidates\"], x[\"top_n\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df[\"transcription_f_TC\"] = df[\"keywords_outcome_weights_TC\"].apply(\n",
    "        lambda x: [i[0] for i in x]\n",
    "    )\n",
    "    return df\n",
    "\n",
    "\n",
    "def keywords_from_MLM_model(df: pd.DataFrame, model: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Extract keywords from the input text using the MLM model\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with the medical transcription text to extract keywords from\n",
    "    model : str\n",
    "        Path to the model to use for keyword extraction\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with the keywords and weights extracted from the input text\n",
    "    \"\"\"\n",
    "    # use                                   \n",
    "    df[\"keywords_outcome_weights_MLM\"] = df.progress_apply(\n",
    "        lambda x: keyword_extraction(\n",
    "            x[\"TEXT_final_cleaned\"], model, x[\"nr_candidates\"], x[\"top_n\"]\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "\n",
    "    df[\"transcription_f_MLM\"] = df[\"keywords_outcome_weights_MLM\"].apply(\n",
    "        lambda x: [item[0] for item in x]\n",
    "    )\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def save_dataframe(df: pd.DataFrame) -> None:\n",
    "    \"\"\"\n",
    "    Save dataframe to csv file\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        This is the final dataframe with the keywords and weights to save\n",
    "    \"\"\"\n",
    "    df.to_csv(MIMIC_FINAL, index=False)\n",
    "\n",
    "\n",
    "# make df column smaller than 512\n",
    "def small_column_df(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Make the transcription column smaller than 512 tokens\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pd.DataFrame\n",
    "        Dataframe with the medical transcription text to extract keywords from\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with the transcription column smaller than 512 tokens\n",
    "    \"\"\"\n",
    "    df[\"TEXT_final_cleaned\"] = df[\"TEXT_final_cleaned\"].str[:512]\n",
    "    return df\n",
    "\n",
    "\n",
    "# calculate the optimal nr candidates for each individual text\n",
    "def calculate_optimal_candidate_nr(text: str) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the optimal number of candidates for each text to use for\n",
    "    keyword extraction\n",
    "    Parameters\n",
    "    ----------\n",
    "    text : str\n",
    "        Text to extract keywords from\n",
    "    Returns\n",
    "    -------\n",
    "    int\n",
    "        Optimal number of candidates to use for keyword extraction\n",
    "    \"\"\"\n",
    "    text = str(text)\n",
    "    nr_words = len(text.split())\n",
    "    nr_candidates = int(nr_words * 20 / 100)\n",
    "    if nr_candidates > 35:\n",
    "        nr_candidates = 35\n",
    "    return nr_candidates\n",
    "\n",
    "def top_n_keywords(num: int) -> int:\n",
    "        \"\"\"\n",
    "        Calculate the optimal number of keywords to extract from each text\n",
    "        Parameters\n",
    "        ----------\n",
    "        num : int\n",
    "                Number of candidates to use for keyword extraction\n",
    "        Returns\n",
    "        -------\n",
    "        int\n",
    "                Optimal number of keywords to extract from each text\n",
    "        \"\"\"\n",
    "        if num < 10:\n",
    "                num = num\n",
    "        elif num > 10:\n",
    "                num = round(num*0.5)\n",
    "        return num\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf = dd.read_csv(\n",
    "                 '../../data/processed/mimic_iii/diagnoses_noteevents_cleaned.csv',\n",
    "        dtype={\"TEXT_final_cleaned\": \"str\"},\n",
    "        engine=\"python\",\n",
    "        error_bad_lines=False,\n",
    "        warn_bad_lines=False,\n",
    "        encoding=\"utf8\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62391\n"
     ]
    }
   ],
   "source": [
    "print(ddf.shape[0].compute())\n",
    "#number of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>specialty</th>\n",
       "      <th>TEXT_final</th>\n",
       "      <th>TEXT_final_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admission Date:  [**2141-9-18**]              ...</td>\n",
       "      <td>Cardiothoracic &amp; Vascular</td>\n",
       "      <td>:\\nhip pain\\n\\n:\\n24yo woman with hx SLE, CKD(...</td>\n",
       "      <td>hip pain woman hx SLE CKD currently HD PD labi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Date:  [**2187-9-19**]              ...</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>:\\ns/p Motor cycle crash; left sided rib pain\\...</td>\n",
       "      <td>Motor cycle crash left sided rib pain driver h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Admission Date:  [**2190-6-5**]       Discharg...</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Admission Date:  [**2101-4-30**]              ...</td>\n",
       "      <td>Infectious Disease Specialty</td>\n",
       "      <td>:\\nOSH transfer for sepsis\\n\\n:\\n75 y/o M with...</td>\n",
       "      <td>sepsis hx type DM ESRD failed renal tx HD mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admission Date:  [**2146-9-15**]              ...</td>\n",
       "      <td>Cardiothoracic &amp; Vascular</td>\n",
       "      <td>:\\nChest Pain, Abdominal Pain, Nausea/Vomiting...</td>\n",
       "      <td>Chest Pain Abdominal Pain Nausea Vomiting Righ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2141-9-18**]              ...   \n",
       "1  Admission Date:  [**2187-9-19**]              ...   \n",
       "2  Admission Date:  [**2190-6-5**]       Discharg...   \n",
       "3  Admission Date:  [**2101-4-30**]              ...   \n",
       "4  Admission Date:  [**2146-9-15**]              ...   \n",
       "\n",
       "                      specialty  \\\n",
       "0     Cardiothoracic & Vascular   \n",
       "1          Emergency Department   \n",
       "2          Emergency Department   \n",
       "3  Infectious Disease Specialty   \n",
       "4     Cardiothoracic & Vascular   \n",
       "\n",
       "                                          TEXT_final  \\\n",
       "0  :\\nhip pain\\n\\n:\\n24yo woman with hx SLE, CKD(...   \n",
       "1  :\\ns/p Motor cycle crash; left sided rib pain\\...   \n",
       "2                                                NaN   \n",
       "3  :\\nOSH transfer for sepsis\\n\\n:\\n75 y/o M with...   \n",
       "4  :\\nChest Pain, Abdominal Pain, Nausea/Vomiting...   \n",
       "\n",
       "                                  TEXT_final_cleaned  \n",
       "0  hip pain woman hx SLE CKD currently HD PD labi...  \n",
       "1  Motor cycle crash left sided rib pain driver h...  \n",
       "2                                                NaN  \n",
       "3  sepsis hx type DM ESRD failed renal tx HD mont...  \n",
       "4  Chest Pain Abdominal Pain Nausea Vomiting Righ...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print(ddf.shape[0].compute())\n",
    "ddf = ddf.head(100)\n",
    "ddf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf[\"nr_candidates\"] = ddf[\"TEXT_final_cleaned\"].apply(\n",
    "        calculate_optimal_candidate_nr\n",
    "    ) \n",
    "ddf[\"top_n\"] = ddf[\"nr_candidates\"].apply(top_n_keywords)\n",
    "ddf[\"TEXT_final_cleaned\"] = ddf[\"TEXT_final_cleaned\"].astype(str)\n",
    "ddf = small_column_df(ddf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "995a04c5d236431ca29558bfbedba606",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name /Users/tara-sophiatumbraegel/.cache/torch/sentence_transformers/emilyalsentzer_Bio_ClinicalBERT. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at /Users/tara-sophiatumbraegel/.cache/torch/sentence_transformers/emilyalsentzer_Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'SentenceTransformer' object has no attribute 'config'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 10\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m model_base \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39memilyalsentzer/Bio_ClinicalBERT\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m MODEL_TMLM_DIR \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m..\u001b[39m\u001b[39m\"\u001b[39m,\u001b[39m\"\u001b[39m\u001b[39mmodels\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mnlp\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmaskedlanguagemodel\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mmodel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m ddf_tc \u001b[39m=\u001b[39m keywords_from_TC_model(ddf, model_base)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 10\u001b[0m in \u001b[0;36mkeywords_from_TC_model\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mExtract keywords from the input text using the TC model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    Dataframe with the keywords and weights extracted from the input text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#progress apply for dask df on keyword_extraction\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mprogress_apply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: keyword_extraction(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         x[\u001b[39m\"\u001b[39;49m\u001b[39mTEXT_final_cleaned\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, x[\u001b[39m\"\u001b[39;49m\u001b[39mnr_candidates\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtop_n\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtranscription_f_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(df, df_function)(wrapper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/frame.py:8848\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8837\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8839\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   8840\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   8841\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8846\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   8847\u001b[0m )\n\u001b[0;32m-> 8848\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:733\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    731\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 733\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:857\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 857\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    859\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    871\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    872\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    874\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    875\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    876\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    877\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 10\u001b[0m in \u001b[0;36mkeywords_from_TC_model.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mExtract keywords from the input text using the TC model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    Dataframe with the keywords and weights extracted from the input text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m#progress apply for dask df on keyword_extraction\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mprogress_apply(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: keyword_extraction(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         x[\u001b[39m\"\u001b[39;49m\u001b[39mTEXT_final_cleaned\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, x[\u001b[39m\"\u001b[39;49m\u001b[39mnr_candidates\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtop_n\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtranscription_f_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 10\u001b[0m in \u001b[0;36mkeyword_extraction\u001b[0;34m(x, model, nr_candidates, top_n)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m# Truncate all the text to 512 tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#model and tokenizer for sentence transformer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     model \u001b[39m=\u001b[39m SentenceTransformer(model, device\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     hf_model \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39m\"\u001b[39;49m\u001b[39mfeature-extraction\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         model\u001b[39m=\u001b[39;49mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         tokenizer\u001b[39m=\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m#use gpu for keybert\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39m#device cuda means use gpu\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X12sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     model \u001b[39m=\u001b[39m hf_model\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/pipelines/__init__.py:660\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, framework, revision, use_fast, use_auth_token, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    649\u001b[0m model_classes \u001b[39m=\u001b[39m {\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mtf\u001b[39m\u001b[39m\"\u001b[39m], \u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m: targeted_task[\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m]}\n\u001b[1;32m    650\u001b[0m framework, model \u001b[39m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    651\u001b[0m     model,\n\u001b[1;32m    652\u001b[0m     model_classes\u001b[39m=\u001b[39mmodel_classes,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    657\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    658\u001b[0m )\n\u001b[0;32m--> 660\u001b[0m model_config \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mconfig\n\u001b[1;32m    662\u001b[0m load_tokenizer \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m TOKENIZER_MAPPING \u001b[39mor\u001b[39;00m model_config\u001b[39m.\u001b[39mtokenizer_class \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    663\u001b[0m load_feature_extractor \u001b[39m=\u001b[39m \u001b[39mtype\u001b[39m(model_config) \u001b[39min\u001b[39;00m FEATURE_EXTRACTOR_MAPPING \u001b[39mor\u001b[39;00m feature_extractor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1207\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1205\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m modules:\n\u001b[1;32m   1206\u001b[0m         \u001b[39mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1207\u001b[0m \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m object has no attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1208\u001b[0m     \u001b[39mtype\u001b[39m(\u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, name))\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'SentenceTransformer' object has no attribute 'config'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "#use gpu\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "model_base = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "MODEL_TMLM_DIR = os.path.join(\"..\", \"..\",\"models\", \"nlp\", \"maskedlanguagemodel\", \"model\")\n",
    "ddf_tc = keywords_from_TC_model(ddf, model_base)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in first function\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f9a352bdd3b4330bcdacdd8f4975993",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in second function\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 11\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m ddf_tc \u001b[39m=\u001b[39m keywords_from_TC_model(ddf, MODEL_TC_DIR)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 11\u001b[0m in \u001b[0;36mkeywords_from_TC_model\u001b[0;34m(df, model)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mExtract keywords from the input text using the TC model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    Dataframe with the keywords and weights extracted from the input text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min first function\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mprogress_apply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mlambda\u001b[39;49;00m x: keyword_extraction(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         x[\u001b[39m\"\u001b[39;49m\u001b[39mTEXT_final_cleaned\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, x[\u001b[39m\"\u001b[39;49m\u001b[39mnr_candidates\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtop_n\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtranscription_f_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/std.py:814\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner\u001b[0;34m(df, func, *args, **kwargs)\u001b[0m\n\u001b[1;32m    811\u001b[0m \u001b[39m# Apply the provided function (in **kwargs)\u001b[39;00m\n\u001b[1;32m    812\u001b[0m \u001b[39m# on the df using our wrapper (which provides bar updating)\u001b[39;00m\n\u001b[1;32m    813\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 814\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(df, df_function)(wrapper, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    815\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    816\u001b[0m     t\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/frame.py:8848\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8837\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8839\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   8840\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   8841\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8846\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   8847\u001b[0m )\n\u001b[0;32m-> 8848\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:733\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    731\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 733\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:857\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 857\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    859\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    871\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    872\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    874\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    875\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    876\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    877\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/std.py:809\u001b[0m, in \u001b[0;36mtqdm.pandas.<locals>.inner_generator.<locals>.inner.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    803\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapper\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m    804\u001b[0m     \u001b[39m# update tbar correctly\u001b[39;00m\n\u001b[1;32m    805\u001b[0m     \u001b[39m# it seems `pandas apply` calls `func` twice\u001b[39;00m\n\u001b[1;32m    806\u001b[0m     \u001b[39m# on the first column/row to decide whether it can\u001b[39;00m\n\u001b[1;32m    807\u001b[0m     \u001b[39m# take a fast or slow code path; so stop when t.total==t.n\u001b[39;00m\n\u001b[1;32m    808\u001b[0m     t\u001b[39m.\u001b[39mupdate(n\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m t\u001b[39m.\u001b[39mtotal \u001b[39mor\u001b[39;00m t\u001b[39m.\u001b[39mn \u001b[39m<\u001b[39m t\u001b[39m.\u001b[39mtotal \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m)\n\u001b[0;32m--> 809\u001b[0m     \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 11\u001b[0m in \u001b[0;36mkeywords_from_TC_model.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mExtract keywords from the input text using the TC model\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    Dataframe with the keywords and weights extracted from the input text\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min first function\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mprogress_apply(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: keyword_extraction(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         x[\u001b[39m\"\u001b[39;49m\u001b[39mTEXT_final_cleaned\u001b[39;49m\u001b[39m\"\u001b[39;49m], model, x[\u001b[39m\"\u001b[39;49m\u001b[39mnr_candidates\u001b[39;49m\u001b[39m\"\u001b[39;49m], x[\u001b[39m\"\u001b[39;49m\u001b[39mtop_n\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     ),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m df[\u001b[39m\"\u001b[39m\u001b[39mtranscription_f_TC\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights_TC\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39mlambda\u001b[39;00m x: [i[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m x]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m \u001b[39mreturn\u001b[39;00m df\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb Cell 11\u001b[0m in \u001b[0;36mkeyword_extraction\u001b[0;34m(x, model, nr_candidates, top_n)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mThis function extracts keywords from the input text.\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m    List of keywords.\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min second function\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m tokenizer \u001b[39m=\u001b[39m AutoTokenizer\u001b[39m.\u001b[39;49mfrom_pretrained(model, model_max_lenght\u001b[39m=\u001b[39;49m\u001b[39m512\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m# Truncate all the text to 512 tokens\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m hf_model \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeature-extraction\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     model\u001b[39m=\u001b[39mmodel,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/nlp/dask_keybert_test.ipynb#X15sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:534\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[1;32m    531\u001b[0m     \u001b[39mreturn\u001b[39;00m tokenizer_class\u001b[39m.\u001b[39mfrom_pretrained(pretrained_model_name_or_path, \u001b[39m*\u001b[39minputs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    533\u001b[0m \u001b[39m# Next, let's try to use the tokenizer_config file to get the tokenizer class.\u001b[39;00m\n\u001b[0;32m--> 534\u001b[0m tokenizer_config \u001b[39m=\u001b[39m get_tokenizer_config(pretrained_model_name_or_path, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    535\u001b[0m config_tokenizer_class \u001b[39m=\u001b[39m tokenizer_config\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mtokenizer_class\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    536\u001b[0m tokenizer_auto_map \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/auto/tokenization_auto.py:392\u001b[0m, in \u001b[0;36mget_tokenizer_config\u001b[0;34m(pretrained_model_name_or_path, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only, **kwargs)\u001b[0m\n\u001b[1;32m    323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_tokenizer_config\u001b[39m(\n\u001b[1;32m    324\u001b[0m     pretrained_model_name_or_path: Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike],\n\u001b[1;32m    325\u001b[0m     cache_dir: Optional[Union[\u001b[39mstr\u001b[39m, os\u001b[39m.\u001b[39mPathLike]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    332\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m    333\u001b[0m ):\n\u001b[1;32m    334\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    335\u001b[0m \u001b[39m    Loads the tokenizer configuration from a pretrained model tokenizer configuration.\u001b[39;00m\n\u001b[1;32m    336\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[39m    tokenizer_config = get_tokenizer_config(\"tokenizer-test\")\u001b[39;00m\n\u001b[1;32m    391\u001b[0m \u001b[39m    ```\"\"\"\u001b[39;00m\n\u001b[0;32m--> 392\u001b[0m     resolved_config_file \u001b[39m=\u001b[39m get_file_from_repo(\n\u001b[1;32m    393\u001b[0m         pretrained_model_name_or_path,\n\u001b[1;32m    394\u001b[0m         TOKENIZER_CONFIG_FILE,\n\u001b[1;32m    395\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    396\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    397\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    398\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    399\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    400\u001b[0m         revision\u001b[39m=\u001b[39;49mrevision,\n\u001b[1;32m    401\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    402\u001b[0m     )\n\u001b[1;32m    403\u001b[0m     \u001b[39mif\u001b[39;00m resolved_config_file \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    404\u001b[0m         logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mCould not locate the tokenizer configuration file, will try to use the model config instead.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/utils/hub.py:704\u001b[0m, in \u001b[0;36mget_file_from_repo\u001b[0;34m(path_or_repo, filename, cache_dir, force_download, resume_download, proxies, use_auth_token, revision, local_files_only)\u001b[0m\n\u001b[1;32m    700\u001b[0m     resolved_file \u001b[39m=\u001b[39m hf_bucket_url(path_or_repo, filename\u001b[39m=\u001b[39mfilename, revision\u001b[39m=\u001b[39mrevision, mirror\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m)\n\u001b[1;32m    702\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    703\u001b[0m     \u001b[39m# Load from URL or cache if already cached\u001b[39;00m\n\u001b[0;32m--> 704\u001b[0m     resolved_file \u001b[39m=\u001b[39m cached_path(\n\u001b[1;32m    705\u001b[0m         resolved_file,\n\u001b[1;32m    706\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    707\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    708\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    709\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    710\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    711\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    712\u001b[0m     )\n\u001b[1;32m    714\u001b[0m \u001b[39mexcept\u001b[39;00m RepositoryNotFoundError:\n\u001b[1;32m    715\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mEnvironmentError\u001b[39;00m(\n\u001b[1;32m    716\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mpath_or_repo\u001b[39m}\u001b[39;00m\u001b[39m is not a local folder and is not a valid model identifier \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    717\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mlisted on \u001b[39m\u001b[39m'\u001b[39m\u001b[39mhttps://huggingface.co/models\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf this is a private repository, make sure to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    718\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpass a token having permission to this repo with `use_auth_token` or log in with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    719\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`huggingface-cli login` and pass `use_auth_token=True`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    720\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/utils/hub.py:284\u001b[0m, in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, cache_dir, force_download, proxies, resume_download, user_agent, extract_compressed_file, force_extract, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    280\u001b[0m     local_files_only \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m    282\u001b[0m \u001b[39mif\u001b[39;00m is_remote_url(url_or_filename):\n\u001b[1;32m    283\u001b[0m     \u001b[39m# URL, so get it from the cache (downloading if necessary)\u001b[39;00m\n\u001b[0;32m--> 284\u001b[0m     output_path \u001b[39m=\u001b[39m get_from_cache(\n\u001b[1;32m    285\u001b[0m         url_or_filename,\n\u001b[1;32m    286\u001b[0m         cache_dir\u001b[39m=\u001b[39;49mcache_dir,\n\u001b[1;32m    287\u001b[0m         force_download\u001b[39m=\u001b[39;49mforce_download,\n\u001b[1;32m    288\u001b[0m         proxies\u001b[39m=\u001b[39;49mproxies,\n\u001b[1;32m    289\u001b[0m         resume_download\u001b[39m=\u001b[39;49mresume_download,\n\u001b[1;32m    290\u001b[0m         user_agent\u001b[39m=\u001b[39;49muser_agent,\n\u001b[1;32m    291\u001b[0m         use_auth_token\u001b[39m=\u001b[39;49muse_auth_token,\n\u001b[1;32m    292\u001b[0m         local_files_only\u001b[39m=\u001b[39;49mlocal_files_only,\n\u001b[1;32m    293\u001b[0m     )\n\u001b[1;32m    294\u001b[0m \u001b[39melif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(url_or_filename):\n\u001b[1;32m    295\u001b[0m     \u001b[39m# File, and it exists.\u001b[39;00m\n\u001b[1;32m    296\u001b[0m     output_path \u001b[39m=\u001b[39m url_or_filename\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/utils/hub.py:562\u001b[0m, in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, use_auth_token, local_files_only)\u001b[0m\n\u001b[1;32m    556\u001b[0m                 \u001b[39mraise\u001b[39;00m EntryNotFoundError(\n\u001b[1;32m    557\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot find the requested file (\u001b[39m\u001b[39m{\u001b[39;00mfname\u001b[39m}\u001b[39;00m\u001b[39m) in the cached path and outgoing traffic has been\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    558\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m disabled. To enable model look-ups and downloads online, set \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlocal_files_only\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    559\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m to False.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    560\u001b[0m                 )\n\u001b[1;32m    561\u001b[0m             \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 562\u001b[0m                 \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    563\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mConnection error, and we cannot find the requested files in the cached path.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    564\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39m Please try again or make sure your Internet connection is on.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    565\u001b[0m                 )\n\u001b[1;32m    567\u001b[0m \u001b[39m# From now on, etag is not None.\u001b[39;00m\n\u001b[1;32m    568\u001b[0m \u001b[39mif\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mexists(cache_path) \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m force_download:\n",
      "\u001b[0;31mValueError\u001b[0m: Connection error, and we cannot find the requested files in the cached path. Please try again or make sure your Internet connection is on."
     ]
    }
   ],
   "source": [
    "ddf_tc = keywords_from_TC_model(ddf, MODEL_TC_DIR)\n",
    "from transformers import pipeline, AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", local_files_only=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased-finetuned-sst-2-english\", local_files_only=True)\n",
    "\n",
    "pipeline('sentiment-analysis', model=model, tokenizer=tokenizer)("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make the dataset smaller by randomly choose 60% of the rows\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function to run the script\n",
    "    \"\"\"\n",
    "\n",
    "    ddf = dd.read_csv(  # type: ignore\n",
    "        MIMIC_PROCESSED_CLEANED_DIR,\n",
    "        dtype={\"TEXT_final_cleaned\": \"str\"},\n",
    "        engine=\"python\",\n",
    "        error_bad_lines=False,\n",
    "        warn_bad_lines=False,\n",
    "        encoding=\"utf8\",\n",
    "    )\n",
    "    # use only small set\n",
    "    ddf = ddf.head(100)\n",
    "    # ddf = ddf.repartition(npartitions=4)\n",
    "    ddf[\"nr_candidates\"] = ddf[\"TEXT_final_cleaned\"].apply(  # type: ignore\n",
    "        calculate_optimal_candidate_nr\n",
    "    )  # , meta=(\"nr_candidates\", \"int\")\n",
    "    # )\n",
    "    ddf[\"top_n\"] = ddf[\"nr_candidates\"].apply(\n",
    "        lambda x: int(x * 0.5)\n",
    "    )  # , meta=(\"top_n\", \"int\")\n",
    "    # )\n",
    "    ddf = ddf.compute()\n",
    "    ddf = small_column_df(ddf)\n",
    "    ddf_tc = keywords_from_TC_model(ddf, MODEL_TC_DIR)\n",
    "    ddf_mlm = keywords_from_MLM_model(ddf, MODEL_MLM_DIR)\n",
    "    # concatenate  the two dataframes\n",
    "    ddf_final = pd.concat([ddf_tc, ddf_mlm], axis=1)\n",
    "    # save\n",
    "    save_dataframe(ddf_final)\n",
    "\n",
    "\n",
    "# Path: src/Keyword_Bert_Training.py\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reduce size of dataset randomly to 60%\n",
    "                "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
