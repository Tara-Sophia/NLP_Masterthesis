{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 1 biobert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 3\u001b[0m in \u001b[0;36m<cell line: 15>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#W2sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m \u001b[39mimport\u001b[39;00m TensorDataset, DataLoader, RandomSampler, SequentialSampler\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#W2sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertConfig\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#W2sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpreprocessing\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msequence\u001b[39;00m \u001b[39mimport\u001b[39;00m pad_sequences\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#W2sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m \u001b[39mimport\u001b[39;00m train_test_split\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/keras/__init__.py:21\u001b[0m, in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[39m\"\"\"Implementation of the Keras API, the high-level API of TensorFlow.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \n\u001b[1;32m     17\u001b[0m \u001b[39mDetailed documentation and user guides are available at\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39m[keras.io](https://keras.io).\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[39m# pylint: disable=unused-import\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpython\u001b[39;00m \u001b[39mimport\u001b[39;00m tf2\n\u001b[1;32m     22\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m distribute\n\u001b[1;32m     24\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mkeras\u001b[39;00m \u001b[39mimport\u001b[39;00m models\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from itertools import chain\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0           0   A 23-year-old white female presents with comp...   \n",
       "1           1           Consult for laparoscopic gastric bypass.   \n",
       "\n",
       "       medical_specialty                                sample_name  \\\n",
       "0   Allergy / Immunology                         Allergic Rhinitis    \n",
       "1             Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
       "\n",
       "                                            keywords  \n",
       "0  allergy / immunology, allergic rhinitis, aller...  \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/raw/mtsamples.csv')\n",
    "df.transcription=df.transcription.astype(str)\n",
    "#print(df.columns)\n",
    "df_test = df.head(15)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/1066114346.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['keywords']=df_test['keywords'].fillna(\"\")\n",
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/1066114346.py:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['keywords_list']= df_test['keywords'].apply(lambda x: x.split(','))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "      <td>[allergy / immunology,  allergic rhinitis,  al...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "      <td>[bariatrics,  laparoscopic gastric bypass,  we...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0           0   A 23-year-old white female presents with comp...   \n",
       "1           1           Consult for laparoscopic gastric bypass.   \n",
       "\n",
       "       medical_specialty                                sample_name  \\\n",
       "0   Allergy / Immunology                         Allergic Rhinitis    \n",
       "1             Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  allergy / immunology, allergic rhinitis, aller...   \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...   \n",
       "\n",
       "                                       keywords_list  \n",
       "0  [allergy / immunology,  allergic rhinitis,  al...  \n",
       "1  [bariatrics,  laparoscopic gastric bypass,  we...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def convert(lst):\n",
    "    '''This function converts the keywords to a list of keywords'''\n",
    "    my_list = lst.split(',')\n",
    "    print(my_list)\n",
    "    return my_list\n",
    "    return ([i for i in lst.split()])\n",
    "df_test['keywords']=df_test['keywords'].fillna(\"\")\n",
    "df_test['keywords_list']= df_test['keywords'].apply(lambda x: x.split(','))\n",
    "#ACHTUNG HIER IST ES IMPORTANT IN EINEM NEXT STEP DIE KEYWORD LIST ZU CLEANEN\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/4118250391.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['location'] = df_test.apply(lambda x: location_indices(x.transcription, x.keywords_list), axis=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female pr...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "      <td>[allergy / immunology,  allergic rhinitis,  al...</td>\n",
       "      <td>([70, 79], [519, 525], [490, 502], [1036, 1044...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climb...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "      <td>[bariatrics,  laparoscopic gastric bypass,  we...</td>\n",
       "      <td>([1401, 1412], [1386, 1392], [658, 664], [1406...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0           0   A 23-year-old white female presents with comp...   \n",
       "1           1           Consult for laparoscopic gastric bypass.   \n",
       "\n",
       "       medical_specialty                                sample_name  \\\n",
       "0   Allergy / Immunology                         Allergic Rhinitis    \n",
       "1             Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female pr...   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climb...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  allergy / immunology, allergic rhinitis, aller...   \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...   \n",
       "\n",
       "                                       keywords_list  \\\n",
       "0  [allergy / immunology,  allergic rhinitis,  al...   \n",
       "1  [bariatrics,  laparoscopic gastric bypass,  we...   \n",
       "\n",
       "                                            location  \n",
       "0  ([70, 79], [519, 525], [490, 502], [1036, 1044...  \n",
       "1  ([1401, 1412], [1386, 1392], [658, 664], [1406...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def location_indices(stringstext, check_list):\n",
    "    '''this function finds the location of the keywords in the string'''\n",
    "    \n",
    "    res = dict()\n",
    "    for ele in check_list :\n",
    "        if ele in stringstext:\n",
    "            # getting front index\n",
    "            strt = stringstext.index(ele)\n",
    "            \n",
    "            # getting ending index\n",
    "            res[ele] = [strt, strt + len(ele) - 1]\n",
    "    return res.values()\n",
    "        \n",
    "df_test['location'] = df_test.apply(lambda x: location_indices(x.transcription, x.keywords_list), axis=1) \n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "      <th>location</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with complaint of allergies.</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,</td>\n",
       "      <td>[allergy / immunology,  allergic rhinitis,  allergies,  asthma,  nasal sprays,  rhinitis,  nasal,  erythematous,  allegra,  sprays,  allergic, ]</td>\n",
       "      <td>([70, 79], [519, 525], [490, 502], [1036, 1044], [490, 495], [842, 854], [463, 469], [0, -1])</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "\n",
       "                                                         description  \\\n",
       "0   A 23-year-old white female presents with complaint of allergies.   \n",
       "\n",
       "       medical_specialty          sample_name  \\\n",
       "0   Allergy / Immunology   Allergic Rhinitis    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                         transcription  \\\n",
       "0  SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.   \n",
       "\n",
       "                                                                                                                              keywords  \\\n",
       "0  allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,   \n",
       "\n",
       "                                                                                                                                      keywords_list  \\\n",
       "0  [allergy / immunology,  allergic rhinitis,  allergies,  asthma,  nasal sprays,  rhinitis,  nasal,  erythematous,  allegra,  sprays,  allergic, ]   \n",
       "\n",
       "                                                                                        location  \\\n",
       "0  ([70, 79], [519, 525], [490, 502], [1036, 1044], [490, 495], [842, 854], [463, 469], [0, -1])   \n",
       "\n",
       "   kfold  \n",
       "0      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#new try Kfold\n",
    "import numpy as np\n",
    "frames = []\n",
    "df_split = np.array_split(df_test, 5)\n",
    "for split in range(0, 5):\n",
    "    df_split[split]['kfold'] = split\n",
    "    frames.append(df_split[split])\n",
    "dfx = pd.concat(frames)\n",
    "dfx\n",
    "\n",
    "#find max len der texte\n",
    "max_len = df_test['transcription'].map(lambda x: len(x)).max()\n",
    "max_len\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "dfx.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "\n",
    "class config:\n",
    "    MAX_LEN = 42 # 416\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE = 8\n",
    "    EPOCHS = 5\n",
    "    \n",
    "    # BERT_PATH = \"../input/bert-base-uncased/\" \n",
    "    # MODEL_PATH = \"model.bin\"\n",
    "    # TOKENIZER = tokenizers.BertWordPieceTokenizer(f\"{BERT_PATH}/vocab.txt\" ,lowercase = True)\n",
    "    \n",
    "    BERT_PATH = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "    MODEL_PATH = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    DROPOUT = 0.2\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_list_to_ints(loc_list):\n",
    "    tuples = []\n",
    "    for sublist in loc_list:\n",
    "        tuples.append(tuple(sublist))\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_tokenize(feature_text, annotation, location, tokenizer, max_len):    ##X , Y, selected_text  \n",
    "    '''this function tokenizes the data'''\n",
    "    location_list = loc_list_to_ints(location)        \n",
    "    char_targets = [0] * len(feature_text) #creating empty list(all zeros) of character;it will be made 1 if annotation in text   \n",
    "    for loc,anno in zip(location_list,annotation): \n",
    "        len_st = loc[1] - loc[0]\n",
    "\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for ind in (i for i, e in enumerate(feature_text) if (e == anno[0] and i == loc[0])):\n",
    "            if feature_text[ind: ind+len_st] == anno:\n",
    "\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                if idx0 != None and idx1 != None:\n",
    "                    for ct in range(idx0, idx1 + 1):\n",
    "                        char_targets[ct] = 1  #replacing zeros with 1 if that part of the text is selected text\n",
    "        \n",
    "                break\n",
    "    print('in process function')\n",
    "    tokenized_input = tokenizer.encode_plus(feature_text,\n",
    "                                            add_special_tokens=False, \n",
    "                                            return_attention_mask=True, \n",
    "                                            return_offsets_mapping=True,\n",
    "                                            padding =  'max_length',\n",
    "                                            truncation = True,\n",
    "                                            max_length=42,\n",
    "                                            return_tensors = 'pt')\n",
    "    \n",
    "       # `encode_plus` will:\n",
    "       \n",
    "       #ValueError: 4422 is not a valid PaddingStrategy, please select one of ['longest', 'max_length', 'do_not_pad']\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "    \n",
    "    # for key, value in tokenized_input.items():\n",
    "    #     print( '{} : {}'.format( key, value ) )\n",
    "    input_ids = tokenized_input.get('input_ids')\n",
    "    mask = tokenized_input.get('attention_mask')\n",
    "    token_type_ids = tokenized_input.get('token_type_ids')\n",
    "    offsets = tokenized_input.get('offset_mapping')\n",
    "    #print(feature_text)\n",
    "    # print('token_type_ids.shape', token_type_ids.shape)\n",
    "    print('offsets.shape', offsets.shape)\n",
    "    \n",
    "    \n",
    "    target_idx = []\n",
    "    for value in offsets:\n",
    "        for j, (offset1, offset2) in enumerate(value):\n",
    "            #print(j, offset1, offset2)\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "    #creating label\n",
    "    ignore_idxes = np.where(np.array(token_type_ids) != 1)[0]\n",
    "    label = np.zeros(offsets.size())#, dtype=np.float32)]))\n",
    "    label[ignore_idxes] = -1\n",
    "    label[target_idx] = 1\n",
    "\n",
    "    \n",
    "    return {\n",
    "    'ids': input_ids,\n",
    "    'mask': mask,\n",
    "    'token_type_ids': token_type_ids,\n",
    "    'labels': label,\n",
    "    'offsets': offsets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "process_data_tokenize\n",
      "annp ['cardiovascular', '/', 'pulmonary,', '2-d', 'm-mode,', 'doppler,', 'aortic', 'valve,', 'atrial', 'enlargement,', 'diastolic', 'function,', 'ejection', 'fraction,', 'mitral,', 'mitral', 'valve,', 'pericardial', 'effusion,', 'pulmonary', 'valve,', 'regurgitation,', 'systolic', 'function,', 'tricuspid,', 'tricuspid', 'valve,', 'normal', 'lv']\n",
      "*************\n",
      "loc dict_values([[290, 295], [297, 302], [24, 29], [216, 224], [176, 183], [304, 309], [243, 253], [339, 347], [136, 143], [318, 326], [299, 300]])\n",
      "*************\n",
      "label [[[0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]\n",
      "  [0. 0.]]]\n",
      "process_data_tokenize\n",
      "annp [['allergy', '/', 'immunology,', 'allergic', 'rhinitis,', 'allergies,', 'asthma,', 'nasal', 'sprays,', 'rhinitis,', 'nasal,', 'erythematous,', 'allegra,', 'sprays,', 'allergic,'], ['bariatrics,', 'laparoscopic', 'gastric', 'bypass,', 'weight', 'loss', 'programs,', 'gastric', 'bypass,', \"atkin's\", 'diet,', 'weight', \"watcher's,\", 'body', 'weight,', 'laparoscopic', 'gastric,', 'weight', 'loss,', 'pounds,', 'months,', 'weight,', 'laparoscopic,', 'band,', 'loss,', 'diets,', 'overweight,', 'lost'], ['bariatrics,', 'laparoscopic', 'gastric', 'bypass,', 'heart', 'attacks,', 'body', 'weight,', 'pulmonary', 'embolism,', 'potential', 'complications,', 'sleep', 'study,', 'weight', 'loss,', 'gastric', 'bypass,', 'anastomosis,', 'loss,', 'sleep,', 'laparoscopic,', 'gastric,', 'bypass,', 'heart,', 'pounds,', 'weight,'], ['cardiovascular', '/', 'pulmonary,', '2-d', 'm-mode,', 'doppler,', 'aortic', 'valve,', 'atrial', 'enlargement,', 'diastolic', 'function,', 'ejection', 'fraction,', 'mitral,', 'mitral', 'valve,', 'pericardial', 'effusion,', 'pulmonary', 'valve,', 'regurgitation,', 'systolic', 'function,', 'tricuspid,', 'tricuspid', 'valve,', 'normal', 'lv'], ['cardiovascular', '/', 'pulmonary,', '2-d,', 'doppler,', 'echocardiogram,', 'annular,', 'aortic', 'root,', 'aortic', 'valve,', 'atrial,', 'atrium,', 'calcification,', 'cavity,', 'ejection', 'fraction,', 'mitral,', 'obliteration,', 'outflow,', 'regurgitation,', 'relaxation', 'pattern,', 'stenosis,', 'systolic', 'function,', 'tricuspid,', 'valve,', 'ventricular,', 'ventricular', 'cavity,', 'wall', 'motion,', 'pulmonary', 'artery'], ['bariatrics,', 'gastric', 'bypass,', 'eea', 'anastomosis,', 'roux-en-y,', 'antegastric,', 'antecolic,', 'morbid', 'obesity,', 'roux', 'limb,', 'gastric', 'pouch,', 'intubation,', 'laparoscopic,', 'bypass,', 'roux,', 'endotracheal,', 'anastomosis,', 'gastric'], ['bariatrics,', 'breast', 'reconstruction,', 'excess,', 'lma', 'anesthesia,', 'lipodystrophy,', 'liposuction,', 'abdomen,', 'drain', 'site,', 'flank,', 'latissimus', 'dorsi', 'flap,', 'soft', 'tissue,', 'supraumbilical,', 'surgical', 'bra,', 'supraumbilical', 'abdomen,', 'reconstruction,', 'breast,', 'tissue,', 'implant,'], ['cardiovascular', '/', 'pulmonary,', '2-d', 'echocardiogram,', 'cardiac', 'function,', 'doppler,', 'echocardiogram,', 'multiple', 'views,', 'aortic', 'valve,', 'coronary', 'arteries,', 'descending', 'aorta,', 'great', 'vessels,', 'heart,', 'hypertrophy,', 'interatrial', 'septum,', 'intracardiac,', 'pericardial', 'effusion,', 'tricuspid,', 'vegetation,', 'venous,', 'pulmonaryNOTE,:', 'Thesetranscribed', 'medical', 'transcription', 'sample', 'reports', 'and', 'examples', 'are', 'provided', 'by', 'various', 'users', 'andare', 'for', 'reference', 'purpose', 'only.', 'MTHelpLine', 'does', 'not', 'certify', 'accuracy', 'and', 'quality', 'of', 'sample', 'reports.These', 'transcribed', 'medical', 'transcription', 'sample', 'reports', 'may', 'include', 'some', 'uncommon', 'or', 'unusual', 'formats;this', 'would', 'be', 'due', 'to', 'the', 'preference', 'of', 'the', 'dictating', 'physician.', 'All', 'names', 'and', 'dates', 'have', 'beenchanged', '(or', 'removed)', 'to', 'keep', 'confidentiality.', 'Any', 'resemblance', 'of', 'any', 'type', 'of', 'name', 'or', 'date', 'orplace', 'or', 'anything', 'else', 'to', 'real', 'world', 'is', 'purely', 'incidental.,'], ['bariatrics,', 'lipodystrophy,', 'abd', 'pads,', 'suction-assisted', 'lipectomy,', 'abdomen,', 'aspirate,', 'lipectomy,', 'perineum,', 'steri-stripped,', 'thighs,', 'umbilicus,', 'abdomen', 'and', 'thighs,', 'abdomen/thighs,'], ['cardiovascular', '/', 'pulmonary,', 'ejection', 'fraction,', 'lv', 'systolic', 'function,', 'cardiac', 'chambers,', 'regurgitation,', 'tricuspid,', 'normal', 'lv', 'systolic', 'function,', 'normal', 'lv', 'systolic,', 'ejection', 'fraction', 'estimated,', 'normal', 'lv,', 'lv', 'systolic,', 'systolic', 'function,', 'function', 'ejection,', 'echocardiogram,', 'doppler,', 'lv,', 'systolic,', 'ejection,', 'mitral,', 'valve'], ['bariatrics,', 'morbid', 'obesity,', 'roux-en-y,', 'gastric', 'bypass,', 'antecolic,', 'antegastric,', 'anastamosis,', 'esophagogastroduodenoscopy,', 'eea,', 'surgidac', 'sutures,', 'roux', 'limb,', 'port,', 'stapler,', 'laparoscopic,', 'intubation'], ['cardiovascular', '/', 'pulmonary,', '2-d', 'study,', 'doppler,', 'tricuspid', 'regurgitation,', 'heart', 'pressures,', 'stenosis,', 'ventricular,', 'heart,', 'ventricle,', 'tricuspid,', 'regurgitation,'], [], ['bariatrics,', 'weight', 'watchers,', 'roux', 'en', 'y,', 'atkins,', 'medifast,', 'meridia,', 'south', 'beach,', 'cabbage,', 'diets,', 'laparoscopic', 'roux', 'en', 'y', 'gastric', 'bypass', 'surgery,', 'rice,', 'weight', 'loss,', 'six', 'weeks', 'of', 'medifast,', 'weight', 'loss', 'modalities,', 'body', 'mass', 'index,', 'gastric', 'bypass', 'surgery,', 'bariatric', 'surgery,', 'gastric', 'bypass,'], ['dentistry,', 'intraoral,', 'bony', 'impacted', 'teeth,', 'throat', 'pack,', 'buccal', 'aspect,', 'saline', 'solution,', 'gut', 'sutures,', 'envelope', 'flap,', 'periosteal', 'elevator,']]\n",
      "*************\n",
      "loc [dict_values([[808, 808], [491, 495]]), dict_values([[1407, 1412], [1402, 1405], [1473, 1476]]), dict_values([[3603, 3614], [3616, 3622], [1693, 1697], [2368, 2371], [2780, 2788], [2790, 2798], [3576, 3584], [1174, 1178], [170, 175], [3754, 3765], [2574, 2579], [238, 244]]), dict_values([[290, 295], [297, 302], [24, 29], [216, 224], [176, 183], [304, 309], [243, 253], [339, 347], [136, 143], [318, 326], [299, 300]]), dict_values([[641, 646], [163, 170], [438, 447], [731, 739], [109, 116], [13, 23], [41, 44], [1112, 1120], [1122, 1127]]), dict_values([[125, 131], [3427, 3438], [2178, 2182]]), dict_values([[45, 50], [1606, 1616], [3322, 3329], [2771, 2775], [602, 611], [613, 617], [79, 82], [84, 90], [3646, 3653], [456, 469], [628, 635]]), dict_values([[84, 90], [587, 592], [620, 627], [730, 739], [51, 55], [500, 510], [227, 237], [47, 49], [570, 572], [34, 35], [73, 74], [182, 183], [354, 355], [37, 39], [142, 143]]), dict_values([[47, 49], [305, 312], [819, 827], [322, 330], [47, 53], [55, 57]]), dict_values([[168, 169], [95, 102], [24, 30], [124, 131], [104, 111], [166, 170]]), dict_values([[469, 474], [123, 129], [131, 137], [139, 148], [177, 188], [259, 268]]), dict_values([[231, 239], [599, 612], [476, 480], [482, 491], [25, 33]]), dict_values([]), dict_values([[1494, 1499], [46, 47], [603, 604], [41, 41], [2759, 2765], [2767, 2772], [3180, 3182], [3184, 3188], [410, 411], [3254, 3257], [2671, 2674], [2676, 2679], [2638, 2646]]), dict_values([[37, 40], [42, 49], [633, 638], [998, 1003], [1610, 1615], [1211, 1213], [949, 958], [960, 968]])]\n",
      "*************\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 12\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df_traindict \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mtranscription,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mkeywords_list\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#list(df_traindict['feature_text'])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output_train \u001b[39m=\u001b[39m process_data_tokenize(\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mfeature_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mannotation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),config\u001b[39m.\u001b[39;49mTOKENIZER,config\u001b[39m.\u001b[39;49mMAX_LEN)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m output_train\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 12\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39m*************\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc,anno \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(location_list,annotation): \n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     len_st \u001b[39m=\u001b[39m loc[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m loc[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     idx0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X15sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     idx1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "output = process_data_tokenize(example[\"feature_text\"],example[\"annotation\"],example[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "\n",
    "#for key in output.keys():\n",
    "#    print(key)\n",
    "#    print(output[key])\n",
    "#    print(\"=\" * 100)\n",
    "    \n",
    "output\n",
    "\n",
    "\n",
    "df_test.head(1)\n",
    "df_traindict = {\n",
    "    \"feature_text\": df_test.transcription,\n",
    "    \"location\": df_test.location,\n",
    "    \"annotation\": df_test.keywords_list\n",
    "}\n",
    "#list(df_traindict['feature_text'])\n",
    "output_train = process_data_tokenize(list(df_traindict[\"feature_text\"]),list(df_traindict[\"annotation\"]),list(df_traindict[\"location\"]),config.TOKENIZER,config.MAX_LEN)\n",
    "#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "output_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loader\n",
    "class NBMEDataset:\n",
    "    def __init__(self,  feature_text, annotation, location):   #text(X) #label(Y), #selected_text #start:end\n",
    "        #self.pn_history = pn_history\n",
    "        self.feature_text = list(feature_text)\n",
    "        self.annotation = list(annotation)\n",
    "        self.location = list(location)\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature_text)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        data = process_data_tokenize(\n",
    "            #self.pn_history[item],\n",
    "            self.feature_text[item],\n",
    "            self.annotation[item],\n",
    "            self.location[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"]), #input_ids\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long), #attention_mask\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long), #segment_ids\n",
    "            'labels': torch.tensor(data[\"labels\"], dtype=torch.long), \n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "class NBMEModel(transformers.BertPreTrainedModel):    #torch.nn.Module\n",
    "    def __init__(self,conf):\n",
    "        super(NBMEModel,self).__init__(conf)\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        self.dropout = torch.nn.Dropout(config.DROPOUT)\n",
    "        self.classifier = torch.nn.Linear(768, 1)\n",
    "        torch.nn.init.normal_(self.classifier.weight, std=0.02) \n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        sequence_out = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)[0] #last_hidden_state\n",
    "        batch_size, max_len, feat_dim = sequence_out.shape\n",
    "        sequence_output = self.dropout(sequence_out)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "#loss function\n",
    "def loss_fn(logits, labels):\n",
    "    print('in loss function')\n",
    "    #labels = labels.unsqueeze(1)\n",
    "    loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "    loss = loss_fct(logits,labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a next step we use data loader to load the data - https://blog.paperspace.com/dataloaders-abstractions-pytorch/\n",
    "# in a next step with the help of the optimizer we will update the weights of the model\n",
    "#training function\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train_fn(dataloader, model, optimizer, scheduler=None): #before: dataloader\n",
    "    model.train()\n",
    "    losses = AverageMeter() \n",
    "    print('in function train_fn')\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, total=len(dataloader))):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        ids = batch['ids'].squeeze(1)\n",
    "        print('batchids',ids.shape)\n",
    "\n",
    "        #output = torch.squeeze(input)\n",
    "        print('batchidssqueeze',ids.shape)\n",
    "        logits = model(ids=ids, mask=batch['mask'], token_type_ids=batch['token_type_ids'])\n",
    "        #model(ids=batch['ids'], mask=batch['mask'], token_type_ids=batch['token_type_ids'])\n",
    "        loss = loss_fn(logits,batch[\"labels\"])\n",
    "        loss = loss.mean()\n",
    "        # loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "        # losses.update(loss.item(),ids.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        losses.update(loss.item(), batch['ids'].size(0))\n",
    "        dataloader.set_postfix(loss=losses.avg) #tk. bfefore\n",
    "        #or like this - https://www.kaggle.com/code/daisybbb/pytorch-pubmedbert-infer\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step() ## Update learning rate schedule\n",
    "    \n",
    "    # #output = torch.argmax(torch.softmax(logits, dim=2),dim=2).cpu().detach().numpy()\n",
    "        tqdm(dataloader, total=len(dataloader)).set_postfix(loss=losses.avg)\n",
    "        \n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df_traindict \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mtranscription,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mkeywords_list\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mlist\u001b[39m(df_traindict[\u001b[39m'\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output_train \u001b[39m=\u001b[39m process_data_tokenize(\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mfeature_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mannotation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),config\u001b[39m.\u001b[39;49mTOKENIZER,config\u001b[39m.\u001b[39;49mMAX_LEN)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output_train\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 25\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m char_targets \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(feature_text) \u001b[39m#creating empty list(all zeros) of character;it will be made 1 if annotation in text   \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc,anno \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(location_list,annotation): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     len_st \u001b[39m=\u001b[39m loc[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m loc[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     idx0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     idx1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    " # train model\n",
    " #adam w - https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218\n",
    " #other optimizer https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008\n",
    " #https://firiuza.medium.com/optimizers-for-training-neural-networks-e0196662e21e\n",
    " #https://medium.com/@ThiyaneshwaranG/types-of-optimizers-in-deep-learning-from-gradient-descent-to-adam-1572e657c8c5\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#-------------------\n",
    "df_test.head(1)\n",
    "df_traindict = {\n",
    "    \"feature_text\": df_test.transcription,\n",
    "    \"location\": df_test.location,\n",
    "    \"annotation\": df_test.keywords_list\n",
    "}\n",
    "list(df_traindict['feature_text'])\n",
    "output_train = process_data_tokenize(list(df_traindict[\"feature_text\"]),list(df_traindict[\"annotation\"]),list(df_traindict[\"location\"]),config.TOKENIZER,config.MAX_LEN)\n",
    "#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "output_train\n",
    "\n",
    "\n",
    "\n",
    "#-----------------#\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "model = NBMEModel(conf=model_config)\n",
    "model.to(DEVICE)\n",
    "    \n",
    "#num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "#print('param_optimizer', param_optimizer)\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=2\n",
    "    )\n",
    "#-----------------#\n",
    "train_loss = train_fn(x, model, optimizer, scheduler=scheduler)\n",
    "#train_loss_data.append(train_loss)\n",
    "print(f\"Train loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def eval_fn(dataloader, model): #before: dataloader\n",
    "    model.eval()\n",
    "    losses = AverageMeter() # Computes and stores the average and current value\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tk = tqdm(dataloader, total=len(dataloader)) \n",
    "        #tk = tqdm(data, total=5)\n",
    "        \n",
    "        # ids = data['ids']\n",
    "        # token_type_ids = data[\"token_type_ids\"]\n",
    "        # mask = data[\"mask\"]\n",
    "        # labels = data[\"labels\"]\n",
    "        # offsets = data[\"offsets\"]#    \n",
    "        # labels = labels.unsqueeze(0)\n",
    "\n",
    "        for batch, data in enumerate(tk):\n",
    "\n",
    "            ids = data['ids']\n",
    "            token_type_ids = data[\"token_type_ids\"]\n",
    "            mask = data[\"mask\"]\n",
    "            labels = data[\"labels\"]\n",
    "            offsets = data[\"offsets\"]\n",
    "\n",
    "            ids = ids.to(DEVICE, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "            mask = mask.to(DEVICE, dtype=torch.long)\n",
    "            labels = labels.to(DEVICE, dtype=torch.float64)\n",
    "\n",
    "            logits = model(ids=ids, mask=mask, token_type_ids=token_type_ids) #last_hidden_state\n",
    "            \n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "            losses.update(loss.item(),ids.size(0))\n",
    "            tk.set_postfix(loss=losses.avg)\n",
    "        \n",
    "        return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_traindict \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mtranscription,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mkeywords_list\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mlist\u001b[39m(df_traindict[\u001b[39m'\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output_train \u001b[39m=\u001b[39m process_data_tokenize(\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mfeature_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mannotation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),config\u001b[39m.\u001b[39;49mTOKENIZER,config\u001b[39m.\u001b[39;49mMAX_LEN)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output_train\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 20\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m char_targets \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(feature_text) \u001b[39m#creating empty list(all zeros) of character;it will be made 1 if annotation in text   \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc,anno \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(location_list,annotation): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     len_st \u001b[39m=\u001b[39m loc[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m loc[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     idx0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     idx1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "df_test.head(1)\n",
    "df_traindict = {\n",
    "    \"feature_text\": df_test.transcription,\n",
    "    \"location\": df_test.location,\n",
    "    \"annotation\": df_test.keywords_list\n",
    "}\n",
    "list(df_traindict['feature_text'])\n",
    "output_train = process_data_tokenize(list(df_traindict[\"feature_text\"]),list(df_traindict[\"annotation\"]),list(df_traindict[\"location\"]),config.TOKENIZER,config.MAX_LEN)\n",
    "#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "output_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "in function train_fn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67e7d2099c944159b2ce0a4b46445487",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/1640454316.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'ids': torch.tensor(data[\"ids\"]), #input_ids\n",
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/1640454316.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'mask': torch.tensor(data[\"mask\"], dtype=torch.long), #attention_mask\n",
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/1640454316.py:26: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long), #segment_ids\n",
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/1640454316.py:28: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "in process function\n",
      "offsets.shape torch.Size([1, 42, 2])\n",
      "batchids torch.Size([8, 42])\n",
      "batchidssqueeze torch.Size([8, 42])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "number of dims don't match in permute",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 28\u001b[0m in \u001b[0;36m<cell line: 86>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=77'>78</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodel_fold1.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m         \u001b[39m#time_elapsed = time.time() - since\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         \u001b[39m#print('Training completed in {:.0f}m {:.0f}s'.format(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         \u001b[39m#    time_elapsed // 60, time_elapsed % 60))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=85'>86</a>\u001b[0m run(fold\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 28\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, config\u001b[39m.\u001b[39mEPOCHS))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train_fn(train_data_loader\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m                       , model, optimizer, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m train_loss_data\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 28\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(dataloader, model, optimizer, scheduler)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m#output = torch.squeeze(input)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39mbatchidssqueeze\u001b[39m\u001b[39m'\u001b[39m,ids\u001b[39m.\u001b[39mshape)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m logits \u001b[39m=\u001b[39m model(ids\u001b[39m=\u001b[39;49mids, mask\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39mmask\u001b[39;49m\u001b[39m'\u001b[39;49m], token_type_ids\u001b[39m=\u001b[39;49mbatch[\u001b[39m'\u001b[39;49m\u001b[39mtoken_type_ids\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m \u001b[39m#model(ids=batch['ids'], mask=batch['mask'], token_type_ids=batch['token_type_ids'])\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m loss \u001b[39m=\u001b[39m loss_fn(logits,batch[\u001b[39m\"\u001b[39m\u001b[39mlabels\u001b[39m\u001b[39m\"\u001b[39m])\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 28\u001b[0m in \u001b[0;36mNBMEModel.forward\u001b[0;34m(self, ids, mask, token_type_ids)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, ids, mask, token_type_ids):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     sequence_out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(ids, attention_mask\u001b[39m=\u001b[39;49mmask, token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids)[\u001b[39m0\u001b[39m] \u001b[39m#last_hidden_state\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     batch_size, max_len, feat_dim \u001b[39m=\u001b[39m sequence_out\u001b[39m.\u001b[39mshape\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     sequence_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(sequence_out)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1022\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1013\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1015\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1016\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1017\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1020\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1021\u001b[0m )\n\u001b[0;32m-> 1022\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1023\u001b[0m     embedding_output,\n\u001b[1;32m   1024\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1025\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1026\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1027\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1028\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1029\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1030\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1031\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1032\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1033\u001b[0m )\n\u001b[1;32m   1034\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1035\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:611\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    602\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    603\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    604\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    608\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    609\u001b[0m     )\n\u001b[1;32m    610\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 611\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    612\u001b[0m         hidden_states,\n\u001b[1;32m    613\u001b[0m         attention_mask,\n\u001b[1;32m    614\u001b[0m         layer_head_mask,\n\u001b[1;32m    615\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    616\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    617\u001b[0m         past_key_value,\n\u001b[1;32m    618\u001b[0m         output_attentions,\n\u001b[1;32m    619\u001b[0m     )\n\u001b[1;32m    621\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    622\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:497\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    485\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    486\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    487\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    494\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    495\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    496\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 497\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    498\u001b[0m         hidden_states,\n\u001b[1;32m    499\u001b[0m         attention_mask,\n\u001b[1;32m    500\u001b[0m         head_mask,\n\u001b[1;32m    501\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    502\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    503\u001b[0m     )\n\u001b[1;32m    504\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    506\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:427\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    417\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    418\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    419\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    425\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    426\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 427\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    428\u001b[0m         hidden_states,\n\u001b[1;32m    429\u001b[0m         attention_mask,\n\u001b[1;32m    430\u001b[0m         head_mask,\n\u001b[1;32m    431\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    432\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    433\u001b[0m         past_key_value,\n\u001b[1;32m    434\u001b[0m         output_attentions,\n\u001b[1;32m    435\u001b[0m     )\n\u001b[1;32m    436\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    437\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:315\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    313\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    314\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 315\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    316\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    318\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:281\u001b[0m, in \u001b[0;36mBertSelfAttention.transpose_for_scores\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    279\u001b[0m new_x_shape \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39msize()[:\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m+\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_attention_heads, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mattention_head_size)\n\u001b[1;32m    280\u001b[0m x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mview(new_x_shape)\n\u001b[0;32m--> 281\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39;49mpermute(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m, \u001b[39m3\u001b[39;49m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: number of dims don't match in permute"
     ]
    }
   ],
   "source": [
    "\n",
    "#training\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "def run(fold):\n",
    "    \n",
    "    train_loss_data, valid_loss_data = [], []\n",
    "    best_loss = np.inf\n",
    "    since = time.time()\n",
    "   \n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True) \n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = NBMEDataset(\n",
    "        feature_text=df_train.transcription.values,\n",
    "        annotation=df_train.keywords_list.values,\n",
    "        location=df_train.location.values\n",
    "        \n",
    "    )\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    valid_dataset = NBMEDataset(\n",
    "        feature_text=df_valid.transcription.values,\n",
    "        annotation=df_valid.keywords_list.values,\n",
    "        location=df_valid.location.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = NBMEModel(conf=model_config)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for i in range(config.EPOCHS):\n",
    "        print(\"Epoch: {}/{}\".format(i + 1, config.EPOCHS))\n",
    "\n",
    "        # train model\n",
    "        train_loss = train_fn(train_data_loader\n",
    "                              , model, optimizer, scheduler=scheduler)\n",
    "        train_loss_data.append(train_loss)\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "\n",
    "        # evaluate model - not now\n",
    "      #  valid_loss = eval_fn(valid_data_loader, model)\n",
    "      #  valid_loss_data.append(valid_loss)\n",
    "      #  print(f\"Valid loss: {valid_loss}\")\n",
    "\n",
    "\n",
    "       # if valid_loss < best_loss:\n",
    "        #    best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"model_fold1.bin\")\n",
    "\n",
    "\n",
    "        #time_elapsed = time.time() - since\n",
    "        #print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        #    time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    \n",
    "run(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test 2 keyword berd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['algorithm',\n",
       " 'algorithm analyzes',\n",
       " 'algorithm analyzes training',\n",
       " 'algorithm correctly',\n",
       " 'algorithm correctly determine',\n",
       " 'algorithm generalize',\n",
       " 'algorithm generalize training',\n",
       " 'allow',\n",
       " 'allow algorithm',\n",
       " 'allow algorithm correctly',\n",
       " 'analyzes',\n",
       " 'analyzes training',\n",
       " 'analyzes training data',\n",
       " 'based',\n",
       " 'based example',\n",
       " 'based example input',\n",
       " 'bias',\n",
       " 'called',\n",
       " 'called supervisory',\n",
       " 'called supervisory signal',\n",
       " 'class',\n",
       " 'class labels',\n",
       " 'class labels unseen',\n",
       " 'consisting',\n",
       " 'consisting input',\n",
       " 'consisting input object',\n",
       " 'consisting set',\n",
       " 'consisting set training',\n",
       " 'correctly',\n",
       " 'correctly determine',\n",
       " 'correctly determine class',\n",
       " 'data',\n",
       " 'data consisting',\n",
       " 'data consisting set',\n",
       " 'data produces',\n",
       " 'data produces inferred',\n",
       " 'data unseen',\n",
       " 'data unseen situations',\n",
       " 'desired',\n",
       " 'desired output',\n",
       " 'desired output value',\n",
       " 'determine',\n",
       " 'determine class',\n",
       " 'determine class labels',\n",
       " 'example',\n",
       " 'example input',\n",
       " 'example input output',\n",
       " 'example pair',\n",
       " 'example pair consisting',\n",
       " 'examples',\n",
       " 'examples optimal',\n",
       " 'examples optimal scenario',\n",
       " 'examples supervised',\n",
       " 'examples supervised learning',\n",
       " 'function',\n",
       " 'function labeled',\n",
       " 'function labeled training',\n",
       " 'function maps',\n",
       " 'function maps input',\n",
       " 'function used',\n",
       " 'function used mapping',\n",
       " 'generalize',\n",
       " 'generalize training',\n",
       " 'generalize training data',\n",
       " 'inductive',\n",
       " 'inductive bias',\n",
       " 'inferred',\n",
       " 'inferred function',\n",
       " 'inferred function used',\n",
       " 'infers',\n",
       " 'infers function',\n",
       " 'infers function labeled',\n",
       " 'input',\n",
       " 'input object',\n",
       " 'input object typically',\n",
       " 'input output',\n",
       " 'input output based',\n",
       " 'input output pairs',\n",
       " 'instances',\n",
       " 'instances requires',\n",
       " 'instances requires learning',\n",
       " 'labeled',\n",
       " 'labeled training',\n",
       " 'labeled training data',\n",
       " 'labels',\n",
       " 'labels unseen',\n",
       " 'labels unseen instances',\n",
       " 'learning',\n",
       " 'learning algorithm',\n",
       " 'learning algorithm analyzes',\n",
       " 'learning algorithm generalize',\n",
       " 'learning example',\n",
       " 'learning example pair',\n",
       " 'learning function',\n",
       " 'learning function maps',\n",
       " 'learning machine',\n",
       " 'learning machine learning',\n",
       " 'learning task',\n",
       " 'learning task learning',\n",
       " 'machine',\n",
       " 'machine learning',\n",
       " 'machine learning task',\n",
       " 'mapping',\n",
       " 'mapping new',\n",
       " 'mapping new examples',\n",
       " 'maps',\n",
       " 'maps input',\n",
       " 'maps input output',\n",
       " 'new',\n",
       " 'new examples',\n",
       " 'new examples optimal',\n",
       " 'object',\n",
       " 'object typically',\n",
       " 'object typically vector',\n",
       " 'optimal',\n",
       " 'optimal scenario',\n",
       " 'optimal scenario allow',\n",
       " 'output',\n",
       " 'output based',\n",
       " 'output based example',\n",
       " 'output pairs',\n",
       " 'output pairs infers',\n",
       " 'output value',\n",
       " 'output value called',\n",
       " 'pair',\n",
       " 'pair consisting',\n",
       " 'pair consisting input',\n",
       " 'pairs',\n",
       " 'pairs infers',\n",
       " 'pairs infers function',\n",
       " 'produces',\n",
       " 'produces inferred',\n",
       " 'produces inferred function',\n",
       " 'reasonable',\n",
       " 'reasonable way',\n",
       " 'reasonable way inductive',\n",
       " 'requires',\n",
       " 'requires learning',\n",
       " 'requires learning algorithm',\n",
       " 'scenario',\n",
       " 'scenario allow',\n",
       " 'scenario allow algorithm',\n",
       " 'set',\n",
       " 'set training',\n",
       " 'set training examples',\n",
       " 'signal',\n",
       " 'signal supervised',\n",
       " 'signal supervised learning',\n",
       " 'situations',\n",
       " 'situations reasonable',\n",
       " 'situations reasonable way',\n",
       " 'supervised',\n",
       " 'supervised learning',\n",
       " 'supervised learning algorithm',\n",
       " 'supervised learning example',\n",
       " 'supervised learning machine',\n",
       " 'supervisory',\n",
       " 'supervisory signal',\n",
       " 'supervisory signal supervised',\n",
       " 'task',\n",
       " 'task learning',\n",
       " 'task learning function',\n",
       " 'training',\n",
       " 'training data',\n",
       " 'training data consisting',\n",
       " 'training data produces',\n",
       " 'training data unseen',\n",
       " 'training examples',\n",
       " 'training examples supervised',\n",
       " 'typically',\n",
       " 'typically vector',\n",
       " 'typically vector desired',\n",
       " 'unseen',\n",
       " 'unseen instances',\n",
       " 'unseen instances requires',\n",
       " 'unseen situations',\n",
       " 'unseen situations reasonable',\n",
       " 'used',\n",
       " 'used mapping',\n",
       " 'used mapping new',\n",
       " 'value',\n",
       " 'value called',\n",
       " 'value called supervisory',\n",
       " 'vector',\n",
       " 'vector desired',\n",
       " 'vector desired output',\n",
       " 'way',\n",
       " 'way inductive',\n",
       " 'way inductive bias']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "https://medium.com/geekculture/3-techniques-to-perform-named-entity-recognition-ner-on-text-data-ec1e91e3a8aa\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "doc = \"\"\"\n",
    "         Supervised learning is the machine learning task of \n",
    "         learning a function that maps an input to an output based \n",
    "         on example input-output pairs.[1] It infers a function \n",
    "         from labeled training data consisting of a set of \n",
    "         training examples.[2] In supervised learning, each \n",
    "         example is a pair consisting of an input object \n",
    "         (typically a vector) and a desired output value (also \n",
    "         called the supervisory signal). A supervised learning \n",
    "         algorithm analyzes the training data and produces an \n",
    "         inferred function, which can be used for mapping new \n",
    "         examples. An optimal scenario will allow for the algorithm \n",
    "         to correctly determine the class labels for unseen \n",
    "         instances. This requires the learning algorithm to  \n",
    "         generalize from the training data to unseen situations \n",
    "         in a 'reasonable' way (see inductive bias).\n",
    "      \"\"\"\n",
    "\n",
    "\n",
    "n_gram_range = (1, 3)\n",
    "stop_words = \"english\"\n",
    "\n",
    "# Extract candidate words/phrases\n",
    "count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([doc])\n",
    "candidates = count.get_feature_names()\n",
    "candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.2811013 , -0.19414134,  0.18961014, ..., -0.6902669 ,\n",
       "        -0.08735337, -0.05631514],\n",
       "       [-0.98599565, -0.3114873 ,  0.11392305, ..., -0.5420821 ,\n",
       "         0.03869215,  0.04444062],\n",
       "       [-0.7794865 , -0.3172609 ,  0.30012545, ..., -0.44612113,\n",
       "         0.09859169,  0.41761875],\n",
       "       ...,\n",
       "       [-0.35578147,  0.01782149,  0.374094  , ..., -0.1977843 ,\n",
       "         0.0460116 , -0.29936978],\n",
       "       [-0.7603124 , -0.5274268 ,  0.29681763, ..., -0.3508133 ,\n",
       "         0.16764224, -0.566111  ],\n",
       "       [-0.6121529 , -0.44180587, -0.27497974, ..., -0.31791598,\n",
       "         0.22374944, -0.78795713]], dtype=float32)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Next, we convert both the document as well as the candidate keywords/keyphrases to numerical data.\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "doc_embedding = model.encode([doc])\n",
    "candidate_embeddings = model.encode(candidates)\n",
    "candidate_embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are Distilbert as it has shown great performance in similarity tasks, which is what we are aiming for with keyword/keyphrase extraction!\n",
    "\n",
    "Since transformer models have a token limit, you might run into some errors when inputting large documents. In that case, you could consider splitting up your document into paragraphs and mean pooling (taking the average of) the resulting vectors.\n",
    "\n",
    "In the final step, we want to find the candidates that are most similar to the document. We assume that the most similar candidates to the document are good keywords/keyphrases for representing the document.\n",
    "\n",
    "To calculate the similarity between candidates and the document, we will be using the cosine similarity between vectors as it performs quite well in high-dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['algorithm analyzes training',\n",
       " 'learning algorithm generalize',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "top_n = 5\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reason why similar results are returned they best represent the document! If we were to diversify the keywords/keyphrases then they are less likely to represent the document well as a collective.\n",
    "\n",
    "Thus, the diversification of our results requires a delicate balance between the accuracy of keywords/keyphrases and the diversity between them.\n",
    "\n",
    "There are two algorithms that we will be using to diversify our results:\n",
    "\n",
    "Max Sum Similarity\n",
    "Maximal Marginal Relevance\n",
    "Max Sum Similarity\n",
    "The maximum sum distance between pairs of data is defined as the pairs of data for which the distance between them is maximized. In our case, we want to maximize the candidate similarity to the document whilst minimizing the similarity between candidates.\n",
    "\n",
    "To do this, we select the top 20 keywords/keyphrases, and from those 20, select the 5 that are the least similar to each other:\n",
    "https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "    distances_candidates = cosine_similarity(candidate_embeddings, \n",
    "                                            candidate_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [candidates[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['maps', 'input', 'class', 'training', 'algorithm']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal Marginal Relevance\n",
    "The final method for diversifying our results is Maximal Marginal Relevance (MMR). MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. Fortunately, a keyword extraction algorithm called EmbedRank has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases.\n",
    "\n",
    "We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords/keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['learning', 'algorithm', 'training', 'class', 'mapping']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/MaartenGr/KeyBERT/blob/master/keybert/_model.py\n",
    "https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea\n",
    "\n",
    "from transformers import DistilBertModel\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class DistilBERT_Model(nn.Module):\n",
    " def __init__(self, classes):\n",
    "   super(DistilBERT_Model, self).__init__()\n",
    "   self.distilbert = DistilBertModel.from_pretrained('distilbert\n",
    "                                                     base-uncased')\n",
    "   self.out = nn.Linear(self.distilbert.config.hidden_size, classes)\n",
    "   self.sigmoid = nn.Sigmoid()\n",
    " def forward(self, input, attention_mask):\n",
    "   _, output = self.distilbert(input, attention_mask \n",
    "                                      = attention_mask)\n",
    "   out = self.sigmoid(self.out(output))\n",
    "   return out"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
