{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Personalizing biobert"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from ast import literal_eval\n",
    "from itertools import chain\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "from tqdm.notebook import tqdm, trange\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import torch\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from tqdm.autonotebook import tqdm\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import BertTokenizer, BertConfig\n",
    "\n",
    "#from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import FunctionTransformer#, ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with complaint of allergies.</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.  He exercises three times a week at home and does cardio.  He has difficulty walking two blocks or five flights of stairs.  Difficulty with snoring.  He has muscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling.  He has gastroesophageal reflux disease.,PAST SURGICAL HISTORY:, Includes reconstructive surgery on his right hand 13 years ago.  ,SOCIAL HISTORY:, He is currently single.  He has about ten drinks a year.  He had smoked significantly up until several months ago.  He now smokes less than three cigarettes a day.,FAMILY HISTORY:, Heart disease in both grandfathers, grandmother with stroke, and a grandmother with diabetes.  Denies obesity and hypertension in other family members.,CURRENT MEDICATIONS:, None.,ALLERGIES:,  He is allergic to Penicillin.,MISCELLANEOUS/EATING HISTORY:, He has been going to support groups for seven months with Lynn Holmberg in Greenwich and he is from Eastchester, New York and he feels that we are the appropriate program.  He had a poor experience with the Greenwich program.  Eating history, he is not an emotional eater.  Does not like sweets.  He likes big portions and carbohydrates.  He likes chicken and not steak.  He currently weighs 312 pounds.  Ideal body weight would be 170 pounds.  He is 142 pounds overweight.  If ,he lost 60% of his excess body weight that would be 84 pounds and he should weigh about 228.,REVIEW OF SYSTEMS: ,Negative for head, neck, heart, lungs, GI, GU, orthopedic, and skin.  Specifically denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, high cholesterol, pulmonary embolism, high blood pressure, CVA, venous insufficiency, thrombophlebitis, asthma, shortness of breath, COPD, emphysema, sleep apnea, diabetes, leg and foot swelling, osteoarthritis, rheumatoid arthritis, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, hemorrhoids, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  Denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:, He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Afebrile.  Vital Signs are stable.</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weight loss programs, gastric bypass, atkin's diet, weight watcher's, body weight, laparoscopic gastric, weight loss, pounds, months, weight, laparoscopic, band, loss, diets, overweight, lost</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "\n",
       "                                                         description  \\\n",
       "0   A 23-year-old white female presents with complaint of allergies.   \n",
       "1                           Consult for laparoscopic gastric bypass.   \n",
       "\n",
       "       medical_specialty                                sample_name  \\\n",
       "0   Allergy / Immunology                         Allergic Rhinitis    \n",
       "1             Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     transcription  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              SUBJECTIVE:,  This 23-year-old white female presents with complaint of allergies.  She used to have allergies when she lived in Seattle but she thinks they are worse here.  In the past, she has tried Claritin, and Zyrtec.  Both worked for short time but then seemed to lose effectiveness.  She has used Allegra also.  She used that last summer and she began using it again two weeks ago.  It does not appear to be working very well.  She has used over-the-counter sprays but no prescription nasal sprays.  She does have asthma but doest not require daily medication for this and does not think it is flaring up.,MEDICATIONS: , Her only medication currently is Ortho Tri-Cyclen and the Allegra.,ALLERGIES: , She has no known medicine allergies.,OBJECTIVE:,Vitals:  Weight was 130 pounds and blood pressure 124/78.,HEENT:  Her throat was mildly erythematous without exudate.  Nasal mucosa was erythematous and swollen.  Only clear drainage was seen.  TMs were clear.,Neck:  Supple without adenopathy.,Lungs:  Clear.,ASSESSMENT:,  Allergic rhinitis.,PLAN:,1.  She will try Zyrtec instead of Allegra again.  Another option will be to use loratadine.  She does not think she has prescription coverage so that might be cheaper.,2.  Samples of Nasonex two sprays in each nostril given for three weeks.  A prescription was written as well.   \n",
       "1  PAST MEDICAL HISTORY:, He has difficulty climbing stairs, difficulty with airline seats, tying shoes, used to public seating, and lifting objects off the floor.  He exercises three times a week at home and does cardio.  He has difficulty walking two blocks or five flights of stairs.  Difficulty with snoring.  He has muscle and joint pains including knee pain, back pain, foot and ankle pain, and swelling.  He has gastroesophageal reflux disease.,PAST SURGICAL HISTORY:, Includes reconstructive surgery on his right hand 13 years ago.  ,SOCIAL HISTORY:, He is currently single.  He has about ten drinks a year.  He had smoked significantly up until several months ago.  He now smokes less than three cigarettes a day.,FAMILY HISTORY:, Heart disease in both grandfathers, grandmother with stroke, and a grandmother with diabetes.  Denies obesity and hypertension in other family members.,CURRENT MEDICATIONS:, None.,ALLERGIES:,  He is allergic to Penicillin.,MISCELLANEOUS/EATING HISTORY:, He has been going to support groups for seven months with Lynn Holmberg in Greenwich and he is from Eastchester, New York and he feels that we are the appropriate program.  He had a poor experience with the Greenwich program.  Eating history, he is not an emotional eater.  Does not like sweets.  He likes big portions and carbohydrates.  He likes chicken and not steak.  He currently weighs 312 pounds.  Ideal body weight would be 170 pounds.  He is 142 pounds overweight.  If ,he lost 60% of his excess body weight that would be 84 pounds and he should weigh about 228.,REVIEW OF SYSTEMS: ,Negative for head, neck, heart, lungs, GI, GU, orthopedic, and skin.  Specifically denies chest pain, heart attack, coronary artery disease, congestive heart failure, arrhythmia, atrial fibrillation, pacemaker, high cholesterol, pulmonary embolism, high blood pressure, CVA, venous insufficiency, thrombophlebitis, asthma, shortness of breath, COPD, emphysema, sleep apnea, diabetes, leg and foot swelling, osteoarthritis, rheumatoid arthritis, hiatal hernia, peptic ulcer disease, gallstones, infected gallbladder, pancreatitis, fatty liver, hepatitis, hemorrhoids, rectal bleeding, polyps, incontinence of stool, urinary stress incontinence, or cancer.  Denies cellulitis, pseudotumor cerebri, meningitis, or encephalitis.,PHYSICAL EXAMINATION:, He is alert and oriented x 3.  Cranial nerves II-XII are intact.  Afebrile.  Vital Signs are stable.   \n",
       "\n",
       "                                                                                                                                                                                                                                   keywords  \n",
       "0                                                                                                       allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,  \n",
       "1  bariatrics, laparoscopic gastric bypass, weight loss programs, gastric bypass, atkin's diet, weight watcher's, body weight, laparoscopic gastric, weight loss, pounds, months, weight, laparoscopic, band, loss, diets, overweight, lost  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "import pandas as pd\n",
    "df = pd.read_csv('../data/raw/mtsamples.csv')\n",
    "df.transcription=df.transcription.astype(str)\n",
    "#print(df.columns)\n",
    "df_test = df.head(15)\n",
    "df_test.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/2744472210.py:31: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test[\"transcription\"] = df_test[\"transcription\"].apply(cleaning)\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "def cleaning(sentence):\n",
    "    '''This function takes a column and cleans it by removing punctuation, stopwords, and lemmatizing\n",
    "    NEXT STEPS : personalize the stop words list, check for different aspects of the words\n",
    "    '''\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence\n",
    "df_test[\"transcription\"] = df_test[\"transcription\"].apply(cleaning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/3971471173.py:5: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['keywords']=df_test['keywords'].fillna(\"\")\n",
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/3971471173.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['keywords_list']= df_test['keywords'].apply(lambda x: x.split(','))\n"
     ]
    }
   ],
   "source": [
    "def convert(lst_kw):\n",
    "    '''This function converts the keywords to a list of keywords'''\n",
    "    list_keywords = lst_kw.split(',')\n",
    "    return list_keywords\n",
    "df_test['keywords']=df_test['keywords'].fillna(\"\")\n",
    "df_test['keywords_list']= df_test['keywords'].apply(lambda x: x.split(','))\n",
    "# NEXT STEP: clean keyowrds - lemmatize, lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/2437295253.py:14: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['location'] = df_test.apply(lambda x: location_indices(x.transcription, x.keywords_list), axis=1)\n"
     ]
    }
   ],
   "source": [
    "def location_indices(stringstext, check_list):\n",
    "    '''this function finds the location of the keywords in the string'''\n",
    "    \n",
    "    res = dict()\n",
    "    for ele in check_list :\n",
    "        if ele in stringstext:\n",
    "            # getting front index\n",
    "            strt = stringstext.index(ele)\n",
    "            \n",
    "            # getting ending index\n",
    "            res[ele] = [strt, strt + len(ele) - 1]\n",
    "    return res.values()\n",
    "        \n",
    "df_test['location'] = df_test.apply(lambda x: location_indices(x.transcription, x.keywords_list), axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "      <th>location</th>\n",
       "      <th>kfold</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with complaint of allergies.</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>subjective yearold white female present complaint allergies use allergies live seattle think worse past try claritin zyrtec work short time seem lose effectiveness use allegra also use last summer begin use two weeks ago appear work well use overthecounter spray prescription nasal spray asthma doest require daily medication think flare upmedications medication currently ortho tricyclen allegraallergies know medicine allergiesobjectivevitals weight pound blood pressure heent throat mildly erythematous without exudate nasal mucosa erythematous swell clear drainage see tms clearneck supple without adenopathylungs clearassessment allergic rhinitisplan try zyrtec instead allegra another option use loratadine think prescription coverage might cheaper sample nasonex two spray nostril give three weeks prescription write well</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,</td>\n",
       "      <td>[allergy / immunology,  allergic rhinitis,  allergies,  asthma,  nasal sprays,  rhinitis,  nasal,  erythematous,  allegra,  sprays,  allergic, ]</td>\n",
       "      <td>([633, 650], [49, 58], [287, 293], [642, 650], [275, 280], [492, 504], [167, 174], [633, 641], [0, -1])</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "\n",
       "                                                         description  \\\n",
       "0   A 23-year-old white female presents with complaint of allergies.   \n",
       "\n",
       "       medical_specialty          sample_name  \\\n",
       "0   Allergy / Immunology   Allergic Rhinitis    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  transcription  \\\n",
       "0  subjective yearold white female present complaint allergies use allergies live seattle think worse past try claritin zyrtec work short time seem lose effectiveness use allegra also use last summer begin use two weeks ago appear work well use overthecounter spray prescription nasal spray asthma doest require daily medication think flare upmedications medication currently ortho tricyclen allegraallergies know medicine allergiesobjectivevitals weight pound blood pressure heent throat mildly erythematous without exudate nasal mucosa erythematous swell clear drainage see tms clearneck supple without adenopathylungs clearassessment allergic rhinitisplan try zyrtec instead allegra another option use loratadine think prescription coverage might cheaper sample nasonex two spray nostril give three weeks prescription write well   \n",
       "\n",
       "                                                                                                                              keywords  \\\n",
       "0  allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,   \n",
       "\n",
       "                                                                                                                                      keywords_list  \\\n",
       "0  [allergy / immunology,  allergic rhinitis,  allergies,  asthma,  nasal sprays,  rhinitis,  nasal,  erythematous,  allegra,  sprays,  allergic, ]   \n",
       "\n",
       "                                                                                                  location  \\\n",
       "0  ([633, 650], [49, 58], [287, 293], [642, 650], [275, 280], [492, 504], [167, 174], [633, 641], [0, -1])   \n",
       "\n",
       "   kfold  \n",
       "0      0  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Kfold\n",
    "import numpy as np\n",
    "frames = []\n",
    "df_split = np.array_split(df_test, 5)\n",
    "for split in range(0, 5):\n",
    "    df_split[split]['kfold'] = split\n",
    "    frames.append(df_split[split])\n",
    "dfx = pd.concat(frames)\n",
    "dfx\n",
    "\n",
    "#find max len der texte\n",
    "max_len = df_test['transcription'].map(lambda x: len(x)).max()\n",
    "max_len\n",
    "pd.set_option('display.max_colwidth', None)\n",
    "dfx.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#configuration\n",
    "\n",
    "class config:\n",
    "    MAX_LEN = 42 # 416\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE = 8\n",
    "    EPOCHS = 5\n",
    "    \n",
    "    # BERT_PATH = \"../input/bert-base-uncased/\" \n",
    "    # MODEL_PATH = \"model.bin\"\n",
    "    # TOKENIZER = tokenizers.BertWordPieceTokenizer(f\"{BERT_PATH}/vocab.txt\" ,lowercase = True)\n",
    "    \n",
    "    BERT_PATH = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "    MODEL_PATH = 'emilyalsentzer/Bio_ClinicalBERT'\n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    DROPOUT = 0.2\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    LEARNING_RATE = 1e-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loc_list_to_ints(loc_list):\n",
    "    tuples = []\n",
    "    for sublist in loc_list:\n",
    "        tuples.append(tuple(sublist))\n",
    "    return tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_data_tokenize(feature_text, annotation, location, tokenizer, max_len):    ##X , Y, selected_text  \n",
    "    '''this function tokenizes the data'''\n",
    "    location_list = loc_list_to_ints(location)        \n",
    "    char_targets = [0] * len(feature_text) #creating empty list(all zeros) of character;it will be made 1 if annotation in text   \n",
    "    for loc,anno in zip(location_list,annotation): \n",
    "        len_st = loc[1] - loc[0]\n",
    "\n",
    "        idx0 = None\n",
    "        idx1 = None\n",
    "        for ind in (i for i, e in enumerate(feature_text) if (e == anno[0] and i == loc[0])):\n",
    "            if feature_text[ind: ind+len_st] == anno:\n",
    "\n",
    "                idx0 = ind\n",
    "                idx1 = ind + len_st - 1\n",
    "                if idx0 != None and idx1 != None:\n",
    "                    for ct in range(idx0, idx1 + 1):\n",
    "                        char_targets[ct] = 1  #replacing zeros with 1 if that part of the text is selected text\n",
    "        \n",
    "                break\n",
    "    print('in process function')\n",
    "    tokenized_input = tokenizer.encode_plus(feature_text,\n",
    "                                            add_special_tokens=True, # this ads tokens like [CLS]- end and [SEP]- start \n",
    "                                            return_attention_mask=True, \n",
    "                                            return_offsets_mapping=True,\n",
    "                                            padding =  'max_length',\n",
    "                                            truncation = True,\n",
    "                                            max_length=42,\n",
    "                                            return_tensors = 'pt')\n",
    "    \n",
    "    #input_ids = tokenized_input['input_ids'].flatten()\n",
    "    input_ids = tokenized_input.get('input_ids')\n",
    "    mask = tokenized_input.get('attention_mask')\n",
    "    token_type_ids = tokenized_input.get('token_type_ids')\n",
    "    offsets = tokenized_input.get('offset_mapping')\n",
    "    \n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    mask = torch.cat(mask, dim=0)\n",
    "    token_type_ids = torch.cat(token_type_ids, dim=0)\n",
    "    offsets = torch.cat(offsets, dim=0)\n",
    "    # input ids - id representation of each token - could decode by tokenizer.decode(input_ids[0])\n",
    "    # tokentype ids - 0 for first sentence, 1 for second sentence\n",
    "    # attention mask - 1 for tokens that are not [PAD] and 0 for [PAD] tokens\n",
    "    \n",
    "    #print('input_ids.specific', input_ids[0])\n",
    "    print('input_ids.shape', input_ids.shape)\n",
    "    \n",
    "    \n",
    "    target_idx = []\n",
    "    for value in offsets:\n",
    "        for j, (offset1, offset2) in enumerate(value):\n",
    "            if sum(char_targets[offset1: offset2]) > 0:\n",
    "                target_idx.append(j)\n",
    "\n",
    "    #creating label\n",
    "    ignore_idxes = np.where(np.array(token_type_ids) != 1)[0]\n",
    "    label = np.zeros(offsets.size())#, dtype=np.float32)]))\n",
    "    label[ignore_idxes] = -1\n",
    "    label[target_idx] = 1\n",
    "    print('label', label)\n",
    "    \n",
    "    return {\n",
    "    'ids': input_ids,\n",
    "    'mask': mask,\n",
    "    'token_type_ids': token_type_ids,\n",
    "    'labels': label,\n",
    "    'offsets': offsets\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'list'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 16\u001b[0m in \u001b[0;36m<cell line: 19>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m df_traindict \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mtranscription,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mkeywords_list\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#list(df_traindict['feature_text'])\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output_train \u001b[39m=\u001b[39m process_data_tokenize(\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mfeature_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mannotation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),config\u001b[39m.\u001b[39;49mTOKENIZER,config\u001b[39m.\u001b[39;49mMAX_LEN)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m output_train\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 16\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m char_targets \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(feature_text) \u001b[39m#creating empty list(all zeros) of character;it will be made 1 if annotation in text   \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc,anno \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(location_list,annotation): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     len_st \u001b[39m=\u001b[39m loc[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39;49m loc[\u001b[39m0\u001b[39;49m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     idx0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X20sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     idx1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'list'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "#output = process_data_tokenize(example[\"feature_text\"],example[\"annotation\"],example[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "\n",
    "#for key in output.keys():\n",
    "#    print(key)\n",
    "#    print(output[key])\n",
    "#    print(\"=\" * 100)\n",
    "    \n",
    "#output\n",
    "\n",
    "\n",
    "df_test.head(1)\n",
    "df_traindict = {\n",
    "    \"feature_text\": df_test.transcription,\n",
    "    \"location\": df_test.location,\n",
    "    \"annotation\": df_test.keywords_list\n",
    "}\n",
    "#list(df_traindict['feature_text'])\n",
    "output_train = process_data_tokenize(list(df_traindict[\"feature_text\"]),list(df_traindict[\"annotation\"]),list(df_traindict[\"location\"]),config.TOKENIZER,config.MAX_LEN)\n",
    "#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "output_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loader\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "class NBMEDataset(Dataset):\n",
    "    def __init__(self,  feature_text, annotation, location):   #text(X) #label(Y), #selected_text #start:end\n",
    "        #self.pn_history = pn_history\n",
    "        self.feature_text = list(feature_text)\n",
    "        self.annotation = list(annotation)\n",
    "        self.location = list(location)\n",
    "        self.tokenizer = config.TOKENIZER\n",
    "        self.max_len = config.MAX_LEN\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.feature_text)\n",
    "        \n",
    "    def __getitem__(self, item):\n",
    "        data = process_data_tokenize(\n",
    "            #self.pn_history[item],\n",
    "            self.feature_text[item],\n",
    "            self.annotation[item],\n",
    "            self.location[item],\n",
    "            self.tokenizer,\n",
    "            self.max_len\n",
    "        )\n",
    "        return {\n",
    "            'ids': torch.tensor(data[\"ids\"]), #input_ids\n",
    "            'mask': torch.tensor(data[\"mask\"], dtype=torch.long), #attention_mask\n",
    "            'token_type_ids': torch.tensor(data[\"token_type_ids\"], dtype=torch.long), #segment_ids\n",
    "            'labels': torch.tensor(data[\"labels\"], dtype=torch.long), \n",
    "            'offsets': torch.tensor(data[\"offsets\"], dtype=torch.long)\n",
    "        }\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "class NBMEModel(transformers.BertPreTrainedModel):    #torch.nn.Module\n",
    "    def __init__(self,conf):\n",
    "        super(NBMEModel,self).__init__(conf)\n",
    "        self.bert = transformers.BertModel.from_pretrained(config.BERT_PATH, config=conf)\n",
    "        self.dropout = torch.nn.Dropout(config.DROPOUT)\n",
    "        self.classifier = torch.nn.Linear(768, 1)\n",
    "        torch.nn.init.normal_(self.classifier.weight, std=0.02) \n",
    "        \n",
    "    def forward(self, ids, mask, token_type_ids):\n",
    "        sequence_out = self.bert(ids, attention_mask=mask, token_type_ids=token_type_ids)[0] #last_hidden_state\n",
    "        batch_size, max_len, feat_dim = sequence_out.shape\n",
    "        sequence_output = self.dropout(sequence_out)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        logits = logits.squeeze(-1) \n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#utility function\n",
    "class AverageMeter:\n",
    "    \"\"\"\n",
    "    Computes and stores the average and current value\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "        \n",
    "#loss function\n",
    "def loss_fn(logits, labels):\n",
    "    print('in loss function')\n",
    "    #labels = labels.unsqueeze(1)\n",
    "    loss_fct = torch.nn.BCEWithLogitsLoss(reduction = \"none\")\n",
    "    loss = loss_fct(logits,labels)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in a next step we use data loader to load the data - https://blog.paperspace.com/dataloaders-abstractions-pytorch/\n",
    "# in a next step with the help of the optimizer we will update the weights of the model\n",
    "#training function\n",
    "\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "def train_fn(dataloader, model, optimizer, scheduler=None): #before: dataloader\n",
    "    model.train()\n",
    "    losses = AverageMeter() \n",
    "    print('in function train_fn')\n",
    "\n",
    "    for batch_idx, batch in enumerate(tqdm(dataloader, total=len(dataloader))):\n",
    "        for k, v in batch.items():\n",
    "            batch[k] = v.to(DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        #ids = batch['ids'].squeeze(1)\n",
    "    \n",
    "        #output = torch.squeeze(input)\n",
    "        logits = model(ids=batch['ids'], mask=batch['mask'], token_type_ids=batch['token_type_ids'])\n",
    "        #model(ids=batch['ids'], mask=batch['mask'], token_type_ids=batch['token_type_ids'])\n",
    "        loss = loss_fn(logits,batch[\"labels\"])\n",
    "        loss = loss.mean()\n",
    "        # loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "        # losses.update(loss.item(),ids.size(0))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "        losses.update(loss.item(), batch['ids'].size(0))\n",
    "        dataloader.set_postfix(loss=losses.avg) #tk. bfefore\n",
    "        #or like this - https://www.kaggle.com/code/daisybbb/pytorch-pubmedbert-infer\n",
    "\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), config.MAX_GRAD_NORM)\n",
    "        optimizer.step()\n",
    "        scheduler.step() ## Update learning rate schedule\n",
    "    \n",
    "    # #output = torch.argmax(torch.softmax(logits, dim=2),dim=2).cpu().detach().numpy()\n",
    "        tqdm(dataloader, total=len(dataloader)).set_postfix(loss=losses.avg)\n",
    "        \n",
    "    return losses.avg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 25\u001b[0m in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m df_traindict \u001b[39m=\u001b[39m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mtranscription,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m    \u001b[39m\"\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mkeywords_list\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m }\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m \u001b[39mlist\u001b[39m(df_traindict[\u001b[39m'\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m output_train \u001b[39m=\u001b[39m process_data_tokenize(\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mfeature_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mannotation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),config\u001b[39m.\u001b[39;49mTOKENIZER,config\u001b[39m.\u001b[39;49mMAX_LEN)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m \u001b[39m#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m output_train\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 25\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m char_targets \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(feature_text) \u001b[39m#creating empty list(all zeros) of character;it will be made 1 if annotation in text   \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc,anno \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(location_list,annotation): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     len_st \u001b[39m=\u001b[39m loc[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m loc[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     idx0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X23sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     idx1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    " # train model\n",
    " #adam w - https://medium.com/@nishantnikhil/adam-optimizer-notes-ddac4fd7218\n",
    " #other optimizer https://medium.com/geekculture/a-2021-guide-to-improving-cnns-optimizers-adam-vs-sgd-495848ac6008\n",
    " #https://firiuza.medium.com/optimizers-for-training-neural-networks-e0196662e21e\n",
    " #https://medium.com/@ThiyaneshwaranG/types-of-optimizers-in-deep-learning-from-gradient-descent-to-adam-1572e657c8c5\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "\n",
    "#-------------------\n",
    "df_test.head(1)\n",
    "df_traindict = {\n",
    "    \"feature_text\": df_test.transcription,\n",
    "    \"location\": df_test.location,\n",
    "    \"annotation\": df_test.keywords_list\n",
    "}\n",
    "list(df_traindict['feature_text'])\n",
    "output_train = process_data_tokenize(list(df_traindict[\"feature_text\"]),list(df_traindict[\"annotation\"]),list(df_traindict[\"location\"]),config.TOKENIZER,config.MAX_LEN)\n",
    "#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "output_train\n",
    "\n",
    "\n",
    "\n",
    "#-----------------#\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "model = NBMEModel(conf=model_config)\n",
    "model.to(DEVICE)\n",
    "    \n",
    "#num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "param_optimizer = list(model.named_parameters())\n",
    "#print('param_optimizer', param_optimizer)\n",
    "no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "\n",
    "optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "optimizer = AdamW(optimizer_parameters, lr=3e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=2\n",
    "    )\n",
    "#-----------------#\n",
    "train_loss = train_fn(x, model, optimizer, scheduler=scheduler)\n",
    "#train_loss_data.append(train_loss)\n",
    "print(f\"Train loss: {train_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## evaluation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#evaluation function\n",
    "def eval_fn(dataloader, model): #before: dataloader\n",
    "    model.eval()\n",
    "    losses = AverageMeter() # Computes and stores the average and current value\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tk = tqdm(dataloader, total=len(dataloader)) \n",
    "        #tk = tqdm(data, total=5)\n",
    "        \n",
    "        # ids = data['ids']\n",
    "        # token_type_ids = data[\"token_type_ids\"]\n",
    "        # mask = data[\"mask\"]\n",
    "        # labels = data[\"labels\"]\n",
    "        # offsets = data[\"offsets\"]#    \n",
    "        # labels = labels.unsqueeze(0)\n",
    "\n",
    "        for batch, data in enumerate(tk):\n",
    "\n",
    "            ids = data['ids']\n",
    "            token_type_ids = data[\"token_type_ids\"]\n",
    "            mask = data[\"mask\"]\n",
    "            labels = data[\"labels\"]\n",
    "            offsets = data[\"offsets\"]\n",
    "\n",
    "            ids = ids.to(DEVICE, dtype=torch.long)\n",
    "            token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n",
    "            mask = mask.to(DEVICE, dtype=torch.long)\n",
    "            labels = labels.to(DEVICE, dtype=torch.float64)\n",
    "\n",
    "            logits = model(ids=ids, mask=mask, token_type_ids=token_type_ids) #last_hidden_state\n",
    "            \n",
    "            loss = loss_fn(logits, labels)\n",
    "            loss = torch.masked_select(loss, labels > -1.0).mean()\n",
    "            losses.update(loss.item(),ids.size(0))\n",
    "            tk.set_postfix(loss=losses.avg)\n",
    "        \n",
    "        return losses.avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_values' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 20\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_traindict \u001b[39m=\u001b[39m {\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mtranscription,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mlocation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mlocation,\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mannotation\u001b[39m\u001b[39m\"\u001b[39m: df_test\u001b[39m.\u001b[39mkeywords_list\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m }\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mlist\u001b[39m(df_traindict[\u001b[39m'\u001b[39m\u001b[39mfeature_text\u001b[39m\u001b[39m'\u001b[39m])\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m output_train \u001b[39m=\u001b[39m process_data_tokenize(\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mfeature_text\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mannotation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),\u001b[39mlist\u001b[39;49m(df_traindict[\u001b[39m\"\u001b[39;49m\u001b[39mlocation\u001b[39;49m\u001b[39m\"\u001b[39;49m]),config\u001b[39m.\u001b[39;49mTOKENIZER,config\u001b[39m.\u001b[39;49mMAX_LEN)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m output_train\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 20\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m char_targets \u001b[39m=\u001b[39m [\u001b[39m0\u001b[39m] \u001b[39m*\u001b[39m \u001b[39mlen\u001b[39m(feature_text) \u001b[39m#creating empty list(all zeros) of character;it will be made 1 if annotation in text   \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m loc,anno \u001b[39min\u001b[39;00m \u001b[39mzip\u001b[39m(location_list,annotation): \n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     len_st \u001b[39m=\u001b[39m loc[\u001b[39m1\u001b[39;49m] \u001b[39m-\u001b[39m loc[\u001b[39m0\u001b[39m]\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     idx0 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#Y123sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     idx1 \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_values' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "df_test.head(1)\n",
    "df_traindict = {\n",
    "    \"feature_text\": df_test.transcription,\n",
    "    \"location\": df_test.location,\n",
    "    \"annotation\": df_test.keywords_list\n",
    "}\n",
    "list(df_traindict['feature_text'])\n",
    "output_train = process_data_tokenize(list(df_traindict[\"feature_text\"]),list(df_traindict[\"annotation\"]),list(df_traindict[\"location\"]),config.TOKENIZER,config.MAX_LEN)\n",
    "#output_train = process_data_tokenize(df_traindict[\"feature_text\"],df_traindict[\"annotation\"],df_traindict[\"location\"],config.TOKENIZER,config.MAX_LEN)\n",
    "output_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/5\n",
      "in function train_fn\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ac76882a7f14d429d78f37098044db1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in process function\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 29\u001b[0m in \u001b[0;36m<cell line: 87>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m         torch\u001b[39m.\u001b[39msave(model\u001b[39m.\u001b[39mstate_dict(), \u001b[39m\"\u001b[39m\u001b[39mmodel_fold1.bin\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m         \u001b[39m#time_elapsed = time.time() - since\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m         \u001b[39m#print('Training completed in {:.0f}m {:.0f}s'.format(\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=83'>84</a>\u001b[0m         \u001b[39m#    time_elapsed // 60, time_elapsed % 60))\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=86'>87</a>\u001b[0m run(fold\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 29\u001b[0m in \u001b[0;36mrun\u001b[0;34m(fold)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m/\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(i \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m, config\u001b[39m.\u001b[39mEPOCHS))\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=64'>65</a>\u001b[0m \u001b[39m# train model\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m train_loss \u001b[39m=\u001b[39m train_fn(train_data_loader\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=66'>67</a>\u001b[0m                       , model, optimizer, scheduler\u001b[39m=\u001b[39;49mscheduler)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=67'>68</a>\u001b[0m train_loss_data\u001b[39m.\u001b[39mappend(train_loss)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=68'>69</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mTrain loss: \u001b[39m\u001b[39m{\u001b[39;00mtrain_loss\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 29\u001b[0m in \u001b[0;36mtrain_fn\u001b[0;34m(dataloader, model, optimizer, scheduler)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m losses \u001b[39m=\u001b[39m AverageMeter() \n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m'\u001b[39m\u001b[39min function train_fn\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch_idx, batch \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(tqdm(dataloader, total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataloader))):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m batch\u001b[39m.\u001b[39mitems():\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m         batch[k] \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mto(DEVICE)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/notebook.py:259\u001b[0m, in \u001b[0;36mtqdm_notebook.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    258\u001b[0m     it \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39m(tqdm_notebook, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__iter__\u001b[39m()\n\u001b[0;32m--> 259\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m it:\n\u001b[1;32m    260\u001b[0m         \u001b[39m# return super(tqdm...) will not catch exception\u001b[39;00m\n\u001b[1;32m    261\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m    262\u001b[0m \u001b[39m# NB: except ... [ as ...] breaks IPython async KeyboardInterrupt\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/std.py:1195\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1192\u001b[0m time \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_time\n\u001b[1;32m   1194\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1195\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1196\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1197\u001b[0m         \u001b[39m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1198\u001b[0m         \u001b[39m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:681\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    679\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    680\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 681\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[1;32m    682\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m    683\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    684\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[1;32m    685\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/utils/data/dataloader.py:721\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m    720\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 721\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    722\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[1;32m    723\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:49\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfetch\u001b[39m(\u001b[39mself\u001b[39m, possibly_batched_index):\n\u001b[1;32m     48\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mauto_collation:\n\u001b[0;32m---> 49\u001b[0m         data \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdataset[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     50\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m         data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 29\u001b[0m in \u001b[0;36mNBMEDataset.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m, item):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     data \u001b[39m=\u001b[39m process_data_tokenize(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m         \u001b[39m#self.pn_history[item],\u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_text[item],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mannotation[item],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlocation[item],\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtokenizer,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmax_len\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m {\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mids\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m\"\u001b[39m\u001b[39mids\u001b[39m\u001b[39m\"\u001b[39m]), \u001b[39m#input_ids\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m\"\u001b[39m\u001b[39mmask\u001b[39m\u001b[39m\"\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong), \u001b[39m#attention_mask\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m         \u001b[39m'\u001b[39m\u001b[39moffsets\u001b[39m\u001b[39m'\u001b[39m: torch\u001b[39m.\u001b[39mtensor(data[\u001b[39m\"\u001b[39m\u001b[39moffsets\u001b[39m\u001b[39m\"\u001b[39m], dtype\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mlong)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m     }\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb Cell 29\u001b[0m in \u001b[0;36mprocess_data_tokenize\u001b[0;34m(feature_text, annotation, location, tokenizer, max_len)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m token_type_ids \u001b[39m=\u001b[39m tokenized_input\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=42'>43</a>\u001b[0m offsets \u001b[39m=\u001b[39m tokenized_input\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39moffset_mapping\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=44'>45</a>\u001b[0m input_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mcat(input_ids, dim\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=45'>46</a>\u001b[0m mask \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(mask, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/src/BioBert_v01.ipynb#X36sZmlsZQ%3D%3D?line=46'>47</a>\u001b[0m token_type_ids \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat(token_type_ids, dim\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#training\n",
    "import time\n",
    "import numpy as np\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "def run(fold):\n",
    "    \n",
    "    train_loss_data, valid_loss_data = [], []\n",
    "    best_loss = np.inf\n",
    "    since = time.time()\n",
    "   \n",
    "    df_train = dfx[dfx.kfold != fold].reset_index(drop=True) \n",
    "    df_valid = dfx[dfx.kfold == fold].reset_index(drop=True)\n",
    "    \n",
    "    train_dataset = NBMEDataset(\n",
    "        feature_text=df_train.transcription.values,\n",
    "        annotation=df_train.keywords_list.values,\n",
    "        location=df_train.location.values\n",
    "        \n",
    "    )\n",
    "    \n",
    "    train_data_loader = torch.utils.data.DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=config.TRAIN_BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    valid_dataset = NBMEDataset(\n",
    "        feature_text=df_valid.transcription.values,\n",
    "        annotation=df_valid.keywords_list.values,\n",
    "        location=df_valid.location.values\n",
    "    )\n",
    "\n",
    "    valid_data_loader = torch.utils.data.DataLoader(\n",
    "        valid_dataset,\n",
    "        batch_size=config.VALID_BATCH_SIZE,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "    model_config.output_hidden_states = True\n",
    "    model = model = model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    # NBMEModel(conf=model_config)\n",
    "    model.to(DEVICE)\n",
    "    \n",
    "    num_train_steps = int(len(df_train) / config.TRAIN_BATCH_SIZE * config.EPOCHS)\n",
    "    param_optimizer = list(model.named_parameters())\n",
    "    no_decay = [\"bias\", \"LayerNorm.bias\", \"LayerNorm.weight\"]\n",
    "    optimizer_parameters = [\n",
    "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.001},\n",
    "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0},\n",
    "    ]\n",
    "    optimizer = AdamW(optimizer_parameters, lr=config.LEARNING_RATE)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, \n",
    "        num_warmup_steps=0, \n",
    "        num_training_steps=num_train_steps\n",
    "    )\n",
    "\n",
    "    best_loss = np.inf\n",
    "    \n",
    "    for i in range(config.EPOCHS):\n",
    "        print(\"Epoch: {}/{}\".format(i + 1, config.EPOCHS))\n",
    "\n",
    "        # train model\n",
    "        train_loss = train_fn(train_data_loader\n",
    "                              , model, optimizer, scheduler=scheduler)\n",
    "        train_loss_data.append(train_loss)\n",
    "        print(f\"Train loss: {train_loss}\")\n",
    "\n",
    "        # evaluate model - not now\n",
    "      #  valid_loss = eval_fn(valid_data_loader, model)\n",
    "      #  valid_loss_data.append(valid_loss)\n",
    "      #  print(f\"Valid loss: {valid_loss}\")\n",
    "\n",
    "\n",
    "       # if valid_loss < best_loss:\n",
    "        #    best_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"model_fold1.bin\")\n",
    "\n",
    "\n",
    "        #time_elapsed = time.time() - since\n",
    "        #print('Training completed in {:.0f}m {:.0f}s'.format(\n",
    "        #    time_elapsed // 60, time_elapsed % 60))\n",
    "    \n",
    "    \n",
    "run(fold=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keyword bert without personalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# doc = \"\"\"\n",
    "#          Supervised learning is the machine learning task of \n",
    "#          learning a function that maps an input to an output based \n",
    "#          on example input-output pairs.[1] It infers a function \n",
    "#          from labeled training data consisting of a set of \n",
    "#          training examples.[2] In supervised learning, each \n",
    "#          example is a pair consisting of an input object \n",
    "#          (typically a vector) and a desired output value (also \n",
    "#          called the supervisory signal). A supervised learning \n",
    "#          algorithm analyzes the training data and produces an \n",
    "#          inferred function, which can be used for mapping new \n",
    "#          examples. An optimal scenario will allow for the algorithm \n",
    "#          to correctly determine the class labels for unseen \n",
    "#          instances. This requires the learning algorithm to  \n",
    "#          generalize from the training data to unseen situations \n",
    "#          in a 'reasonable' way (see inductive bias).\n",
    "#       \"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def embed(input):\n",
    "      n_gram_range = (1, 3)\n",
    "      stop_words = \"english\"\n",
    "      # Extract candidate words/phrases - count vectorizer works with the time a word appears in the document\n",
    "      count = CountVectorizer(ngram_range=n_gram_range, stop_words=stop_words).fit([input])\n",
    "      candidates = count.get_feature_names()\n",
    "      #Next, we convert both the document as well as the candidate keywords/keyphrases to numerical data.\n",
    "      model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "      #this transofrms the document into a vector, and the next transforms the candidate words into vectors\n",
    "      doc_embedding = model.encode([input])\n",
    "      candidate_embeddings = model.encode(candidates)\n",
    "      return doc_embedding, candidate_embeddings, candidates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I use Distilbert as it has shown great performance in similarity tasks, which is what we are aiming for with keyword/keyphrase extraction!\n",
    "\n",
    "In the final step, we want to find the candidates that are most similar to the document. We assume that the most similar candidates to the document are good keywords/keyphrases for representing the document.\n",
    "\n",
    "To calculate the similarity between candidates and the document, we will be using the cosine similarity between vectors as it performs quite well in high-dimensionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['supervised learning example',\n",
       " 'analyzes training data',\n",
       " 'machine learning task',\n",
       " 'examples supervised learning',\n",
       " 'requires learning algorithm',\n",
       " 'signal supervised learning',\n",
       " 'learning algorithm',\n",
       " 'learning function maps',\n",
       " 'supervised learning machine',\n",
       " 'supervised learning algorithm',\n",
       " 'algorithm analyzes training',\n",
       " 'learning algorithm generalize',\n",
       " 'learning machine learning',\n",
       " 'learning algorithm analyzes',\n",
       " 'algorithm generalize training']"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "top_n = 15\n",
    "distances = cosine_similarity(doc_embedding, candidate_embeddings)\n",
    "keywords = [candidates[index] for index in distances.argsort()[0][-top_n:]]\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a reason why similar results are returned… they best represent the document! If we were to diversify the keywords/keyphrases then they are less likely to represent the document well as a collective.\n",
    "\n",
    "Thus, the diversification of our results requires a delicate balance between the accuracy of keywords/keyphrases and the diversity between them.\n",
    "\n",
    "There are two algorithms that we will be using to diversify our results:\n",
    "\n",
    "Max Sum Similarity\n",
    "Maximal Marginal Relevance\n",
    "Max Sum Similarity\n",
    "The maximum sum distance between pairs of data is defined as the pairs of data for which the distance between them is maximized. In our case, we want to maximize the candidate similarity to the document whilst minimizing the similarity between candidates.\n",
    "\n",
    "To do this, we select the top 20 keywords/keyphrases, and from those 20, select the 5 that are the least similar to each other:\n",
    "https://towardsdatascience.com/keyword-extraction-with-bert-724efca412ea"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import itertools\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "\n",
    "def max_sum_sim(doc_embedding, word_embeddings, words, top_n, nr_candidates):\n",
    "    # Calculate distances and extract keywords\n",
    "    distances = cosine_similarity(doc_embedding, word_embeddings)\n",
    "    distances_candidates = cosine_similarity(word_embeddings, \n",
    "                                            word_embeddings)\n",
    "\n",
    "    # Get top_n words as candidates based on cosine similarity\n",
    "    words_idx = list(distances.argsort()[0][-nr_candidates:])\n",
    "    words_vals = [words[index] for index in words_idx]\n",
    "    distances_candidates = distances_candidates[np.ix_(words_idx, words_idx)]\n",
    "\n",
    "    # Calculate the combination of words that are the least similar to each other\n",
    "    min_sim = np.inf\n",
    "    candidate = None\n",
    "    for combination in itertools.combinations(range(len(words_idx)), top_n):\n",
    "        sim = sum([distances_candidates[i][j] for i in combination for j in combination if i != j])\n",
    "        if sim < min_sim:\n",
    "            candidate = combination\n",
    "            min_sim = sim\n",
    "\n",
    "    return [words_vals[idx] for idx in candidate]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximal Marginal Relevance\n",
    "The final method for diversifying our results is Maximal Marginal Relevance (MMR). MMR tries to minimize redundancy and maximize the diversity of results in text summarization tasks. Fortunately, a keyword extraction algorithm called EmbedRank has implemented a version of MMR that allows us to use it for diversifying our keywords/keyphrases.\n",
    "\n",
    "We start by selecting the keyword/keyphrase that is the most similar to the document. Then, we iteratively select new candidates that are both similar to the document and not similar to the already selected keywords/keyphrases:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "def mmr(doc_embedding, word_embeddings, words, top_n, diversity):\n",
    "\n",
    "    # Extract similarity within words, and between words and the document\n",
    "    word_doc_similarity = cosine_similarity(word_embeddings, doc_embedding)\n",
    "    word_similarity = cosine_similarity(word_embeddings)\n",
    "\n",
    "    # Initialize candidates and already choose best keyword/keyphras\n",
    "    keywords_idx = [np.argmax(word_doc_similarity)]\n",
    "    candidates_idx = [i for i in range(len(words)) if i != keywords_idx[0]]\n",
    "\n",
    "    for _ in range(top_n - 1):\n",
    "        # Extract similarities within candidates and\n",
    "        # between candidates and selected keywords/phrases\n",
    "        candidate_similarities = word_doc_similarity[candidates_idx, :]\n",
    "        target_similarities = np.max(word_similarity[candidates_idx][:, keywords_idx], axis=1)\n",
    "\n",
    "        # Calculate MMR\n",
    "        mmr = (1-diversity) * candidate_similarities - diversity * target_similarities.reshape(-1, 1)\n",
    "        mmr_idx = candidates_idx[np.argmax(mmr)]\n",
    "\n",
    "        # Update keywords & candidates\n",
    "        keywords_idx.append(mmr_idx)\n",
    "        candidates_idx.remove(mmr_idx)\n",
    "\n",
    "    return [words[idx] for idx in keywords_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/utils/deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1971/193457483.py:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_test['oup'] = list_output\n"
     ]
    }
   ],
   "source": [
    "#for dataframe\n",
    "list_output = []\n",
    "for column in df_test.transcription:\n",
    "    doc_embedding, candidate_embeddings, candidates = embed(column)\n",
    "    max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)\n",
    "    list_output.append(mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2))\n",
    "df_test['oup'] = list_output    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "      <th>location</th>\n",
       "      <th>oup</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with complaint of allergies.</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>subjective yearold white female present complaint allergies use allergies live seattle think worse past try claritin zyrtec work short time seem lose effectiveness use allegra also use last summer begin use two weeks ago appear work well use overthecounter spray prescription nasal spray asthma doest require daily medication think flare upmedications medication currently ortho tricyclen allegraallergies know medicine allergiesobjectivevitals weight pound blood pressure heent throat mildly erythematous without exudate nasal mucosa erythematous swell clear drainage see tms clearneck supple without adenopathylungs clearassessment allergic rhinitisplan try zyrtec instead allegra another option use loratadine think prescription coverage might cheaper sample nasonex two spray nostril give three weeks prescription write well</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,</td>\n",
       "      <td>[allergy / immunology,  allergic rhinitis,  allergies,  asthma,  nasal sprays,  rhinitis,  nasal,  erythematous,  allegra,  sprays,  allergic, ]</td>\n",
       "      <td>([633, 650], [49, 58], [287, 293], [642, 650], [275, 280], [492, 504], [167, 174], [633, 641], [0, -1])</td>\n",
       "      <td>[clearneck supple adenopathylungs, allergies live seattle, medicine allergiesobjectivevitals, adenopathylungs clearassessment allergic, nostril weeks prescription]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>past medical history difficulty climb stairs difficulty airline seat tie shoe use public seat lift object floor exercise three time week home cardio difficulty walk two block five flight stairs difficulty snore muscle joint pain include knee pain back pain foot ankle pain swell gastroesophageal reflux diseasepast surgical history include reconstructive surgery right hand years ago social history currently single ten drink year smoke significantly several months ago smoke less three cigarettes dayfamily history heart disease grandfathers grandmother stroke grandmother diabetes deny obesity hypertension family memberscurrent medications noneallergies allergic penicillinmiscellaneouseating history go support group seven months lynn holmberg greenwich eastchester new york feel appropriate program poor experience greenwich program eat history emotional eater like sweets like big portion carbohydrates like chicken steak currently weigh pound ideal body weight would pound pound overweight lose excess body weight would pound weigh review systems negative head neck heart lungs gi gu orthopedic skin specifically deny chest pain heart attack coronary artery disease congestive heart failure arrhythmia atrial fibrillation pacemaker high cholesterol pulmonary embolism high blood pressure cva venous insufficiency thrombophlebitis asthma shortness breath copd emphysema sleep apnea diabetes leg foot swell osteoarthritis rheumatoid arthritis hiatal hernia peptic ulcer disease gallstones infect gallbladder pancreatitis fatty liver hepatitis hemorrhoids rectal bleed polyps incontinence stool urinary stress incontinence cancer deny cellulitis pseudotumor cerebri meningitis encephalitisphysical examination alert orient x cranial nerve iixii intact afebrile vital sign stable</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weight loss programs, gastric bypass, atkin's diet, weight watcher's, body weight, laparoscopic gastric, weight loss, pounds, months, weight, laparoscopic, band, loss, diets, overweight, lost</td>\n",
       "      <td>[bariatrics,  laparoscopic gastric bypass,  weight loss programs,  gastric bypass,  atkin's diet,  weight watcher's,  body weight,  laparoscopic gastric,  weight loss,  pounds,  months,  weight,  laparoscopic,  band,  loss,  diets,  overweight,  lost]</td>\n",
       "      <td>([955, 966], [458, 464], [960, 966], [985, 995])</td>\n",
       "      <td>[stroke grandmother diabetes, sleep apnea diabetes, gallbladder pancreatitis fatty, heart disease grandfathers, rheumatoid arthritis hiatal]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 1</td>\n",
       "      <td>history present illness see abc today pleasant gentleman years old pound bmi overweight ten years since age highest pound lowest pursue surgical attempt weight loss feel good get healthy begin exercise want able exercise play volleyball physically sluggish get tire quickly go often lose weight always regain gain back lose biggest weight loss pound three months gain back six months drink alcohol take many calories multiple commercial weight loss program include slim fast one month one year ago atkins diet one month two years agopast medical history difficulty climb stairs difficulty airline seat tie shoe use public seat difficulty walk high cholesterol high blood pressure asthma difficulty walk two block go eight ten step sleep apnea snore diabetic medication joint pain knee pain back pain foot ankle pain leg foot swell hemorrhoidspast surgical history include orthopedic knee surgerysocial history currently single drink alcohol ten twelve drink week drink five days week binge drink smoke one half pack day years recently stop smoke past two weeksfamily history obesity heart disease diabetes family history negative hypertension strokecurrent medications include diovan crestor tricormiscellaneouseating history say couple friends heart attack die use drink everyday stop two years ago drink weekend second week chantix medication come smoke completely eat eat bad food single eat things like bacon egg cheese cheeseburgers fast food eat four time day seven morning noon pm currently weigh pound ideal body weight pound pound overweight lose excess body weight would pound would get review systems negative head neck heart lungs gi gu orthopedic skin also positive gout deny chest pain heart attack coronary artery disease congestive heart failure arrhythmia atrial fibrillation pacemaker pulmonary embolism cva deny venous insufficiency thrombophlebitis deny shortness breath copd emphysema deny thyroid problems hip pain osteoarthritis rheumatoid arthritis gerd hiatal hernia peptic ulcer disease gallstones infect gallbladder pancreatitis fatty liver hepatitis rectal bleed polyps incontinence stool urinary stress incontinence cancer deny cellulitis pseudotumor cerebri meningitis encephalitisphysical examination alert orient x cranial nerve iixii intact neck soft supple lungs positive wheeze bilaterally heart regular rhythm rate abdomen soft extremities pit edemaimpressionplan explain risk potential complications laparoscopic gastric bypass detail include bleed infection deep venous thrombosis pulmonary embolism leakage gastrojejunoanastomosis jejunojejunoanastomosis possible bowel obstruction among potential complications understand want proceed workup evaluation laparoscopic rouxeny gastric bypass need get letter approval dr xyz need see nutritionist mental health worker need upper endoscopy either dr xyz need go dr xyz previously sleep study need another sleep study need h pylori test thyroid function test lfts glycosylated hemoglobin fast blood sugar perform submit insurance approval</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, heart attacks, body weight, pulmonary embolism, potential complications, sleep study, weight loss, gastric bypass, anastomosis, loss, sleep, laparoscopic, gastric, bypass, heart, pounds, weight,</td>\n",
       "      <td>[bariatrics,  laparoscopic gastric bypass,  heart attacks,  body weight,  pulmonary embolism,  potential complications,  sleep study,  weight loss,  gastric bypass,  anastomosis,  loss,  sleep,  laparoscopic,  gastric,  bypass,  heart,  pounds,  weight, ]</td>\n",
       "      <td>([2436, 2463], [1515, 1526], [1802, 1820], [2412, 2435], [2864, 2875], [152, 163], [2449, 2463], [159, 163], [730, 735], [2436, 2448], [2449, 2456], [2457, 2463], [1082, 1087], [152, 158], [0, -1])</td>\n",
       "      <td>[obesity heart disease, walk high cholesterol, weeksfamily history obesity, biggest weight loss, cheese cheeseburgers fast]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2-D M-Mode. Doppler.</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 1</td>\n",
       "      <td>mmode leave atrial enlargement leave atrial diameter cm normal size right leave ventricle normal lv systolic function leave ventricular ejection fraction normal lv diastolic function pericardial effusion normal morphology aortic valve mitral valve tricuspid valve pulmonary valve pa systolic pressure mmhgdoppler mild mitral tricuspid regurgitation trace aortic pulmonary regurgitation</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d m-mode, doppler, aortic valve, atrial enlargement, diastolic function, ejection fraction, mitral, mitral valve, pericardial effusion, pulmonary valve, regurgitation, systolic function, tricuspid, tricuspid valve, normal lv</td>\n",
       "      <td>[cardiovascular / pulmonary,  2-d m-mode,  doppler,  aortic valve,  atrial enlargement,  diastolic function,  ejection fraction,  mitral,  mitral valve,  pericardial effusion,  pulmonary valve,  regurgitation,  systolic function,  tricuspid,  tricuspid valve,  normal lv ]</td>\n",
       "      <td>([221, 233], [11, 29], [163, 181], [135, 152], [234, 240], [234, 246], [182, 202], [263, 278], [334, 347], [99, 116], [247, 256], [247, 262], [89, 99])</td>\n",
       "      <td>[aortic pulmonary regurgitation, ventricle normal lv, tricuspid valve pulmonary, morphology aortic valve, ventricular ejection fraction]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2-D Echocardiogram</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 2</td>\n",
       "      <td>leave ventricular cavity size wall thickness appear normal wall motion leave ventricular systolic function appear hyperdynamic estimate ejection fraction nearcavity obliteration see also appear increase leave ventricular outflow tract gradient mid cavity level consistent hyperdynamic leave ventricular systolic function abnormal leave ventricular relaxation pattern see well elevate leave atrial pressure see doppler examination leave atrium appear mildly dilate right atrium right ventricle appear normal aortic root appear normal aortic valve appear calcify mild aortic valve stenosis calculate aortic valve area cm square maximum instantaneous gradient mean gradient mm mitral annular calcification extend leaflets supportive structure thicken mitral valve leaflets mild mitral regurgitation tricuspid valve appear normal trace tricuspid regurgitation moderate pulmonary artery hypertension estimate pulmonary artery systolic pressure mmhg estimate right atrial pressure mmhg pulmonary valve appear normal trace pulmonary insufficiency pericardial effusion intracardiac mass see color doppler suggestive patent foramen ovale lipomatous hypertrophy interatrial septum study somewhat technically limit hence subtle abnormalities could miss study</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d, doppler, echocardiogram, annular, aortic root, aortic valve, atrial, atrium, calcification, cavity, ejection fraction, mitral, obliteration, outflow, regurgitation, relaxation pattern, stenosis, systolic function, tricuspid, valve, ventricular, ventricular cavity, wall motion, pulmonary artery</td>\n",
       "      <td>[cardiovascular / pulmonary,  2-d,  doppler,  echocardiogram,  annular,  aortic root,  aortic valve,  atrial,  atrium,  calcification,  cavity,  ejection fraction,  mitral,  obliteration,  outflow,  regurgitation,  relaxation pattern,  stenosis,  systolic function,  tricuspid,  valve,  ventricular,  ventricular cavity,  wall motion,  pulmonary artery]</td>\n",
       "      <td>([409, 416], [680, 687], [506, 517], [532, 544], [389, 395], [435, 441], [688, 701], [17, 23], [135, 152], [673, 679], [164, 176], [220, 227], [781, 794], [347, 365], [578, 586], [88, 105], [795, 804], [539, 544], [5, 16], [5, 23], [58, 69], [864, 880])</td>\n",
       "      <td>[dilate right atrium, normal aortic valve, pulmonary insufficiency pericardial, hyperdynamic leave ventricular, artery hypertension estimate]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "2           2   \n",
       "3           3   \n",
       "4           4   \n",
       "\n",
       "                                                         description  \\\n",
       "0   A 23-year-old white female presents with complaint of allergies.   \n",
       "1                           Consult for laparoscopic gastric bypass.   \n",
       "2                           Consult for laparoscopic gastric bypass.   \n",
       "3                                             2-D M-Mode. Doppler.     \n",
       "4                                                 2-D Echocardiogram   \n",
       "\n",
       "             medical_specialty                                sample_name  \\\n",
       "0         Allergy / Immunology                         Allergic Rhinitis    \n",
       "1                   Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "2                   Bariatrics   Laparoscopic Gastric Bypass Consult - 1    \n",
       "3   Cardiovascular / Pulmonary                    2-D Echocardiogram - 1    \n",
       "4   Cardiovascular / Pulmonary                    2-D Echocardiogram - 2    \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    transcription  \\\n",
       "0                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    subjective yearold white female present complaint allergies use allergies live seattle think worse past try claritin zyrtec work short time seem lose effectiveness use allegra also use last summer begin use two weeks ago appear work well use overthecounter spray prescription nasal spray asthma doest require daily medication think flare upmedications medication currently ortho tricyclen allegraallergies know medicine allergiesobjectivevitals weight pound blood pressure heent throat mildly erythematous without exudate nasal mucosa erythematous swell clear drainage see tms clearneck supple without adenopathylungs clearassessment allergic rhinitisplan try zyrtec instead allegra another option use loratadine think prescription coverage might cheaper sample nasonex two spray nostril give three weeks prescription write well   \n",
       "1                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          past medical history difficulty climb stairs difficulty airline seat tie shoe use public seat lift object floor exercise three time week home cardio difficulty walk two block five flight stairs difficulty snore muscle joint pain include knee pain back pain foot ankle pain swell gastroesophageal reflux diseasepast surgical history include reconstructive surgery right hand years ago social history currently single ten drink year smoke significantly several months ago smoke less three cigarettes dayfamily history heart disease grandfathers grandmother stroke grandmother diabetes deny obesity hypertension family memberscurrent medications noneallergies allergic penicillinmiscellaneouseating history go support group seven months lynn holmberg greenwich eastchester new york feel appropriate program poor experience greenwich program eat history emotional eater like sweets like big portion carbohydrates like chicken steak currently weigh pound ideal body weight would pound pound overweight lose excess body weight would pound weigh review systems negative head neck heart lungs gi gu orthopedic skin specifically deny chest pain heart attack coronary artery disease congestive heart failure arrhythmia atrial fibrillation pacemaker high cholesterol pulmonary embolism high blood pressure cva venous insufficiency thrombophlebitis asthma shortness breath copd emphysema sleep apnea diabetes leg foot swell osteoarthritis rheumatoid arthritis hiatal hernia peptic ulcer disease gallstones infect gallbladder pancreatitis fatty liver hepatitis hemorrhoids rectal bleed polyps incontinence stool urinary stress incontinence cancer deny cellulitis pseudotumor cerebri meningitis encephalitisphysical examination alert orient x cranial nerve iixii intact afebrile vital sign stable   \n",
       "2  history present illness see abc today pleasant gentleman years old pound bmi overweight ten years since age highest pound lowest pursue surgical attempt weight loss feel good get healthy begin exercise want able exercise play volleyball physically sluggish get tire quickly go often lose weight always regain gain back lose biggest weight loss pound three months gain back six months drink alcohol take many calories multiple commercial weight loss program include slim fast one month one year ago atkins diet one month two years agopast medical history difficulty climb stairs difficulty airline seat tie shoe use public seat difficulty walk high cholesterol high blood pressure asthma difficulty walk two block go eight ten step sleep apnea snore diabetic medication joint pain knee pain back pain foot ankle pain leg foot swell hemorrhoidspast surgical history include orthopedic knee surgerysocial history currently single drink alcohol ten twelve drink week drink five days week binge drink smoke one half pack day years recently stop smoke past two weeksfamily history obesity heart disease diabetes family history negative hypertension strokecurrent medications include diovan crestor tricormiscellaneouseating history say couple friends heart attack die use drink everyday stop two years ago drink weekend second week chantix medication come smoke completely eat eat bad food single eat things like bacon egg cheese cheeseburgers fast food eat four time day seven morning noon pm currently weigh pound ideal body weight pound pound overweight lose excess body weight would pound would get review systems negative head neck heart lungs gi gu orthopedic skin also positive gout deny chest pain heart attack coronary artery disease congestive heart failure arrhythmia atrial fibrillation pacemaker pulmonary embolism cva deny venous insufficiency thrombophlebitis deny shortness breath copd emphysema deny thyroid problems hip pain osteoarthritis rheumatoid arthritis gerd hiatal hernia peptic ulcer disease gallstones infect gallbladder pancreatitis fatty liver hepatitis rectal bleed polyps incontinence stool urinary stress incontinence cancer deny cellulitis pseudotumor cerebri meningitis encephalitisphysical examination alert orient x cranial nerve iixii intact neck soft supple lungs positive wheeze bilaterally heart regular rhythm rate abdomen soft extremities pit edemaimpressionplan explain risk potential complications laparoscopic gastric bypass detail include bleed infection deep venous thrombosis pulmonary embolism leakage gastrojejunoanastomosis jejunojejunoanastomosis possible bowel obstruction among potential complications understand want proceed workup evaluation laparoscopic rouxeny gastric bypass need get letter approval dr xyz need see nutritionist mental health worker need upper endoscopy either dr xyz need go dr xyz previously sleep study need another sleep study need h pylori test thyroid function test lfts glycosylated hemoglobin fast blood sugar perform submit insurance approval   \n",
       "3                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               mmode leave atrial enlargement leave atrial diameter cm normal size right leave ventricle normal lv systolic function leave ventricular ejection fraction normal lv diastolic function pericardial effusion normal morphology aortic valve mitral valve tricuspid valve pulmonary valve pa systolic pressure mmhgdoppler mild mitral tricuspid regurgitation trace aortic pulmonary regurgitation   \n",
       "4                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 leave ventricular cavity size wall thickness appear normal wall motion leave ventricular systolic function appear hyperdynamic estimate ejection fraction nearcavity obliteration see also appear increase leave ventricular outflow tract gradient mid cavity level consistent hyperdynamic leave ventricular systolic function abnormal leave ventricular relaxation pattern see well elevate leave atrial pressure see doppler examination leave atrium appear mildly dilate right atrium right ventricle appear normal aortic root appear normal aortic valve appear calcify mild aortic valve stenosis calculate aortic valve area cm square maximum instantaneous gradient mean gradient mm mitral annular calcification extend leaflets supportive structure thicken mitral valve leaflets mild mitral regurgitation tricuspid valve appear normal trace tricuspid regurgitation moderate pulmonary artery hypertension estimate pulmonary artery systolic pressure mmhg estimate right atrial pressure mmhg pulmonary valve appear normal trace pulmonary insufficiency pericardial effusion intracardiac mass see color doppler suggestive patent foramen ovale lipomatous hypertrophy interatrial septum study somewhat technically limit hence subtle abnormalities could miss study   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                  keywords  \\\n",
       "0                                                                                                                                                                                                      allergy / immunology, allergic rhinitis, allergies, asthma, nasal sprays, rhinitis, nasal, erythematous, allegra, sprays, allergic,   \n",
       "1                                                                                                 bariatrics, laparoscopic gastric bypass, weight loss programs, gastric bypass, atkin's diet, weight watcher's, body weight, laparoscopic gastric, weight loss, pounds, months, weight, laparoscopic, band, loss, diets, overweight, lost   \n",
       "2                                                                                              bariatrics, laparoscopic gastric bypass, heart attacks, body weight, pulmonary embolism, potential complications, sleep study, weight loss, gastric bypass, anastomosis, loss, sleep, laparoscopic, gastric, bypass, heart, pounds, weight,   \n",
       "3                                                                          cardiovascular / pulmonary, 2-d m-mode, doppler, aortic valve, atrial enlargement, diastolic function, ejection fraction, mitral, mitral valve, pericardial effusion, pulmonary valve, regurgitation, systolic function, tricuspid, tricuspid valve, normal lv    \n",
       "4  cardiovascular / pulmonary, 2-d, doppler, echocardiogram, annular, aortic root, aortic valve, atrial, atrium, calcification, cavity, ejection fraction, mitral, obliteration, outflow, regurgitation, relaxation pattern, stenosis, systolic function, tricuspid, valve, ventricular, ventricular cavity, wall motion, pulmonary artery   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                       keywords_list  \\\n",
       "0                                                                                                                                                                                                                   [allergy / immunology,  allergic rhinitis,  allergies,  asthma,  nasal sprays,  rhinitis,  nasal,  erythematous,  allegra,  sprays,  allergic, ]   \n",
       "1                                                                                                        [bariatrics,  laparoscopic gastric bypass,  weight loss programs,  gastric bypass,  atkin's diet,  weight watcher's,  body weight,  laparoscopic gastric,  weight loss,  pounds,  months,  weight,  laparoscopic,  band,  loss,  diets,  overweight,  lost]   \n",
       "2                                                                                                    [bariatrics,  laparoscopic gastric bypass,  heart attacks,  body weight,  pulmonary embolism,  potential complications,  sleep study,  weight loss,  gastric bypass,  anastomosis,  loss,  sleep,  laparoscopic,  gastric,  bypass,  heart,  pounds,  weight, ]   \n",
       "3                                                                                   [cardiovascular / pulmonary,  2-d m-mode,  doppler,  aortic valve,  atrial enlargement,  diastolic function,  ejection fraction,  mitral,  mitral valve,  pericardial effusion,  pulmonary valve,  regurgitation,  systolic function,  tricuspid,  tricuspid valve,  normal lv ]   \n",
       "4  [cardiovascular / pulmonary,  2-d,  doppler,  echocardiogram,  annular,  aortic root,  aortic valve,  atrial,  atrium,  calcification,  cavity,  ejection fraction,  mitral,  obliteration,  outflow,  regurgitation,  relaxation pattern,  stenosis,  systolic function,  tricuspid,  valve,  ventricular,  ventricular cavity,  wall motion,  pulmonary artery]   \n",
       "\n",
       "                                                                                                                                                                                                                                                        location  \\\n",
       "0                                                                                                                                                        ([633, 650], [49, 58], [287, 293], [642, 650], [275, 280], [492, 504], [167, 174], [633, 641], [0, -1])   \n",
       "1                                                                                                                                                                                                               ([955, 966], [458, 464], [960, 966], [985, 995])   \n",
       "2                                                          ([2436, 2463], [1515, 1526], [1802, 1820], [2412, 2435], [2864, 2875], [152, 163], [2449, 2463], [159, 163], [730, 735], [2436, 2448], [2449, 2456], [2457, 2463], [1082, 1087], [152, 158], [0, -1])   \n",
       "3                                                                                                        ([221, 233], [11, 29], [163, 181], [135, 152], [234, 240], [234, 246], [182, 202], [263, 278], [334, 347], [99, 116], [247, 256], [247, 262], [89, 99])   \n",
       "4  ([409, 416], [680, 687], [506, 517], [532, 544], [389, 395], [435, 441], [688, 701], [17, 23], [135, 152], [673, 679], [164, 176], [220, 227], [781, 794], [347, 365], [578, 586], [88, 105], [795, 804], [539, 544], [5, 16], [5, 23], [58, 69], [864, 880])   \n",
       "\n",
       "                                                                                                                                                                   oup  \n",
       "0  [clearneck supple adenopathylungs, allergies live seattle, medicine allergiesobjectivevitals, adenopathylungs clearassessment allergic, nostril weeks prescription]  \n",
       "1                         [stroke grandmother diabetes, sleep apnea diabetes, gallbladder pancreatitis fatty, heart disease grandfathers, rheumatoid arthritis hiatal]  \n",
       "2                                          [obesity heart disease, walk high cholesterol, weeksfamily history obesity, biggest weight loss, cheese cheeseburgers fast]  \n",
       "3                             [aortic pulmonary regurgitation, ventricle normal lv, tricuspid valve pulmonary, morphology aortic valve, ventricular ejection fraction]  \n",
       "4                        [dilate right atrium, normal aortic valve, pulmonary insufficiency pericardial, hyperdynamic leave ventricular, artery hypertension estimate]  "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for custom input\n",
    "doc_embedding, candidate_embeddings, candidates = embed(doc)\n",
    "max_sum_sim(doc_embedding, candidate_embeddings, candidates, top_n=5, nr_candidates=10)\n",
    "mmr(doc_embedding, candidate_embeddings, candidates, top_n=5, diversity=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
