{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForMaskedLM\n",
    "from transformers import AutoTokenizer, AutoConfig\n",
    "from transformers import BertForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tokenizers import BertWordPieceTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configs and hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# HYPERPARAMS\n",
    "SEED_SPLIT = 0\n",
    "SEED_TRAIN = 0\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 2#16\n",
    "EVAL_BATCH_SIZE = 2#16\n",
    "LEARNING_RATE = 2e-5 \n",
    "LR_WARMUP_STEPS = 100\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# load data\n",
    "dtf_mlm = pd.read_csv('../data/raw/mtsamples_cleaned.csv')\n",
    "#dtf_mlm = dtf_mlm.rename(columns={\"review_content\": \"text\"})\n",
    "\n",
    "# Train/Valid Split\n",
    "df_train, df_valid = train_test_split(\n",
    "    dtf_mlm, test_size=0.15, random_state=SEED_SPLIT\n",
    ")\n",
    "\n",
    "len(df_train), len(df_valid)\n",
    "\n",
    "# Convert to Dataset object\n",
    "train_dataset = Dataset.from_pandas(df_train[['transcription']].dropna())\n",
    "valid_dataset = Dataset.from_pandas(df_valid[['transcription']].dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chose the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/bert-base-cased/resolve/main/vocab.txt from cache at /Users/tara-sophiatumbraegel/.cache/huggingface/transformers/6508e60ab3c1200bffa26c95f4b58ac6b6d95fba4db1f195f632fa3cd7bc64cc.437aa611e89f6fc6675a049d2b5545390adbc617e7d655286421c191d2be2791\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/bert-base-cased/resolve/main/tokenizer_config.json from cache at /Users/tara-sophiatumbraegel/.cache/huggingface/transformers/ec84e86ee39bfe112543192cf981deebf7e6cbe8c91b8f7f8f63c9be44366158.ec5c189f89475aac7d8cbd243960a0655cfadc3d0474da8ff2ed0bf1699c2a5f\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/tara-sophiatumbraegel/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"bert-base-cased\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading configuration file https://huggingface.co/bert-base-cased/resolve/main/config.json from cache at /Users/tara-sophiatumbraegel/.cache/huggingface/transformers/a803e0468a8fe090683bdc453f4fac622804f49de86d7cecaee92365d4a0f829.a64a22196690e0e82ead56f388a3ef3a50de93335926ccfa20610217db589307\n",
      "Model config BertConfig {\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/bert-base-cased/resolve/main/pytorch_model.bin from cache at /Users/tara-sophiatumbraegel/.cache/huggingface/transformers/092cc582560fc3833e556b3f833695c26343cb54b7e88cd02d40821462a74999.1f48cab6c959fc6c360d22bea39d06959e90f5b002e77e836d2da45464875cda\n",
      "Some weights of the model checkpoint at bert-base-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertForMaskedLM were initialized from the model checkpoint at bert-base-cased.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForMaskedLM for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "bert-base-uncased  # 12-layer, 768-hidden, 12-heads, 109M parameters\n",
    "distilbert-base-uncased  # 6-layer, 768-hidden, 12-heads, 65M parameters\n",
    "'''\n",
    "\n",
    "MODEL = 'bert'\n",
    "bert_type = 'bert-base-cased'\n",
    "checkpoint = 'emilyalsentzer/Bio_ClinicalBERT' #\"bert-base-cased\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "#model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "#tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "if MODEL == 'distilbert':\n",
    "    TokenizerClass = DistilBertTokenizer \n",
    "    ModelClass = DistilBertForMaskedLM \n",
    "elif MODEL == 'bert':\n",
    "    TokenizerClass = BertTokenizer\n",
    "    ModelClass = BertForMaskedLM \n",
    "elif MODEL == 'roberta':\n",
    "    TokenizerClass = RobertaTokenizer\n",
    "    ModelClass = RobertaForMaskedLM\n",
    "elif MODEL == 'scibert':\n",
    "    TokenizerClass = AutoTokenizer\n",
    "    ModelClass = AutoModelForMaskedLM\n",
    "\n",
    "\n",
    "tokenizer = TokenizerClass.from_pretrained(\n",
    "            bert_type, use_fast=True, do_lower_case=False, max_len=MAX_SEQ_LEN\n",
    "            )\n",
    "model = ModelClass.from_pretrained(bert_type)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# tokenize the  data and train test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "408ae8c80fb04499a68d52813f8e1cff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f5141da0c49e498ba5c26336c1dcf74e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a29aabb83bf4b9ba51c53e3b59a1eae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf4250b0c8f8473e86ae3238de99827c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85104f534a194abaab0196e0907fd0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0683ecc77d0c4264bc7ee51241598880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe072e7c38cc423ea7afc8bcc2df29de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94cc263f5fa0455ab82a003d79923e52",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_function(row):\n",
    "    return tokenizer(\n",
    "        row['transcription'],\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        max_length=MAX_SEQ_LEN,\n",
    "        return_special_tokens_mask=True)\n",
    "  \n",
    "column_names = train_dataset.column_names\n",
    "\n",
    "train_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=column_names,\n",
    ")\n",
    "\n",
    "valid_dataset = valid_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    num_proc=multiprocessing.cpu_count(),\n",
    "    remove_columns=column_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:3lu7vlh6) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▁</td></tr><tr><td>eval/runtime</td><td>█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁█</td></tr><tr><td>train/epoch</td><td>▁██</td></tr><tr><td>train/global_step</td><td>▁██</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>4.22527</td></tr><tr><td>eval/runtime</td><td>6.9124</td></tr><tr><td>eval/samples_per_second</td><td>2.893</td></tr><tr><td>eval/steps_per_second</td><td>0.289</td></tr><tr><td>train/epoch</td><td>2.0</td></tr><tr><td>train/global_step</td><td>10</td></tr><tr><td>train/total_flos</td><td>10528004997120.0</td></tr><tr><td>train/train_loss</td><td>4.54433</td></tr><tr><td>train/train_runtime</td><td>413.9781</td></tr><tr><td>train/train_samples_per_second</td><td>0.386</td></tr><tr><td>train/train_steps_per_second</td><td>0.024</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Synced <strong style=\"color:#cdcd00\">./bert-news</strong>: <a href=\"https://wandb.ai/taratumbraegel/huggingface/runs/3lu7vlh6\" target=\"_blank\">https://wandb.ai/taratumbraegel/huggingface/runs/3lu7vlh6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20221026_180304-3lu7vlh6/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:3lu7vlh6). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9dc57f4ffb4362bdf47577efa3d2f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01667044899998776, max=1.0)…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/wandb/run-20221027_114016-3k0u3rgk</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/taratumbraegel/keyberto2/runs/3k0u3rgk\" target=\"_blank\">whole-river-1</a></strong> to <a href=\"https://wandb.ai/taratumbraegel/keyberto2\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 80\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 80\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c388e18c0f8401a9f4988bdf746999d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/80 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79e8e931d2a241ddaf76a945050e13f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./bert-news/checkpoint-40\n",
      "Configuration saved in ./bert-news/checkpoint-40/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.954698085784912, 'eval_runtime': 5.5139, 'eval_samples_per_second': 3.627, 'eval_steps_per_second': 1.814, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./bert-news/checkpoint-40/pytorch_model.bin\n",
      "tokenizer config file saved in ./bert-news/checkpoint-40/tokenizer_config.json\n",
      "Special tokens file saved in ./bert-news/checkpoint-40/special_tokens_map.json\n",
      "Deleting older checkpoint [bert-news/checkpoint-208] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForMaskedLM.forward` and have been ignored: special_tokens_mask. If special_tokens_mask are not expected by `BertForMaskedLM.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 20\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97059b108bca44e89928ed04ad4d1b8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./bert-news/checkpoint-80\n",
      "Configuration saved in ./bert-news/checkpoint-80/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 3.921980381011963, 'eval_runtime': 5.364, 'eval_samples_per_second': 3.729, 'eval_steps_per_second': 1.864, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ./bert-news/checkpoint-80/pytorch_model.bin\n",
      "tokenizer config file saved in ./bert-news/checkpoint-80/tokenizer_config.json\n",
      "Special tokens file saved in ./bert-news/checkpoint-80/special_tokens_map.json\n",
      "Deleting older checkpoint [bert-news/checkpoint-5] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./bert-news/checkpoint-80 (score: 3.921980381011963).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 671.7744, 'train_samples_per_second': 0.238, 'train_steps_per_second': 0.119, 'train_loss': 4.2029365539550785, 'epoch': 2.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../model/modelUnsupervised\n",
      "Configuration saved in ../model/modelUnsupervised/config.json\n",
      "Model weights saved in ../model/modelUnsupervised/pytorch_model.bin\n",
      "tokenizer config file saved in ../model/modelUnsupervised/tokenizer_config.json\n",
      "Special tokens file saved in ../model/modelUnsupervised/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n",
      "    self._process(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 308, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 305, in send\n",
      "    send_handler(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 317, in send_request\n",
      "    logger.debug(f\"send_request: {request_type}\")\n",
      "Message: 'send_request: stop_status'\n",
      "Arguments: ()\n",
      "Exception in thread OutRawRd-stderr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 946, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1027, in _output_raw_reader_thread\n",
      "    self._output_raw_flush(stream)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1042, in _output_raw_flush\n",
      "    self._output_raw_file.write(data.encode(\"utf-8\"))\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 64, in write\n",
      "    super().write(b\"\\n\".join(ret) + b\"\\n\")\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 31, in write\n",
      "    self.f.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 102, in _run\n",
      "    self._finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 266, in _finish\n",
      "    self._hm.finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 814, in finish\n",
      "    logger.info(\"shutting down handler\")\n",
      "Message: 'shutting down handler'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/vendor/watchdog/observers/api.py\", line 199, in run\n",
      "    self.dispatch_events(self.event_queue, self.timeout)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/vendor/watchdog/observers/api.py\", line 368, in dispatch_events\n",
      "    handler.dispatch(event)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/vendor/watchdog/events.py\", line 454, in dispatch\n",
      "    _method_map[event_type](event)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 292, in _on_file_modified\n",
      "    logger.info(f\"file/dir modified: { event.src_path}\")\n",
      "Message: 'file/dir modified: /Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/wandb/run-20221027_114016-3k0u3rgk/files/output.log'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/service/streams.py\", line 48, in run\n",
      "    self._target(**self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 163, in wandb_internal\n",
      "    logger.error(f\"Thread {thread.name}:\", exc_info=exc_info)\n",
      "Message: 'Thread SenderThread:'\n",
      "Arguments: ()\n",
      "Thread SenderThread:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 102, in _run\n",
      "    self._finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 311, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1334, in finish\n",
      "    self._output_raw_finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 998, in _output_raw_finish\n",
      "    self._output_raw_file.close()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 68, in close\n",
      "    super().write(self._buff)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 31, in write\n",
      "    self.f.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "wandb: ERROR Internal wandb error: file data was not synced\n",
      "/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/multiprocessing/resource_tracker.py:224: UserWarning: resource_tracker: There appear to be 15 leaked semaphore objects to clean up at shutdown\n",
      "  warnings.warn('resource_tracker: There appear to be %d '\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "\n",
    "\n",
    "# 1. Start a W&B run\n",
    "wandb.init(project='keyberto2')\n",
    "#!pip install wandb\n",
    "#!wandb login\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(\n",
    "    tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    ")\n",
    "\n",
    "\n",
    "steps_per_epoch = int(len(train_dataset) / TRAIN_BATCH_SIZE)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./bert-news',\n",
    "    logging_dir='./LMlogs',             \n",
    "    num_train_epochs=2,\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "    per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "    warmup_steps=LR_WARMUP_STEPS,\n",
    "    save_steps=steps_per_epoch,\n",
    "    save_total_limit=3,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    "    learning_rate=LEARNING_RATE, \n",
    "    evaluation_strategy='epoch',\n",
    "    save_strategy='epoch',\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='loss', \n",
    "    greater_is_better=False,\n",
    "    seed=SEED_TRAIN, \n",
    "    report_to = 'wandb'\n",
    ")\n",
    "small_train_df = train_dataset.select(range(80))\n",
    "small_valid_df = valid_dataset.select(range(20))\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=small_train_df,\n",
    "    eval_dataset=small_valid_df,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(\"../model/modelUnsupervised\") #save your custom model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'KeyBERT' from 'transformers' (/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v04keywordBert.ipynb Cell 7\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v04keywordBert.ipynb#X11sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtransformers\u001b[39;00m \u001b[39mimport\u001b[39;00m BertTokenizer, BertForMaskedLM, KeyBERT, pipeline\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v04keywordBert.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m trainer\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39m../models/mtsamples\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v04keywordBert.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m model \u001b[39m=\u001b[39m BertForMaskedLM\u001b[39m.\u001b[39mfrom_pretrained(\u001b[39m'\u001b[39m\u001b[39m../models/mtsamples\u001b[39m\u001b[39m'\u001b[39m) \u001b[39m#/model\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'KeyBERT' from 'transformers' (/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/__init__.py)"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertForMaskedLM, KeyBERT, pipeline\n",
    "model = BertForMaskedLM.from_pretrained('../models/mtsamples') #/model\n",
    "tokenizer = BertTokenizer.from_pretrained('../models/mtsamples')\n",
    "hf_model = pipeline('feature-extraction', model=model, tokenizer=tokenizer)\n",
    "\n",
    "example_text = \"I am a sentence that I want embeddings for. I am so sick my neck hurts\"\n",
    "\n",
    "\n",
    "kw_model = KeyBERT(model=hf_model)\n",
    "keywords = kw_model.extract_keywords(example_text, keyphrase_ngram_range=(1, 2), \n",
    "                                     stop_words='english', \n",
    "                                     use_maxsum=True, \n",
    "                                     nr_candidates=10, \n",
    "                                     top_n=5, \n",
    "                                     use_mmr=True, \n",
    "                                     diversity=0.7, \n",
    "                                     only_keywords=True)\n",
    "\n",
    "keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', use_fast = False, do_lower_case=True)\n",
    "model = AutoModelForMaskedLM.from_pretrained('bert-base-uncased')\n",
    "import torch\n",
    "import math\n",
    "\n",
    "trainer = Trainer(\n",
    "  model=model,\n",
    "  data_collator=data_collator,\n",
    "  #train_dataset=tokenized_dataset_2['train'],\n",
    "  eval_dataset=valid_dataset,\n",
    "  tokenizer=tokenizer,\n",
    "  )\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "\n",
    "print('Evaluation results: ', eval_results)\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.3f}\")\n",
    "print('-'*30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Save model inputs and hyperparameters\n",
    "config = trainer.model.config\n",
    "config.save_pretrained(wandb.run.dir)\n",
    "tokenizer.save_pretrained(wandb.run.dir)\n",
    "        \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
