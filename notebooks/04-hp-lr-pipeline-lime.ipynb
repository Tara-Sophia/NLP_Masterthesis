{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Import Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import re\n",
    "import nltk\n",
    "import imblearn\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.manifold import TSNE    \n",
    "\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'muscle', 'coronary', 'fat', 'nitroglycerin', 'teeth', 'andor', 'blood', 'heart', 'breast', 'oxygen', 'valve', 'ear', 'thyroid', 'men', 'bone', 'tablet', 'salt', 'artery'}\""
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load data\n",
    "df_test = pd.read_csv('../data/processed/mtsamples_nlp.csv')\n",
    "df_test.transcription=df_test.transcription.astype(str)\n",
    "df_test.tail()\n",
    "df_test.transcription_f[2973]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve labels as function\n",
    "def get_labels(data):\n",
    "    return data['medical_specialty'].tolist()\n",
    "\n",
    "df_test_label = get_labels(df_test)\n",
    "\n",
    "df_test_X = df_test['transcription_f'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train and test set (first split data into train and test set to only transform the train set)\n",
    "def split_data(X, y):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "X_train, X_test, y_train, y_test = split_data(df_test_X, df_test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2976, 2702)\n"
     ]
    }
   ],
   "source": [
    "# vectorize df_test_X for correlation matrix between features\n",
    "vectorizer = CountVectorizer()\n",
    "df_test_X_vec = vectorizer.fit_transform(df_test_X)\n",
    "df_test_X_vec = df_test_X_vec.toarray()\n",
    "print(df_test_X_vec.shape)\n",
    "\n",
    "df_test_X_vec = pd.DataFrame(df_test_X_vec, columns=vectorizer.get_feature_names_out ())\n",
    "#df_test_X_vec.corr()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Strong negative correlations:\n",
      "patient         set        -0.294474\n",
      "intervertebral  patient    -0.165295\n",
      "heart           tissue     -0.136525\n",
      "wound           heart      -0.127911\n",
      "patient         pancreas   -0.122850\n",
      "set             left       -0.119657\n",
      "adrenal         patient    -0.117364\n",
      "patient         hilar      -0.113338\n",
      "joint           tube       -0.109961\n",
      "coronary        tissue     -0.109453\n",
      "dtype: float64\n",
      "Strong positive correlations:\n",
      "elevated      290                   0.953302\n",
      "mmhg          290                   0.912563\n",
      "homograft     hydroxychloroquine    0.894367\n",
      "carboplatin   taxol                 0.894277\n",
      "mmhg          elevated              0.869889\n",
      "phosphorus    126000                0.865880\n",
      "qam           phosphorus            0.865880\n",
      "bicuspid      qam                   0.865880\n",
      "hypodense     intramural            0.865880\n",
      "semicircular  intermuscular         0.865880\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# show highly correlated features (what is the best threshold for correlation?)\n",
    "corr_unstack = df_test_X_vec.corr().unstack()\n",
    "corr_unstack = corr_unstack[corr_unstack != 1.0]\n",
    "corr_unstack = corr_unstack.sort_values().drop_duplicates()\n",
    "print(\"Strong negative correlations:\")\n",
    "print(corr_unstack.nsmallest(10))\n",
    "print(\"Strong positive correlations:\")\n",
    "print(corr_unstack.nlargest(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The “saga” solver is a variant of “sag” that also supports the non-smooth penalty=\"l1\" \n",
    "# This is therefore the solver of choice for sparse multinomial logistic regression\n",
    "model_pipeline = imbPipeline([\n",
    "        ('preprocessing',CountVectorizer()),\n",
    "        ('smote', SMOTE(random_state=42)),\n",
    "        ('classifier', LogisticRegression(random_state=42, multi_class='multinomial', solver='saga')),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                precision    recall  f1-score   support\n",
      "\n",
      "    Cardiovascular / Pulmonary       0.25      0.27      0.26        52\n",
      "    Consult - History and Phy.       0.16      0.15      0.15        40\n",
      "              Gastroenterology       0.25      0.31      0.27        42\n",
      "              General Medicine       0.05      0.08      0.06        25\n",
      "                     Neurology       0.04      0.04      0.04        23\n",
      "       Obstetrics / Gynecology       0.24      0.33      0.28        30\n",
      "                    Orthopedic       0.11      0.14      0.12        57\n",
      "                     Radiology       0.07      0.06      0.06        51\n",
      " SOAP / Chart / Progress Notes       0.15      0.14      0.14        35\n",
      "                       Surgery       0.35      0.26      0.30       212\n",
      "                       Urology       0.20      0.24      0.22        29\n",
      "\n",
      "                      accuracy                           0.21       596\n",
      "                     macro avg       0.17      0.18      0.17       596\n",
      "                  weighted avg       0.23      0.21      0.21       596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The max_iter was reached which means the coef_ did not converge\n"
     ]
    }
   ],
   "source": [
    "# poor performance without fine tuning\n",
    "lr = model_pipeline.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "category_list = df_test.medical_specialty.unique()\n",
    "print(classification_report(y_test, y_pred, target_names=lr.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py:378: FitFailedWarning: \n",
      "20 fits failed out of a total of 60.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "20 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/imblearn/pipeline.py\", line 272, in fit\n",
      "    self._final_estimator.fit(Xt, yt, **fit_params_last_step)\n",
      "  File \"/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py\", line 1101, in fit\n",
      "    raise ValueError(\n",
      "ValueError: l1_ratio must be between 0 and 1; got (l1_ratio=None)\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/model_selection/_search.py:953: UserWarning: One or more of the test scores are non-finite: [0.33991597 0.4092437         nan 0.39831933 0.34957983        nan\n",
      " 0.30966387 0.25840336        nan 0.23109244 0.23067227        nan]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameter {'classifier': LogisticRegression(C=0.01, multi_class='multinomial', random_state=42,\n",
      "                   solver='saga'), 'classifier__C': 0.01, 'classifier__penalty': 'l2'}\n",
      "                                precision    recall  f1-score   support\n",
      "\n",
      "    Cardiovascular / Pulmonary       0.44      0.60      0.50        52\n",
      "    Consult - History and Phy.       0.36      0.30      0.33        40\n",
      "              Gastroenterology       0.46      0.55      0.50        42\n",
      "              General Medicine       0.17      0.20      0.19        25\n",
      "                     Neurology       0.21      0.30      0.25        23\n",
      "       Obstetrics / Gynecology       0.37      0.57      0.45        30\n",
      "                    Orthopedic       0.35      0.51      0.41        57\n",
      "                     Radiology       0.15      0.10      0.12        51\n",
      " SOAP / Chart / Progress Notes       0.18      0.23      0.20        35\n",
      "                       Surgery       0.58      0.37      0.45       212\n",
      "                       Urology       0.32      0.41      0.36        29\n",
      "\n",
      "                      accuracy                           0.38       596\n",
      "                     macro avg       0.33      0.38      0.34       596\n",
      "                  weighted avg       0.41      0.38      0.38       596\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# take best model from grid search and perform evaluation\n",
    "\n",
    "param_grid = [\n",
    "    { 'classifier__C': [0.01, 0.1, 1, 10],\n",
    "      'classifier': [LogisticRegression(multi_class='multinomial', random_state=42, solver='saga')],\n",
    "      'classifier__penalty': ['l1', 'l2', 'elasticnet'],\n",
    "    }\n",
    "]\n",
    "\n",
    "def grid_search(X_train, y_train, model_pipeline, param_grid):\n",
    "    search = GridSearchCV(model_pipeline, param_grid, cv=5)\n",
    "    search.fit(X_train, y_train)\n",
    "    print(\"Best parameter\", search.best_params_)\n",
    "    return search.best_estimator_\n",
    "    \n",
    "best_model = grid_search(X_train, y_train, model_pipeline, param_grid)\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "category_list = df_test.medical_specialty.unique()\n",
    "# predict and evaluate\n",
    "print(classification_report(y_test, y_pred, target_names=best_model.classes_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"{'lip', 'abdomen', 'needle', 'endometrial', 'wall', 'anterior', 'cervix', 'tissue', 'fallopian', 'decidual', 'patient', 'tube', 'unclotted', 'liver', 'umbilicus', 'mesosalpinx', 'saline', 'vulsellum', 'vagina', 'left', 'midline', 'curettings', 'clot', 'fundus', 'peritoneum', 'omental', 'vesicouterine', 'uterus', 'uterine', 'blood'}\""
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sample from test set\n",
    "sample = X_test.iloc[13]\n",
    "sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [' Obstetrics / Gynecology']\n",
      "Actual:  Obstetrics / Gynecology\n",
      "                                Probability\n",
      " Obstetrics / Gynecology              0.605\n",
      " Surgery                              0.283\n",
      " Cardiovascular / Pulmonary           0.032\n",
      " Gastroenterology                     0.020\n",
      " Radiology                            0.017\n",
      " Orthopedic                           0.011\n",
      " Neurology                            0.008\n",
      " Consult - History and Phy.           0.008\n",
      " SOAP / Chart / Progress Notes        0.006\n",
      " Urology                              0.005\n",
      " General Medicine                     0.004\n"
     ]
    }
   ],
   "source": [
    "# Prediction for sample from test set\n",
    "sample = X_test.iloc[13]\n",
    "print(\"Prediction:\", best_model.predict([sample]))\n",
    "# Actual category of first sample from test set\n",
    "print(\"Actual:\", y_test[13])\n",
    "# Predict probabilities for a sample from test set\n",
    "prob_array = best_model.predict_proba(X_test)[13,:]\n",
    "prob_df = pd.DataFrame(prob_array, index=best_model.classes_, columns=['Probability']).sort_values(by='Probability', ascending=False)\n",
    "prob_df.Probability = prob_df.Probability.round(3)\n",
    "print(prob_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Probability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Obstetrics / Gynecology</th>\n",
       "      <td>0.299320</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Radiology</th>\n",
       "      <td>0.148991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Gastroenterology</th>\n",
       "      <td>0.123282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Cardiovascular / Pulmonary</th>\n",
       "      <td>0.070136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Surgery</th>\n",
       "      <td>0.069127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>General Medicine</th>\n",
       "      <td>0.062495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Consult - History and Phy.</th>\n",
       "      <td>0.061271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neurology</th>\n",
       "      <td>0.050716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Urology</th>\n",
       "      <td>0.039040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SOAP / Chart / Progress Notes</th>\n",
       "      <td>0.038013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Orthopedic</th>\n",
       "      <td>0.037610</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Probability\n",
       " Obstetrics / Gynecology           0.299320\n",
       " Radiology                         0.148991\n",
       " Gastroenterology                  0.123282\n",
       " Cardiovascular / Pulmonary        0.070136\n",
       " Surgery                           0.069127\n",
       " General Medicine                  0.062495\n",
       " Consult - History and Phy.        0.061271\n",
       " Neurology                         0.050716\n",
       " Urology                           0.039040\n",
       " SOAP / Chart / Progress Notes     0.038013\n",
       " Orthopedic                        0.037610"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_pred = \"lip abdomen needle endometrial wall anterior cervix\"\n",
    "def predict_probability(model: imblearn.pipeline.Pipeline, value) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    get probabilities for sample\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : imblearn.pipeline.Pipeline\n",
    "        best model from train.py\n",
    "    value : str\n",
    "        sample\n",
    "    category_list: list[str]\n",
    "        list of unique labels\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Probabilities for labels\n",
    "    \"\"\"\n",
    "\n",
    "    prob_array = model.predict_proba(value)\n",
    "    prob_df = pd.DataFrame(\n",
    "        prob_array, index=[\"Probability\"], columns=model.classes_\n",
    "    ).transpose().sort_values(by=\"Probability\", ascending=False)\n",
    "    return prob_df\n",
    "\n",
    "res_df = predict_probability(best_model, [to_pred])\n",
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['0007',\n",
       " '005',\n",
       " '01',\n",
       " '0125',\n",
       " '020',\n",
       " '025',\n",
       " '03',\n",
       " '0395',\n",
       " '05',\n",
       " '050',\n",
       " '075',\n",
       " '092assessment1',\n",
       " '10',\n",
       " '100',\n",
       " '1001',\n",
       " '1007',\n",
       " '100complications',\n",
       " '102',\n",
       " '103',\n",
       " '1032',\n",
       " '104',\n",
       " '107',\n",
       " '108',\n",
       " '10872',\n",
       " '109',\n",
       " '10drains',\n",
       " '11',\n",
       " '11000',\n",
       " '1100000',\n",
       " '11070',\n",
       " '12',\n",
       " '120',\n",
       " '1200000',\n",
       " '12161',\n",
       " '122',\n",
       " '126000',\n",
       " '129',\n",
       " '12959',\n",
       " '12yearold',\n",
       " '13',\n",
       " '131',\n",
       " '13172',\n",
       " '136',\n",
       " '137',\n",
       " '13878',\n",
       " '13975',\n",
       " '13gauge',\n",
       " '14',\n",
       " '14078',\n",
       " '14080',\n",
       " '143',\n",
       " '14383',\n",
       " '15',\n",
       " '150',\n",
       " '1500',\n",
       " '155',\n",
       " '158',\n",
       " '16',\n",
       " '16080',\n",
       " '168',\n",
       " '16870',\n",
       " '17',\n",
       " '17291',\n",
       " '18',\n",
       " '180110',\n",
       " '1937',\n",
       " '19373',\n",
       " '1cc',\n",
       " '1well',\n",
       " '20',\n",
       " '200',\n",
       " '2004',\n",
       " '2008',\n",
       " '2020',\n",
       " '2030',\n",
       " '204',\n",
       " '20meql',\n",
       " '21',\n",
       " '22',\n",
       " '223',\n",
       " '22591',\n",
       " '23',\n",
       " '24',\n",
       " '24hour',\n",
       " '25',\n",
       " '250',\n",
       " '25mg',\n",
       " '26',\n",
       " '269000',\n",
       " '27',\n",
       " '290',\n",
       " '297',\n",
       " '298',\n",
       " '2hypercholesterolemia',\n",
       " '30',\n",
       " '300',\n",
       " '313',\n",
       " '31493',\n",
       " '32',\n",
       " '32095',\n",
       " '32593',\n",
       " '32mm',\n",
       " '32yearold',\n",
       " '3431anesthesia',\n",
       " '35',\n",
       " '35000',\n",
       " '355c',\n",
       " '358fmsa',\n",
       " '36',\n",
       " '3695',\n",
       " '37',\n",
       " '372cms',\n",
       " '378',\n",
       " '37yearold',\n",
       " '3bipolar',\n",
       " '3rd',\n",
       " '40',\n",
       " '400',\n",
       " '40mg',\n",
       " '41',\n",
       " '412',\n",
       " '42',\n",
       " '425',\n",
       " '43',\n",
       " '45',\n",
       " '48',\n",
       " '484',\n",
       " '4r',\n",
       " '4th',\n",
       " '50',\n",
       " '500',\n",
       " '5050',\n",
       " '51yearold',\n",
       " '54',\n",
       " '56',\n",
       " '563',\n",
       " '5french',\n",
       " '60',\n",
       " '602',\n",
       " '6300',\n",
       " '64',\n",
       " '64yearold',\n",
       " '65',\n",
       " '66',\n",
       " '665',\n",
       " '6french',\n",
       " '6mm',\n",
       " '70',\n",
       " '704',\n",
       " '71',\n",
       " '713',\n",
       " '72',\n",
       " '74',\n",
       " '77',\n",
       " '78',\n",
       " '79',\n",
       " '80',\n",
       " '81',\n",
       " '81695in',\n",
       " '82',\n",
       " '83',\n",
       " '84',\n",
       " '84general',\n",
       " '86',\n",
       " '87',\n",
       " '88',\n",
       " '90',\n",
       " '91',\n",
       " '9163',\n",
       " '94',\n",
       " '95',\n",
       " '965',\n",
       " '975mr',\n",
       " '9th',\n",
       " 'a1c',\n",
       " 'abc',\n",
       " 'abd',\n",
       " 'abdomen',\n",
       " 'abdominal',\n",
       " 'abdominalpelvic',\n",
       " 'abdominis',\n",
       " 'abdominus',\n",
       " 'abductor',\n",
       " 'abf',\n",
       " 'accutane',\n",
       " 'ace',\n",
       " 'acetabular',\n",
       " 'acetaminophen',\n",
       " 'acetazolamide',\n",
       " 'acetowhite',\n",
       " 'acetylcholine',\n",
       " 'achieved',\n",
       " 'acl',\n",
       " 'acromial',\n",
       " 'activase',\n",
       " 'acular',\n",
       " 'adenocarcinoma',\n",
       " 'adenoid',\n",
       " 'adenoidal',\n",
       " 'adenoma',\n",
       " 'adenosine',\n",
       " 'adhesed',\n",
       " 'adipose',\n",
       " 'adnexa',\n",
       " 'adrenal',\n",
       " 'adrenaline',\n",
       " 'adriamycin',\n",
       " 'adventitial',\n",
       " 'afb',\n",
       " 'agarose',\n",
       " 'airspace',\n",
       " 'airway',\n",
       " 'alar',\n",
       " 'albuginea',\n",
       " 'albumin',\n",
       " 'albuminanesthesia',\n",
       " 'albuterol',\n",
       " 'alcohol',\n",
       " 'aldosterone',\n",
       " 'alfentanil',\n",
       " 'alkaline',\n",
       " 'alloderm',\n",
       " 'allograft',\n",
       " 'alpha1antitrypsin',\n",
       " 'alphablocker',\n",
       " 'alphablockers',\n",
       " 'alphafetoprotein',\n",
       " 'alprazolam',\n",
       " 'alt',\n",
       " 'alta',\n",
       " 'alupent',\n",
       " 'alveolar',\n",
       " 'amiodarone',\n",
       " 'amitriptyline',\n",
       " 'amniotic',\n",
       " 'amoxacillin',\n",
       " 'amoxicillin',\n",
       " 'amoxil',\n",
       " 'amphetamine',\n",
       " 'amphotericin',\n",
       " 'ampicillin',\n",
       " 'ampulla',\n",
       " 'amylase',\n",
       " 'ana',\n",
       " 'anal',\n",
       " 'andor',\n",
       " 'anesthesia',\n",
       " 'anesthetic',\n",
       " 'aneurysm',\n",
       " 'angelica',\n",
       " 'ani',\n",
       " 'annular',\n",
       " 'anorectal',\n",
       " 'anorectum',\n",
       " 'anserine',\n",
       " 'anserinus',\n",
       " 'antacid',\n",
       " 'antebrachial',\n",
       " 'antecubital',\n",
       " 'anterior',\n",
       " 'anteroapical',\n",
       " 'anterolateral',\n",
       " 'anteromedial',\n",
       " 'anteroseptal',\n",
       " 'anterosuperior',\n",
       " 'anthracotic',\n",
       " 'antigen',\n",
       " 'antihistamine',\n",
       " 'antineuronal',\n",
       " 'antiplatelet',\n",
       " 'antral',\n",
       " 'antrostomies',\n",
       " 'antrum',\n",
       " 'anus',\n",
       " 'anxiety2',\n",
       " 'aorta',\n",
       " 'apc',\n",
       " 'apical',\n",
       " 'apl',\n",
       " 'apolipoprotein',\n",
       " 'appendiceal',\n",
       " 'approximately',\n",
       " 'aqueous',\n",
       " 'arachnoid',\n",
       " 'arch',\n",
       " 'area',\n",
       " 'areolar',\n",
       " 'aricept',\n",
       " 'aromasin',\n",
       " 'arterial',\n",
       " 'arteriosus',\n",
       " 'arteriovenous',\n",
       " 'artery',\n",
       " 'articular',\n",
       " 'asacol',\n",
       " 'ascus',\n",
       " 'aseptically',\n",
       " 'aspirin',\n",
       " 'ast',\n",
       " 'astrocyte',\n",
       " 'astrocytoma',\n",
       " 'asv',\n",
       " 'atenolol',\n",
       " 'ativan',\n",
       " 'atlantis',\n",
       " 'atresia',\n",
       " 'atrial',\n",
       " 'atrioventricular',\n",
       " 'atrium',\n",
       " 'atropine',\n",
       " 'atrovent',\n",
       " 'augmentin',\n",
       " 'auricle',\n",
       " 'auricular',\n",
       " 'autoimmune',\n",
       " 'avascular',\n",
       " 'avastin',\n",
       " 'avelox',\n",
       " 'avf',\n",
       " 'axilla',\n",
       " 'axillary',\n",
       " 'axonal',\n",
       " 'azathioprine',\n",
       " 'azithromycin',\n",
       " 'azulfidine',\n",
       " 'azygos',\n",
       " 'b12',\n",
       " 'babcock',\n",
       " 'bacitracin',\n",
       " 'back',\n",
       " 'bald',\n",
       " 'balloon',\n",
       " 'barbotage',\n",
       " 'barium',\n",
       " 'barrett',\n",
       " 'barretts',\n",
       " 'bartholin',\n",
       " 'basal',\n",
       " 'basalganglia',\n",
       " 'basil',\n",
       " 'basilar',\n",
       " 'basilic',\n",
       " 'basket',\n",
       " 'basophilic',\n",
       " 'bay',\n",
       " 'bayer',\n",
       " 'bcell',\n",
       " 'bcnu',\n",
       " 'bed',\n",
       " 'beginning',\n",
       " 'belly',\n",
       " 'benzocaine',\n",
       " 'benzoin',\n",
       " 'benzoyl',\n",
       " 'berry',\n",
       " 'beta',\n",
       " 'betablockers',\n",
       " 'betadine',\n",
       " 'betamethasone',\n",
       " 'bibasilar',\n",
       " 'bicarbonate',\n",
       " 'bicep',\n",
       " 'biceps',\n",
       " 'bicipital',\n",
       " 'bicuspid',\n",
       " 'bid3',\n",
       " 'bilateral',\n",
       " 'bilaterally',\n",
       " 'bilaterallyneuro',\n",
       " 'bilaterallyskin',\n",
       " 'bile',\n",
       " 'bileaflet',\n",
       " 'biliary',\n",
       " 'bilirubin',\n",
       " 'billroth',\n",
       " 'bilobular',\n",
       " 'biomet',\n",
       " 'biopsy',\n",
       " 'biopsysamples',\n",
       " 'birads',\n",
       " 'birthdate',\n",
       " 'bitch',\n",
       " 'biventricular',\n",
       " 'bladder',\n",
       " 'blade',\n",
       " 'blake',\n",
       " 'bleeding2',\n",
       " 'bloc',\n",
       " 'block',\n",
       " 'blocked',\n",
       " 'blood',\n",
       " 'bloody',\n",
       " 'blowout',\n",
       " 'bmp',\n",
       " 'body',\n",
       " 'body2',\n",
       " 'body3',\n",
       " 'boggy',\n",
       " 'bone',\n",
       " 'boot',\n",
       " 'bos',\n",
       " 'botox',\n",
       " 'botulinum',\n",
       " 'bovie',\n",
       " 'bowed',\n",
       " 'bowel',\n",
       " 'bowman',\n",
       " 'bp',\n",
       " 'bp12157mmhg',\n",
       " 'bp12670',\n",
       " 'bp15786',\n",
       " 'bp16090',\n",
       " 'brachial',\n",
       " 'brachialis',\n",
       " 'brachiobasilic',\n",
       " 'brachiocephalic',\n",
       " 'brachioradialis',\n",
       " 'brachium',\n",
       " 'brain',\n",
       " 'brainstem',\n",
       " 'branchial',\n",
       " 'branchii',\n",
       " 'brat',\n",
       " 'breast',\n",
       " 'brenner',\n",
       " 'bretylium',\n",
       " 'brevis',\n",
       " 'brevital',\n",
       " 'briefly',\n",
       " 'bronchioalveolar',\n",
       " 'bronchiole',\n",
       " 'bronchioloalveolar',\n",
       " 'bronchoalveolar',\n",
       " 'bronchogenic',\n",
       " 'bronchomalacia',\n",
       " 'bronchoscope',\n",
       " 'bronchus',\n",
       " 'broviac',\n",
       " 'brow',\n",
       " 'brown',\n",
       " 'brownish',\n",
       " 'brush',\n",
       " 'buccal',\n",
       " 'buck',\n",
       " 'bulb',\n",
       " 'bulbocavernosus',\n",
       " 'bulbous',\n",
       " 'bullet',\n",
       " 'bun',\n",
       " 'bupivacaine',\n",
       " 'bursa',\n",
       " 'bursal',\n",
       " 'butter',\n",
       " 'buttock',\n",
       " 'buttonholed',\n",
       " 'c1',\n",
       " 'c2',\n",
       " 'c3',\n",
       " 'c3c4',\n",
       " 'c4c5',\n",
       " 'c5',\n",
       " 'c54',\n",
       " 'c5c64',\n",
       " 'c5c7',\n",
       " 'c7t1',\n",
       " 'cad',\n",
       " 'caffeine',\n",
       " 'calcaneus',\n",
       " 'calcium',\n",
       " 'calf',\n",
       " 'caliber',\n",
       " 'caliceal',\n",
       " 'california',\n",
       " 'callosum',\n",
       " 'callus',\n",
       " 'calvarial',\n",
       " 'calvarium',\n",
       " 'calyceal',\n",
       " 'calyx',\n",
       " 'came',\n",
       " 'canal',\n",
       " 'canalicular',\n",
       " 'canaliculus',\n",
       " 'cancellous',\n",
       " 'cancer',\n",
       " 'cancerous',\n",
       " 'cane',\n",
       " 'canthal',\n",
       " 'capillary',\n",
       " 'capoten',\n",
       " 'capsular',\n",
       " 'capsule',\n",
       " 'carbohydrate',\n",
       " 'carbondioxide',\n",
       " 'carboplatin',\n",
       " 'carcinoma',\n",
       " 'carcinomaprocedures1',\n",
       " 'carcinomatumor',\n",
       " 'cardia',\n",
       " 'cardiac',\n",
       " 'cardioesophageal',\n",
       " 'cardioplegia',\n",
       " 'care',\n",
       " 'carina',\n",
       " 'carious',\n",
       " 'carmine',\n",
       " 'carnitine',\n",
       " 'carotid',\n",
       " 'carotids',\n",
       " 'carrell',\n",
       " 'cartilage',\n",
       " 'carvedilol',\n",
       " 'catheter',\n",
       " 'catheteranesthesia',\n",
       " 'catheterization4',\n",
       " 'caucasian',\n",
       " 'cauda',\n",
       " 'caudal',\n",
       " 'caudate',\n",
       " 'cauteryanesthetic',\n",
       " 'cava',\n",
       " 'cavernosum',\n",
       " 'cavitary',\n",
       " 'cavum',\n",
       " 'cbc',\n",
       " 'cc',\n",
       " 'cccomplications',\n",
       " 'ccfluids',\n",
       " 'cchistory',\n",
       " 'ccollar',\n",
       " 'ccspecimens',\n",
       " 'ccurine',\n",
       " 'cd4',\n",
       " 'cecal',\n",
       " 'cecum',\n",
       " 'cefazolin',\n",
       " 'cefotaxime',\n",
       " 'ceftriaxone',\n",
       " 'celebrex',\n",
       " 'celestone',\n",
       " 'cell',\n",
       " 'cellular',\n",
       " 'centerin',\n",
       " 'centimeter',\n",
       " 'central',\n",
       " 'centrum',\n",
       " 'cephalad',\n",
       " 'cephalic',\n",
       " 'cephazolin',\n",
       " 'cerebellar',\n",
       " 'cerebelli',\n",
       " 'cerebellum',\n",
       " 'cerebrospinal',\n",
       " 'cerebrovascular',\n",
       " 'cerumen',\n",
       " 'cervical',\n",
       " 'cervicalis',\n",
       " 'cervicovaginal',\n",
       " 'cervix',\n",
       " 'cetacaine',\n",
       " 'chang',\n",
       " 'cheek',\n",
       " 'cheese',\n",
       " 'chiasm',\n",
       " 'chlorambucil',\n",
       " 'chlorhexidine',\n",
       " 'chloride',\n",
       " 'choana',\n",
       " 'cholangiogram',\n",
       " 'cholecystic',\n",
       " 'cholecystitis',\n",
       " 'cholecystogram',\n",
       " 'cholesteatoma',\n",
       " 'cholesterol',\n",
       " 'chondral',\n",
       " 'chondroitin',\n",
       " 'chondromalacia4',\n",
       " 'chondrosarcoma',\n",
       " 'chorda',\n",
       " 'chordae',\n",
       " 'chorioretinal',\n",
       " 'choroid',\n",
       " 'choroidal',\n",
       " 'chromogranin',\n",
       " 'chyle',\n",
       " 'cialis',\n",
       " 'cicatrix',\n",
       " 'ciliary',\n",
       " 'cin',\n",
       " 'cineangiography3',\n",
       " 'cipro',\n",
       " 'ciprofloxacin',\n",
       " 'circ',\n",
       " 'circumferential',\n",
       " 'circumflex',\n",
       " 'cisplatin',\n",
       " 'cisterna',\n",
       " 'citalopram',\n",
       " 'cl',\n",
       " 'claustrum',\n",
       " 'clavicular',\n",
       " 'clearcardiovascular',\n",
       " 'cleft',\n",
       " 'cleocin',\n",
       " 'clindamycin',\n",
       " 'clinic',\n",
       " 'clinoid',\n",
       " 'clip',\n",
       " 'clitoromegaly',\n",
       " 'clivus',\n",
       " 'cll2',\n",
       " 'clobazam',\n",
       " 'clonazepam',\n",
       " 'clonidine',\n",
       " 'closed',\n",
       " 'closure',\n",
       " 'clot',\n",
       " 'clozaril',\n",
       " 'cm',\n",
       " 'cm2',\n",
       " 'cmv',\n",
       " 'cmvisceral',\n",
       " 'cns',\n",
       " 'co2',\n",
       " 'cobalt',\n",
       " 'cocaine',\n",
       " 'cocci',\n",
       " 'coccygeal',\n",
       " 'codeine',\n",
       " 'codman',\n",
       " 'colestid',\n",
       " 'collage',\n",
       " 'collagen',\n",
       " 'collarbone',\n",
       " 'collateral',\n",
       " 'collection2',\n",
       " 'colon',\n",
       " 'colonic',\n",
       " 'colorectal',\n",
       " 'columella',\n",
       " 'columellar',\n",
       " 'commissural',\n",
       " 'commissure',\n",
       " 'communis',\n",
       " 'compazine',\n",
       " 'completed',\n",
       " 'completeimpression1',\n",
       " 'concha',\n",
       " 'condyle',\n",
       " 'condylomatous',\n",
       " 'cone',\n",
       " 'conjunctiva',\n",
       " 'conray',\n",
       " 'consisting',\n",
       " 'containing',\n",
       " 'contin',\n",
       " 'conus',\n",
       " 'cookie',\n",
       " 'copaxone',\n",
       " 'copdexam',\n",
       " 'coracoacromial',\n",
       " 'coracoclavicular',\n",
       " 'coracoid',\n",
       " 'cord',\n",
       " 'cornea',\n",
       " 'corneal',\n",
       " 'corneoscleral',\n",
       " 'cornu',\n",
       " 'corona',\n",
       " 'coronary',\n",
       " 'corporal',\n",
       " 'corpus',\n",
       " 'corpuscular',\n",
       " 'cortex',\n",
       " 'cortical',\n",
       " 'corticosteroid',\n",
       " 'cortisone',\n",
       " 'cortisporin',\n",
       " 'cortrosyn',\n",
       " 'costovertebral',\n",
       " 'coumadin',\n",
       " 'count',\n",
       " 'cranial',\n",
       " 'craniopharyngioma',\n",
       " 'cream',\n",
       " 'crease',\n",
       " 'creatinine',\n",
       " 'cremasteric',\n",
       " 'cricoid',\n",
       " 'cricopharyngeus',\n",
       " 'cricothyroid',\n",
       " 'crossclamped',\n",
       " 'crp',\n",
       " 'cruciate',\n",
       " 'crural',\n",
       " 'cryoprocedure',\n",
       " 'crypt',\n",
       " 'crystalloid',\n",
       " 'crystalloidurine',\n",
       " 'csf11993',\n",
       " 'ctap',\n",
       " 'cubital',\n",
       " 'culture',\n",
       " 'curetted',\n",
       " 'curettings',\n",
       " 'cuspid',\n",
       " 'cyclogyl',\n",
       " 'cyclophosphamide',\n",
       " 'cyclosporin',\n",
       " 'cymbalta',\n",
       " 'cystic',\n",
       " 'cysto',\n",
       " 'cystoid',\n",
       " 'cytobrush',\n",
       " 'cytokeratin',\n",
       " 'cytomolecular',\n",
       " 'cytoplasmic',\n",
       " 'cytoxan',\n",
       " 'day',\n",
       " 'days2',\n",
       " 'decadron',\n",
       " 'decidual',\n",
       " 'decline',\n",
       " 'decreased',\n",
       " 'decubitus',\n",
       " 'deferens',\n",
       " 'deltoid',\n",
       " 'demerol',\n",
       " 'denies',\n",
       " 'dentin',\n",
       " 'depakene',\n",
       " 'depakote',\n",
       " 'depomedrol',\n",
       " 'dermal',\n",
       " 'desire',\n",
       " 'dexamethasone',\n",
       " 'diaphragm',\n",
       " 'diastase',\n",
       " 'diazepam',\n",
       " 'digastric',\n",
       " 'digitorum',\n",
       " 'digoxin',\n",
       " 'dilantin',\n",
       " 'dilaudid',\n",
       " 'diltiazem',\n",
       " 'dioxide',\n",
       " 'diploic',\n",
       " 'disc',\n",
       " 'diverticular',\n",
       " 'dobutamine',\n",
       " 'dolichoectasia',\n",
       " 'dolichoectatic',\n",
       " 'dopamine',\n",
       " 'dorsal',\n",
       " 'dorsi',\n",
       " 'dorsiflexors',\n",
       " 'dose',\n",
       " 'doxorubicin',\n",
       " 'doxycycline',\n",
       " 'droplet',\n",
       " 'duct',\n",
       " 'duodenal',\n",
       " 'duodenum',\n",
       " 'dura',\n",
       " 'dural',\n",
       " 'dyspepsia',\n",
       " 'dyssynergia',\n",
       " 'ear',\n",
       " 'eardrum',\n",
       " 'ebu',\n",
       " 'ecchymotic',\n",
       " 'ecstasy',\n",
       " 'edema',\n",
       " 'edematous',\n",
       " 'effaces',\n",
       " 'efferent',\n",
       " 'egg',\n",
       " 'ejaculatory',\n",
       " 'electrolyte',\n",
       " 'elevated',\n",
       " 'elixir',\n",
       " 'eminence',\n",
       " 'emissary',\n",
       " 'emphysema',\n",
       " 'enalapril',\n",
       " 'end',\n",
       " 'enddiastolic',\n",
       " 'endobronchial',\n",
       " 'endocervical',\n",
       " 'endocrine',\n",
       " 'endometrial',\n",
       " 'endometrium',\n",
       " 'endopelvic',\n",
       " 'endothoracic',\n",
       " 'endotracheal',\n",
       " 'endovascular',\n",
       " 'endplate',\n",
       " 'enema',\n",
       " 'enteral',\n",
       " 'envelope',\n",
       " 'eom',\n",
       " 'epicardial',\n",
       " 'epicardium',\n",
       " 'epicondylar',\n",
       " 'epicondyle',\n",
       " 'epicortex',\n",
       " 'epidermal',\n",
       " 'epidermis',\n",
       " 'epididymal',\n",
       " 'epididymidis',\n",
       " 'epididymis',\n",
       " 'epidural',\n",
       " 'epigastric',\n",
       " 'epigastrium',\n",
       " 'epiglottic',\n",
       " 'epinephrine',\n",
       " 'epineurium',\n",
       " 'epinuclear',\n",
       " 'epiretinal',\n",
       " 'episcleral',\n",
       " 'epithelial',\n",
       " 'epithelium',\n",
       " 'epitrochlear',\n",
       " 'epl',\n",
       " 'equina',\n",
       " 'erector',\n",
       " 'erythematous',\n",
       " 'erythromycin',\n",
       " 'eserine',\n",
       " 'esmolol',\n",
       " 'esophageal',\n",
       " 'esophagitis',\n",
       " 'esophagus',\n",
       " 'estrace',\n",
       " 'estradiol',\n",
       " 'estrogen',\n",
       " 'ethanol',\n",
       " 'ethmoid',\n",
       " 'ethmoidal',\n",
       " 'eugenol',\n",
       " 'eustachian',\n",
       " 'evaluation',\n",
       " 'event',\n",
       " 'examneurovascular',\n",
       " 'excised',\n",
       " 'exophytic',\n",
       " 'expander',\n",
       " 'expanders',\n",
       " 'extensor',\n",
       " 'extraarticular',\n",
       " 'extrahepatic',\n",
       " 'extranodal',\n",
       " 'extraocular',\n",
       " 'extraoral',\n",
       " 'extrapulmonary',\n",
       " 'exudate',\n",
       " 'eye',\n",
       " 'eyeball',\n",
       " 'eyedrop',\n",
       " 'eyelid',\n",
       " 'fallopian',\n",
       " 'fascia',\n",
       " 'fascial',\n",
       " 'fascicular',\n",
       " 'fashion',\n",
       " 'fat',\n",
       " 'fatigue3',\n",
       " 'fatty',\n",
       " 'faucial',\n",
       " 'fdp',\n",
       " 'feed',\n",
       " 'feeder',\n",
       " 'femoral',\n",
       " 'femorals',\n",
       " 'femoris',\n",
       " 'femorosaphenous',\n",
       " 'femorotibial',\n",
       " 'femur',\n",
       " 'fenofibrate',\n",
       " 'fenoldopam',\n",
       " 'fentanyl',\n",
       " 'ferric',\n",
       " 'ferritin',\n",
       " 'ferrous',\n",
       " 'fetal',\n",
       " 'fhx',\n",
       " 'fiber',\n",
       " 'fibrin',\n",
       " 'fibrinous',\n",
       " 'fibroadenoma',\n",
       " 'fibroglandular',\n",
       " 'fibroid',\n",
       " 'fibromucosa',\n",
       " 'fibromuscular',\n",
       " 'fibula',\n",
       " 'fibular',\n",
       " 'filamentous',\n",
       " 'filled',\n",
       " 'fimbria',\n",
       " 'fimbriated',\n",
       " 'fineneedle',\n",
       " 'fingerbreadth',\n",
       " 'fingernail',\n",
       " 'fingertip',\n",
       " 'fishbone',\n",
       " 'fistulous',\n",
       " 'fixed',\n",
       " 'flabby',\n",
       " 'flagyl',\n",
       " 'flail',\n",
       " 'flaky',\n",
       " 'flank',\n",
       " 'flaxseed',\n",
       " 'flexeril',\n",
       " 'flexure',\n",
       " 'flooded',\n",
       " 'floor',\n",
       " 'florinef',\n",
       " 'floxin',\n",
       " 'fludrocortisone',\n",
       " 'fluid',\n",
       " 'fluids2lurine',\n",
       " 'fluorescein',\n",
       " 'fluoride',\n",
       " 'fluoxetine',\n",
       " 'flushed',\n",
       " 'focal',\n",
       " 'fogarty',\n",
       " 'folate',\n",
       " 'folic',\n",
       " 'follicle',\n",
       " 'follicular',\n",
       " 'followed',\n",
       " 'fontanelle',\n",
       " 'foot',\n",
       " 'footanesthesia',\n",
       " 'foramen',\n",
       " 'forearm',\n",
       " 'forefoot',\n",
       " 'forehead',\n",
       " 'foreskin',\n",
       " 'fork',\n",
       " 'fornix',\n",
       " 'fosphenytoin',\n",
       " 'foveal',\n",
       " 'fraction',\n",
       " 'frayed',\n",
       " 'fremitus',\n",
       " 'french',\n",
       " 'frenulum',\n",
       " 'fresh',\n",
       " 'friable',\n",
       " 'frontal',\n",
       " 'frontoorbital',\n",
       " 'frontoparietal',\n",
       " 'frontotemporal',\n",
       " 'frontotemporoparietal',\n",
       " 'ft4',\n",
       " 'fundal',\n",
       " 'fundic',\n",
       " 'fundus',\n",
       " 'furosemide',\n",
       " 'fusiform',\n",
       " 'gabapentin',\n",
       " 'gadolinium',\n",
       " 'gag',\n",
       " 'galea',\n",
       " 'gallbladder',\n",
       " 'ganciclovir',\n",
       " 'ganglion',\n",
       " 'gangrenous',\n",
       " 'gastric',\n",
       " 'gastroc',\n",
       " 'gastrocnemius',\n",
       " 'gastrocsoleus',\n",
       " 'gastroepiploic',\n",
       " 'gastroesophageal',\n",
       " 'gastrohepatic',\n",
       " 'gastrointestinal',\n",
       " 'gastrojejunal',\n",
       " 'gbq',\n",
       " 'gemelli',\n",
       " 'gemfibrozil',\n",
       " 'general',\n",
       " 'genital',\n",
       " 'genitourinary',\n",
       " 'gentamicin',\n",
       " 'gentamycin',\n",
       " 'gentleman',\n",
       " 'gerd',\n",
       " 'gfap',\n",
       " 'gfr',\n",
       " 'gia',\n",
       " 'gingiva',\n",
       " 'given',\n",
       " 'glabella',\n",
       " 'glabellar',\n",
       " 'gland',\n",
       " 'glandular',\n",
       " 'glans',\n",
       " ...]"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# just for exploration of features, not needed for further code\n",
    "X_train_df = X_train.to_frame()\n",
    "vectorizer = CountVectorizer()\n",
    "matrix = vectorizer.fit_transform(X_train_df[\"transcription_f\"])\n",
    "feat_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "feat = list(feat_df.columns)\n",
    "print(len(feat))\n",
    "feat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test lime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lime does not work here but works in notebook 02-hp-lrmodel-nlpinput.ipynb\n",
      "difference of X_train split before or after pipeline\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "tuple index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [53], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mlime\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mlime_tabular\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlime does not work here but works in notebook 02-hp-lrmodel-nlpinput.ipynb\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mdifference of X_train split before or after pipeline\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 5\u001b[0m explainer \u001b[39m=\u001b[39m lime\u001b[39m.\u001b[39mlime_tabular\u001b[39m.\u001b[39mLimeTabularExplainer(X_train, feature_names\u001b[39m=\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out(), class_names\u001b[39m=\u001b[39mbest_model\u001b[39m.\u001b[39mclasses_)\n\u001b[1;32m      6\u001b[0m \u001b[39m# num features is the number of features to be shown\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# top labels is the number of labels with the highest probability to be shown\u001b[39;00m\n\u001b[1;32m      8\u001b[0m exp \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mexplain_instance(X_test[\u001b[39m4\u001b[39m], best_model\u001b[39m.\u001b[39mpredict_proba, num_features\u001b[39m=\u001b[39m\u001b[39m5\u001b[39m, top_labels\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/lime/lime_tabular.py:215\u001b[0m, in \u001b[0;36mLimeTabularExplainer.__init__\u001b[0;34m(self, training_data, mode, training_labels, feature_names, categorical_features, categorical_names, kernel_width, kernel, verbose, class_names, feature_selection, discretize_continuous, discretizer, sample_around_instance, random_state, training_data_stats)\u001b[0m\n\u001b[1;32m    209\u001b[0m     discretizer \u001b[39m=\u001b[39m StatsDiscretizer(training_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_features,\n\u001b[1;32m    210\u001b[0m                                    \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, labels\u001b[39m=\u001b[39mtraining_labels,\n\u001b[1;32m    211\u001b[0m                                    data_stats\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_data_stats,\n\u001b[1;32m    212\u001b[0m                                    random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n\u001b[1;32m    214\u001b[0m \u001b[39mif\u001b[39;00m discretizer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mquartile\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 215\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretizer \u001b[39m=\u001b[39m QuartileDiscretizer(\n\u001b[1;32m    216\u001b[0m             training_data, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcategorical_features,\n\u001b[1;32m    217\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeature_names, labels\u001b[39m=\u001b[39;49mtraining_labels,\n\u001b[1;32m    218\u001b[0m             random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state)\n\u001b[1;32m    219\u001b[0m \u001b[39melif\u001b[39;00m discretizer \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mdecile\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    220\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdiscretizer \u001b[39m=\u001b[39m DecileDiscretizer(\n\u001b[1;32m    221\u001b[0m             training_data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcategorical_features,\n\u001b[1;32m    222\u001b[0m             \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeature_names, labels\u001b[39m=\u001b[39mtraining_labels,\n\u001b[1;32m    223\u001b[0m             random_state\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrandom_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/lime/discretize.py:178\u001b[0m, in \u001b[0;36mQuartileDiscretizer.__init__\u001b[0;34m(self, data, categorical_features, feature_names, labels, random_state)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, categorical_features, feature_names, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 178\u001b[0m     BaseDiscretizer\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39mself\u001b[39;49m, data, categorical_features,\n\u001b[1;32m    179\u001b[0m                              feature_names, labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m    180\u001b[0m                              random_state\u001b[39m=\u001b[39;49mrandom_state)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/lime/discretize.py:39\u001b[0m, in \u001b[0;36mBaseDiscretizer.__init__\u001b[0;34m(self, data, categorical_features, feature_names, labels, random_state, data_stats)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, data, categorical_features, feature_names, labels\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, random_state\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m,\n\u001b[1;32m     23\u001b[0m              data_stats\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m     24\u001b[0m     \u001b[39m\"\"\"Initializer\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[39m        data: numpy 2d array\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[39m            if you don't want these values to be computed from data\u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_discretize \u001b[39m=\u001b[39m ([x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(data\u001b[39m.\u001b[39;49mshape[\u001b[39m1\u001b[39;49m])\n\u001b[1;32m     40\u001b[0m                            \u001b[39mif\u001b[39;00m x \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m categorical_features])\n\u001b[1;32m     41\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata_stats \u001b[39m=\u001b[39m data_stats\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnames \u001b[39m=\u001b[39m {}\n",
      "\u001b[0;31mIndexError\u001b[0m: tuple index out of range"
     ]
    }
   ],
   "source": [
    "import lime\n",
    "import lime.lime_tabular\n",
    "print(\"lime does not work here but works in notebook 02-hp-lrmodel-nlpinput.ipynb\\ndifference of X_train split before or after pipeline\")\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(X_train, feature_names=vectorizer.get_feature_names_out(), class_names=best_model.classes_)\n",
    "# num features is the number of features to be shown\n",
    "# top labels is the number of labels with the highest probability to be shown\n",
    "exp = explainer.explain_instance(X_test[4], best_model.predict_proba, num_features=5, top_labels=2)\n",
    "exp.show_in_notebook(show_table=True, show_all=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Shap "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "lower not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [84], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mshap\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpydoc\u001b[39;00m \u001b[39mimport\u001b[39;00m classname\n\u001b[0;32m----> 3\u001b[0m X_train \u001b[39m=\u001b[39m model_pipeline\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39mpreprocessing\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfit_transform(X_train)\n\u001b[1;32m      4\u001b[0m X_train \u001b[39m=\u001b[39m model_pipeline\u001b[39m.\u001b[39mnamed_steps[\u001b[39m'\u001b[39m\u001b[39msmote\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mfit_transform(X_train)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1209\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1207\u001b[0m \u001b[39mfor\u001b[39;00m doc \u001b[39min\u001b[39;00m raw_documents:\n\u001b[1;32m   1208\u001b[0m     feature_counter \u001b[39m=\u001b[39m {}\n\u001b[0;32m-> 1209\u001b[0m     \u001b[39mfor\u001b[39;00m feature \u001b[39min\u001b[39;00m analyze(doc):\n\u001b[1;32m   1210\u001b[0m         \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1211\u001b[0m             feature_idx \u001b[39m=\u001b[39m vocabulary[feature]\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:111\u001b[0m, in \u001b[0;36m_analyze\u001b[0;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    110\u001b[0m     \u001b[39mif\u001b[39;00m preprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 111\u001b[0m         doc \u001b[39m=\u001b[39m preprocessor(doc)\n\u001b[1;32m    112\u001b[0m     \u001b[39mif\u001b[39;00m tokenizer \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    113\u001b[0m         doc \u001b[39m=\u001b[39m tokenizer(doc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:69\u001b[0m, in \u001b[0;36m_preprocess\u001b[0;34m(doc, accent_function, lower)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39m\"\"\"Chain together an optional series of text preprocessing steps to\u001b[39;00m\n\u001b[1;32m     51\u001b[0m \u001b[39mapply to a document.\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[39m    preprocessed string\u001b[39;00m\n\u001b[1;32m     67\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     68\u001b[0m \u001b[39mif\u001b[39;00m lower:\n\u001b[0;32m---> 69\u001b[0m     doc \u001b[39m=\u001b[39m doc\u001b[39m.\u001b[39;49mlower()\n\u001b[1;32m     70\u001b[0m \u001b[39mif\u001b[39;00m accent_function \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m     71\u001b[0m     doc \u001b[39m=\u001b[39m accent_function(doc)\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/scipy/sparse/_base.py:771\u001b[0m, in \u001b[0;36mspmatrix.__getattr__\u001b[0;34m(self, attr)\u001b[0m\n\u001b[1;32m    769\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgetnnz()\n\u001b[1;32m    770\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 771\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(attr \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m not found\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: lower not found"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from pydoc import classname\n",
    "X_train = model_pipeline.named_steps['preprocessing'].fit_transform(X_train)\n",
    "X_train = model_pipeline.named_steps['smote'].fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using 2380 background data samples could cause slower run times. Consider using shap.sample(data, K) or shap.kmeans(data, K) to summarize the background as K samples.\n"
     ]
    }
   ],
   "source": [
    "explainer = shap.KernelExplainer(model_pipeline.named_steps['classifier'].predict_proba, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [75], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mshap_values(X_test)\n\u001b[1;32m      2\u001b[0m shap\u001b[39m.\u001b[39msummary_plot(shap_values, X_test, plot_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbar\u001b[39m\u001b[39m\"\u001b[39m, class_names\u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mclasses_, feature_names\u001b[39m=\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/shap/explainers/_kernel.py:167\u001b[0m, in \u001b[0;36mKernel.shap_values\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_index:\n\u001b[1;32m    166\u001b[0m     data \u001b[39m=\u001b[39m convert_to_instance_with_index(data, column_name, index_name, index_value)\n\u001b[0;32m--> 167\u001b[0m explanation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexplain(data, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    169\u001b[0m \u001b[39m# vector-output\u001b[39;00m\n\u001b[1;32m    170\u001b[0m s \u001b[39m=\u001b[39m explanation\u001b[39m.\u001b[39mshape\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/shap/explainers/_kernel.py:217\u001b[0m, in \u001b[0;36mKernel.explain\u001b[0;34m(self, incoming_instance, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m match_instance_to_data(instance, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata)\n\u001b[1;32m    215\u001b[0m \u001b[39m# find the feature groups we will test. If a feature does not change from its\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[39m# current value then we know it doesn't impact the model\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvaryingInds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mvarying_groups(instance\u001b[39m.\u001b[39;49mx)\n\u001b[1;32m    218\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mgroups \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvaryingFeatureGroups \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray([i \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvaryingInds])\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/shap/explainers/_kernel.py:410\u001b[0m, in \u001b[0;36mKernel.varying_groups\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    408\u001b[0m varying \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mgroups_size)\n\u001b[1;32m    409\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m0\u001b[39m, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mgroups_size):\n\u001b[0;32m--> 410\u001b[0m     inds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mgroups[i]\n\u001b[1;32m    411\u001b[0m     x_group \u001b[39m=\u001b[39m x[\u001b[39m0\u001b[39m, inds]\n\u001b[1;32m    412\u001b[0m     \u001b[39mif\u001b[39;00m sp\u001b[39m.\u001b[39msparse\u001b[39m.\u001b[39missparse(x_group):\n",
      "\u001b[0;31mTypeError\u001b[0m: 'NoneType' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\", class_names= lr.classes_, feature_names=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lime does not work here but works in notebook 02-hp-lrmodel-nlpinput.ipynb\n",
      "difference of X_train split before or after pipeline\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'warnings' has no attribute 'DeprecationWarning'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [60], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mlime does not work here but works in notebook 02-hp-lrmodel-nlpinput.ipynb\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mdifference of X_train split before or after pipeline\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m      4\u001b[0m explainer \u001b[39m=\u001b[39m shap\u001b[39m.\u001b[39mExplainer(lr\u001b[39m.\u001b[39mpredict, X_train)\n\u001b[0;32m----> 5\u001b[0m shap_values \u001b[39m=\u001b[39m explainer\u001b[39m.\u001b[39mshap_values(X_test)\n\u001b[1;32m      6\u001b[0m shap\u001b[39m.\u001b[39msummary_plot(shap_values, X_test, class_names\u001b[39m=\u001b[39m lr\u001b[39m.\u001b[39mclasses_, feature_names\u001b[39m=\u001b[39mvectorizer\u001b[39m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/shap/explainers/_permutation.py:214\u001b[0m, in \u001b[0;36mPermutation.shap_values\u001b[0;34m(self, X, npermutations, main_effects, error_bounds, batch_evals, silent)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mshap_values\u001b[39m(\u001b[39mself\u001b[39m, X, npermutations\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m, main_effects\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, error_bounds\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, batch_evals\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, silent\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m):\n\u001b[1;32m    191\u001b[0m     \u001b[39m\"\"\" Legacy interface to estimate the SHAP values for a set of samples.\u001b[39;00m\n\u001b[1;32m    192\u001b[0m \n\u001b[1;32m    193\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[39m        of such matrices, one for each output.\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 214\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\u001b[39m\"\u001b[39m\u001b[39mshap_values() is deprecated; use __call__().\u001b[39m\u001b[39m\"\u001b[39m, warnings\u001b[39m.\u001b[39;49mDeprecationWarning)\n\u001b[1;32m    216\u001b[0m     explanation \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m(X, max_evals\u001b[39m=\u001b[39mnpermutations \u001b[39m*\u001b[39m X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], main_effects\u001b[39m=\u001b[39mmain_effects)\n\u001b[1;32m    217\u001b[0m     \u001b[39mreturn\u001b[39;00m explanation\u001b[39m.\u001b[39mvalues\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'warnings' has no attribute 'DeprecationWarning'"
     ]
    }
   ],
   "source": [
    "import shap\n",
    "from pydoc import classname\n",
    "print(\"lime does not work here but works in notebook 02-hp-lrmodel-nlpinput.ipynb\\ndifference of X_train split before or after pipeline\")\n",
    "explainer = shap.Explainer(lr.predict, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test, class_names= lr.classes_, feature_names=vectorizer.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b1167c26af54c6340f890887cef801dce27ac37ff6fc6a99bb1eb896d088a43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
