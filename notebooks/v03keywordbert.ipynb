{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keybert import KeyBERT\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocess "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /Users/tara-\n",
      "[nltk_data]     sophiatumbraegel/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "df = pd.read_csv('../data/raw/nlp/mtsamples/mtsamples.csv')\n",
    "df = df.dropna()\n",
    "df['transcription'] = df['transcription'].tolist()\n",
    "df #= df.head(200)\n",
    "\n",
    "def convert(lst_kw):\n",
    "    '''This function converts the keywords to a list of keywords'''\n",
    "    list_keywords = lst_kw.split(',')\n",
    "    return list_keywords\n",
    "df['keywords']=df['keywords'].fillna(\"\")\n",
    "df['keywords_list']= df['keywords'].apply(lambda x: x.split(','))\n",
    "\n",
    "\n",
    "\n",
    "def location_indices(stringstext, check_list):\n",
    "    '''this function finds the location of the keywords in the string'''\n",
    "    \n",
    "    res = dict()\n",
    "    for ele in check_list :\n",
    "        if ele in stringstext:\n",
    "            # getting front index\n",
    "            strt = stringstext.index(ele)\n",
    "            \n",
    "            # getting ending index\n",
    "            res[ele] = [strt, strt + len(ele) - 1]\n",
    "    return res.values()\n",
    "        \n",
    "df['location'] = df.apply(lambda x: location_indices(x.transcription, x.keywords_list), axis=1) \n",
    "\n",
    "\n",
    "\n",
    "def location_indices(stringstext, check_list):\n",
    "    '''this function finds the location of the keywords in the string'''\n",
    "    \n",
    "    res = dict()\n",
    "    for ele in check_list :\n",
    "        if ele in stringstext:\n",
    "            # getting front index\n",
    "            strt = stringstext.index(ele)\n",
    "            \n",
    "            # getting ending index\n",
    "            res[ele] = [strt, strt + len(ele) - 1]\n",
    "    return res.values()\n",
    "        \n",
    "df['location'] = df.apply(lambda x: location_indices(x.transcription, x.keywords_list), axis=1) \n",
    "\n",
    "\n",
    "import string\n",
    "def cleaning(sentence):\n",
    "    '''This function takes a column and cleans it by removing punctuation, stopwords, and lemmatizing\n",
    "    NEXT STEPS : personalize the stop words list, check for different aspects of the words\n",
    "    '''\n",
    "    \n",
    "    # Basic cleaning\n",
    "    sentence = sentence.strip() ## remove whitespaces\n",
    "    sentence = sentence.lower() ## lowercase \n",
    "    sentence = ''.join(char for char in sentence if not char.isdigit()) ## remove numbers\n",
    "    \n",
    "    # Advanced cleaning\n",
    "    for punctuation in string.punctuation:\n",
    "        sentence = sentence.replace(punctuation, '') ## remove punctuation\n",
    "    \n",
    "    tokenized_sentence = word_tokenize(sentence) ## tokenize \n",
    "    stop_words = set(stopwords.words('english')) ## define stopwords\n",
    "    \n",
    "    tokenized_sentence_cleaned = [ ## remove stopwords\n",
    "        w for w in tokenized_sentence if not w in stop_words\n",
    "    ]\n",
    "\n",
    "    lemmatized = [\n",
    "        WordNetLemmatizer().lemmatize(word, pos = \"v\") \n",
    "        for word in tokenized_sentence_cleaned\n",
    "    ]\n",
    "    \n",
    "    cleaned_sentence = ' '.join(word for word in lemmatized)\n",
    "    \n",
    "    return cleaned_sentence\n",
    "df[\"transcription\"] = df[\"transcription\"].apply(cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df.medical_specialty.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prepare data from the medical specialty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "      <th>location</th>\n",
       "      <th>labels_val</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>A 23-year-old white female presents with comp...</td>\n",
       "      <td>Allergy / Immunology</td>\n",
       "      <td>Allergic Rhinitis</td>\n",
       "      <td>subjective yearold white female present compla...</td>\n",
       "      <td>allergy / immunology, allergic rhinitis, aller...</td>\n",
       "      <td>[allergy / immunology,  allergic rhinitis,  al...</td>\n",
       "      <td>([70, 79], [519, 525], [490, 502], [1036, 1044...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>Consult for laparoscopic gastric bypass.</td>\n",
       "      <td>Bariatrics</td>\n",
       "      <td>Laparoscopic Gastric Bypass Consult - 2</td>\n",
       "      <td>past medical history difficulty climb stairs d...</td>\n",
       "      <td>bariatrics, laparoscopic gastric bypass, weigh...</td>\n",
       "      <td>[bariatrics,  laparoscopic gastric bypass,  we...</td>\n",
       "      <td>([1401, 1412], [1386, 1392], [658, 664], [1406...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                        description  \\\n",
       "0           0   A 23-year-old white female presents with comp...   \n",
       "1           1           Consult for laparoscopic gastric bypass.   \n",
       "\n",
       "       medical_specialty                                sample_name  \\\n",
       "0   Allergy / Immunology                         Allergic Rhinitis    \n",
       "1             Bariatrics   Laparoscopic Gastric Bypass Consult - 2    \n",
       "\n",
       "                                       transcription  \\\n",
       "0  subjective yearold white female present compla...   \n",
       "1  past medical history difficulty climb stairs d...   \n",
       "\n",
       "                                            keywords  \\\n",
       "0  allergy / immunology, allergic rhinitis, aller...   \n",
       "1  bariatrics, laparoscopic gastric bypass, weigh...   \n",
       "\n",
       "                                       keywords_list  \\\n",
       "0  [allergy / immunology,  allergic rhinitis,  al...   \n",
       "1  [bariatrics,  laparoscopic gastric bypass,  we...   \n",
       "\n",
       "                                            location  labels_val  \n",
       "0  ([70, 79], [519, 525], [490, 502], [1036, 1044...           0  \n",
       "1  ([1401, 1412], [1386, 1392], [658, 664], [1406...           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.medical_specialty.unique()\n",
    "dict_change = {' Allergy / Immunology':0, ' Bariatrics':1,\n",
    "       ' Cardiovascular / Pulmonary':2, ' Dentistry':3, ' Urology':4,\n",
    "       ' General Medicine':5, ' Surgery':6, ' Speech - Language':7,\n",
    "       ' SOAP / Chart / Progress Notes':8, ' Sleep Medicine':9,\n",
    "       ' Rheumatology':10, ' Radiology':11, ' Psychiatry / Psychology':12,\n",
    "       ' Podiatry':13, ' Physical Medicine - Rehab':14,\n",
    "       ' Pediatrics - Neonatal':15, ' Pain Management':16, ' Orthopedic':17,\n",
    "       ' Ophthalmology':18, ' Office Notes':19, ' Obstetrics / Gynecology':20,\n",
    "       ' Neurosurgery':21, ' Neurology':22, ' Nephrology':23, ' Letters':24,\n",
    "       ' Lab Medicine - Pathology':25, ' IME-QME-Work Comp etc.':26,\n",
    "       ' Hospice - Palliative Care':27, ' Hematology - Oncology':28,\n",
    "       ' Gastroenterology':29, ' ENT - Otolaryngology':30, ' Endocrinology':31,\n",
    "       ' Emergency Room Reports':32, ' Discharge Summary':33,\n",
    "       ' Diets and Nutritions':34, ' Dermatology':35,\n",
    "       ' Cosmetic / Plastic Surgery':36, ' Consult - History and Phy.':37,\n",
    "       ' Chiropractic':38}\n",
    "\n",
    "#dict_change = {' Allergy / Immunology':0, \n",
    " #              ' Bariatrics':1,' Cardiovascular / Pulmonary': 2, ' Dentistry': 3, ' Urology': 4,\n",
    " #      ' General Medicine': 5, ' Surgery': 6, ' Gastroenterology': 7 }\n",
    "\n",
    "\n",
    "df['labels_val'] = df['medical_specialty'].map(dict_change)\n",
    "#df2=df.replace({\"medical_specialty\": dict_change})\n",
    "\n",
    "df = df.dropna()\n",
    "#df['medical_specialty'].map(dict_change)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised and keybert "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8f0a0c89f09a597c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset csv/default to /Users/tara-sophiatumbraegel/.cache/huggingface/datasets/csv/default-8f0a0c89f09a597c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d1df80ee5184e238e25e7c0b2e76c30",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b2ef47379954a02b03f9f902702a3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting data files:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ebea6251da043d1a3f7440d01474208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "0 tables [00:00, ? tables/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /Users/tara-sophiatumbraegel/.cache/huggingface/datasets/csv/default-8f0a0c89f09a597c/0.0.0/6b34fb8fcf56f7c8ba51dc895bfa2bfbe43546f190a60fcf74bb5e8afdcc2317. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3432de2290b47658988d6f2bc2b4cde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'description', 'medical_specialty', 'sample_name', 'transcription', 'keywords', 'keywords_list', 'location', 'labels_val'],\n",
       "        num_rows: 3435\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'description', 'medical_specialty', 'sample_name', 'transcription', 'keywords', 'keywords_list', 'location', 'labels_val'],\n",
       "        num_rows: 382\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train a model \n",
    "from datasets import Dataset, DatasetDict\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "\n",
    "#df = pd.read_csv('../data/raw/mtsamples.csv')\n",
    "#df['transcription'] = df['transcription'].tolist()\n",
    "df.to_csv('../data/processed/nlp/mtsamples/mtsamples_cleaned_test.csv', index=False)\n",
    "#dataset = load_dataset('../data/raw/mtsamples.csv')\n",
    "dataset = load_dataset('csv', data_files='../data/processed/nlp/mtsamples/mtsamples_cleaned_test.csv')\n",
    "#df.transcription=df.transcription.astype(str)\n",
    "dataset = dataset['train']\n",
    "dataset = dataset.train_test_split(test_size=0.1)\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['Unnamed: 0', 'description', 'medical_specialty', 'sample_name', 'transcription', 'keywords', 'keywords_list', 'location', 'labels_val'],\n",
       "    num_rows: 3435\n",
       "})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data to tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2321ca2324db4c4aa37b9de3edd2f87f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "693b0603dac84084bef28b7cc1b69985",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "checkpoint = 'emilyalsentzer/Bio_ClinicalBERT' #\"bert-base-cased\"\n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "#model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"transcription\"], padding=True, truncation=True, max_length=512)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "tokenized_dataset = tokenized_dataset.remove_columns(['Unnamed: 0', 'description', 'medical_specialty', 'sample_name', 'transcription', 'keywords', 'keywords_list', 'location'])\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"labels_val\", \"labels\")\n",
    "tokenized_dataset.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtaratumbraegel\u001b[0m (\u001b[33mnlp_masterthesis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/wandb/run-20221030_185723-semxjli3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_masterthesis/gpt3/runs/semxjli3\" target=\"_blank\">eerie-spider-1</a></strong> to <a href=\"https://wandb.ai/nlp_masterthesis/gpt3\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at emilyalsentzer/Bio_ClinicalBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "***** Running training *****\n",
      "  Num examples = 10\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 1\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 1\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 10\n",
      "Automatic Weights & Biases logging enabled, to disable set os.environ[\"WANDB_DISABLED\"] = \"true\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f058970bf5a4f25a74d9ffdbe1d7764",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 2\n",
      "  Batch size = 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "036548ca8c0a4eccb0bdc9ee595b5bf3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ../models/mtsamples\n",
      "Configuration saved in ../models/mtsamples/config.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'eval_loss': 4.030138969421387, 'eval_accuracy': 0.0, 'eval_runtime': 6.3244, 'eval_samples_per_second': 0.316, 'eval_steps_per_second': 0.316, 'epoch': 1.0}\n",
      "{'train_runtime': 116.9805, 'train_samples_per_second': 0.085, 'train_steps_per_second': 0.085, 'train_loss': 2.898737144470215, 'epoch': 1.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in ../models/mtsamples/pytorch_model.bin\n",
      "tokenizer config file saved in ../models/mtsamples/tokenizer_config.json\n",
      "Special tokens file saved in ../models/mtsamples/special_tokens_map.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ConnectionError), entering retry loop.\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n",
      "    self._process(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 308, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 305, in send\n",
      "    send_handler(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 317, in send_request\n",
      "    logger.debug(f\"send_request: {request_type}\")\n",
      "Message: 'send_request: stop_status'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n",
      "    self._process(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 263, in _process\n",
      "    self._hm.handle(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 130, in handle\n",
      "    handler(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/handler.py\", line 138, in handle_request\n",
      "    logger.debug(f\"handle_request: {request_type}\")\n",
      "Message: 'handle_request: stop_status'\n",
      "Arguments: ()\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 101, in _run\n",
      "    self._process(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 308, in _process\n",
      "    self._sm.send(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 305, in send\n",
      "    send_handler(record)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 317, in send_request\n",
      "    logger.debug(f\"send_request: {request_type}\")\n",
      "Message: 'send_request: stop_status'\n",
      "Arguments: ()\n",
      "Exception in thread OutRawRd-stderr:\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 946, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1027, in _output_raw_reader_thread\n",
      "    self._output_raw_flush(stream)\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1042, in _output_raw_flush\n",
      "    self._output_raw_file.write(data.encode(\"utf-8\"))\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 64, in write\n",
      "    super().write(b\"\\n\".join(ret) + b\"\\n\")\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/lib/filesystem.py\", line 31, in write\n",
      "    self.f.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "--- Logging error ---\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1104, in emit\n",
      "    self.flush()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/logging/__init__.py\", line 1084, in flush\n",
      "    self.stream.flush()\n",
      "OSError: [Errno 28] No space left on device\n",
      "Call stack:\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 966, in _bootstrap\n",
      "    self._bootstrap_inner()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/threading.py\", line 1009, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 50, in run\n",
      "    self._run()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal_util.py\", line 102, in _run\n",
      "    self._finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/internal.py\", line 311, in _finish\n",
      "    self._sm.finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/sdk/internal/sender.py\", line 1336, in finish\n",
      "    self._dir_watcher.finish()\n",
      "  File \"/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/wandb/filesync/dir_watcher.py\", line 406, in finish\n",
      "    logger.info(\"scan save: %s %s\", file_path, save_name)\n",
      "Message: 'scan save: %s %s'\n",
      "Arguments: ('/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/wandb/run-20221030_185723-semxjli3/files/conda-environment.yaml', 'conda-environment.yaml')\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "#metric = load_metric(\"f1\",  average=\"macro\")\n",
    "import wandb\n",
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "# 1. Start a W&B run\n",
    "wandb.init(project='gpt3')\n",
    "\n",
    "# 2. Save model inputs and hyperparameters\n",
    "#config = wandb.config\n",
    "#config.learning_rate = config.learning_rate\n",
    "\n",
    "\n",
    "# Model training code here\n",
    "# 3. Log metrics over time to visualize performance\n",
    "#export WANDB_PROJECT=test01\n",
    "#export TASK_NAME=sentence_prediction\n",
    "\n",
    "\n",
    "\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\", average=\"macro\")\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "#smaller dataset for testing purpose\n",
    "small_train_dataset = tokenized_dataset[\"train\"].shuffle(seed=42).select(range(10))\n",
    "small_eval_dataset = tokenized_dataset[\"test\"].shuffle(seed=42).select(range(2))\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=39)\n",
    "from transformers import Trainer, TrainingArguments\n",
    "training_arguments = TrainingArguments('test_trainer', evaluation_strategy=\"epoch\",\n",
    "                                       per_device_train_batch_size=1, \n",
    "                                       per_device_eval_batch_size=1, num_train_epochs=1,\n",
    "                                       report_to = 'wandb'\n",
    "                                       )\n",
    "\n",
    "\n",
    "trainer = Trainer(model=model, args=training_arguments, \n",
    "                  train_dataset=small_train_dataset,\n",
    "                 eval_dataset = small_eval_dataset,\n",
    "                  #data_collator=data_collator,\n",
    "                  tokenizer=tokenizer, compute_metrics=compute_metrics)\n",
    "trainer.train()\n",
    "trainer.save_model('../models/mtsamples')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Call KeyBert with the model name and the number of keywords to extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting keyphrase_vectorizers\n",
      "  Downloading keyphrase_vectorizers-0.0.10-py3-none-any.whl (28 kB)\n",
      "Requirement already satisfied: scipy>=1.7.3 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from keyphrase_vectorizers) (1.7.3)\n",
      "Requirement already satisfied: spacy>=3.0.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from keyphrase_vectorizers) (3.4.1)\n",
      "Requirement already satisfied: nltk>=3.6.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from keyphrase_vectorizers) (3.7)\n",
      "Collecting spacy-transformers>=1.1.6\n",
      "  Downloading spacy_transformers-1.1.8-py2.py3-none-any.whl (53 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.4/53.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.18.5 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from keyphrase_vectorizers) (1.22.3)\n",
      "Requirement already satisfied: scikit-learn>=1.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from keyphrase_vectorizers) (1.1.1)\n",
      "Requirement already satisfied: psutil>=5.8.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from keyphrase_vectorizers) (5.9.0)\n",
      "Requirement already satisfied: click in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from nltk>=3.6.1->keyphrase_vectorizers) (8.0.4)\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from nltk>=3.6.1->keyphrase_vectorizers) (4.64.1)\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from nltk>=3.6.1->keyphrase_vectorizers) (1.1.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from nltk>=3.6.1->keyphrase_vectorizers) (2022.9.13)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from scikit-learn>=1.0->keyphrase_vectorizers) (2.2.0)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (3.0.3)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (2.0.6)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (3.3.0)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (3.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (1.0.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (2.28.1)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (0.4.2)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (2.0.8)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (0.10.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (21.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (1.0.8)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (1.9.2)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.9 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (3.0.10)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (0.6.2)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (8.1.2)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (63.4.1)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy>=3.0.1->keyphrase_vectorizers) (2.4.4)\n",
      "Requirement already satisfied: torch>=1.6.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from spacy-transformers>=1.1.6->keyphrase_vectorizers) (1.12.1)\n",
      "Collecting transformers<4.22.0,>=3.4.0\n",
      "  Downloading transformers-4.21.3-py3-none-any.whl (4.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting spacy-alignments<1.0.0,>=0.7.2\n",
      "  Downloading spacy_alignments-0.8.6-cp310-cp310-macosx_10_9_x86_64.whl (319 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.8/319.8 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from packaging>=20.0->spacy>=3.0.1->keyphrase_vectorizers) (3.0.9)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from pathy>=0.3.5->spacy>=3.0.1->keyphrase_vectorizers) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from pydantic!=1.8,!=1.8.1,<1.10.0,>=1.7.4->spacy>=3.0.1->keyphrase_vectorizers) (4.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase_vectorizers) (2022.9.24)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase_vectorizers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase_vectorizers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from requests<3.0.0,>=2.13.0->spacy>=3.0.1->keyphrase_vectorizers) (1.26.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.1->keyphrase_vectorizers) (0.0.2)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from thinc<8.2.0,>=8.1.0->spacy>=3.0.1->keyphrase_vectorizers) (0.7.8)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase_vectorizers) (3.6.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/PyYAML-6.0-py3.10-macosx-10.9-x86_64.egg (from transformers<4.22.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase_vectorizers) (6.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase_vectorizers) (0.10.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from transformers<4.22.0,>=3.4.0->spacy-transformers>=1.1.6->keyphrase_vectorizers) (0.12.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages (from jinja2->spacy>=3.0.1->keyphrase_vectorizers) (2.1.1)\n",
      "Installing collected packages: spacy-alignments, transformers, spacy-transformers, keyphrase_vectorizers\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.22.2\n",
      "    Uninstalling transformers-4.22.2:\n",
      "      Successfully uninstalled transformers-4.22.2\n",
      "Successfully installed keyphrase_vectorizers-0.0.10 spacy-alignments-0.8.6 spacy-transformers-1.1.8 transformers-4.21.3\n"
     ]
    }
   ],
   "source": [
    "!pip install keyphrase_vectorizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ../models/mtsamples\n",
      "Configuration saved in ../models/mtsamples/config.json\n",
      "Model weights saved in ../models/mtsamples/pytorch_model.bin\n",
      "tokenizer config file saved in ../models/mtsamples/tokenizer_config.json\n",
      "Special tokens file saved in ../models/mtsamples/special_tokens_map.json\n",
      "loading configuration file ../models/mtsamples/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"emilyalsentzer/Bio_ClinicalBERT\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ../models/mtsamples/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../models/mtsamples were not used when initializing BertForMaskedLM: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForMaskedLM were not initialized from the model checkpoint at ../models/mtsamples and are newly initialized: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Didn't find file ../models/mtsamples/added_tokens.json. We won't load it.\n",
      "loading file ../models/mtsamples/vocab.txt\n",
      "loading file None\n",
      "loading file ../models/mtsamples/special_tokens_map.json\n",
      "loading file ../models/mtsamples/tokenizer_config.json\n",
      "Disabling tokenizer parallelism, we're using DataLoader multithreading already\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('sick neck', 0.8674),\n",
       " ('sentence like', 0.8549),\n",
       " ('embedding sick', 0.7738),\n",
       " ('head', 0.6698),\n",
       " ('sentence', 0.6611)]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "# https://towardsdatascience.com/enhancing-keybert-keyword-extraction-results-with-keyphrasevectorizers-3796fa93f4db\n",
    "from transformers import BertForMaskedLM, BertTokenizer\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer #- instead of n gram\n",
    "\n",
    "from transformers.pipelines import pipeline\n",
    "model = BertForMaskedLM.from_pretrained('../models/mtsamples') #/model\n",
    "tokenizer = BertTokenizer.from_pretrained('../models/mtsamples')\n",
    "hf_model = pipeline('feature-extraction', model=model, tokenizer=tokenizer)\n",
    "example_text = \"I am a sentence for which I'd like to get its embedding. i am sick my neck hurts my head hurts\"\n",
    "kw_model = KeyBERT(model=hf_model)\n",
    "keywords = kw_model.extract_keywords(example_text, vectorizer=KeyphraseCountVectorizer(), # keyphrase_ngram_range=(1, 2), \n",
    "                                     stop_words='english', \n",
    "                                     use_maxsum=True, \n",
    "                                     nr_candidates=10, \n",
    "                                     top_n=5, \n",
    "                                     use_mmr=True, \n",
    "                                     diversity=0.7)\n",
    "\n",
    "keywords\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name ../models/mtsamples. Creating a new one with MEAN pooling.\n",
      "loading configuration file ../models/mtsamples/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"../models/mtsamples\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\",\n",
      "    \"19\": \"LABEL_19\",\n",
      "    \"20\": \"LABEL_20\",\n",
      "    \"21\": \"LABEL_21\",\n",
      "    \"22\": \"LABEL_22\",\n",
      "    \"23\": \"LABEL_23\",\n",
      "    \"24\": \"LABEL_24\",\n",
      "    \"25\": \"LABEL_25\",\n",
      "    \"26\": \"LABEL_26\",\n",
      "    \"27\": \"LABEL_27\",\n",
      "    \"28\": \"LABEL_28\",\n",
      "    \"29\": \"LABEL_29\",\n",
      "    \"30\": \"LABEL_30\",\n",
      "    \"31\": \"LABEL_31\",\n",
      "    \"32\": \"LABEL_32\",\n",
      "    \"33\": \"LABEL_33\",\n",
      "    \"34\": \"LABEL_34\",\n",
      "    \"35\": \"LABEL_35\",\n",
      "    \"36\": \"LABEL_36\",\n",
      "    \"37\": \"LABEL_37\",\n",
      "    \"38\": \"LABEL_38\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_19\": 19,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_20\": 20,\n",
      "    \"LABEL_21\": 21,\n",
      "    \"LABEL_22\": 22,\n",
      "    \"LABEL_23\": 23,\n",
      "    \"LABEL_24\": 24,\n",
      "    \"LABEL_25\": 25,\n",
      "    \"LABEL_26\": 26,\n",
      "    \"LABEL_27\": 27,\n",
      "    \"LABEL_28\": 28,\n",
      "    \"LABEL_29\": 29,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_30\": 30,\n",
      "    \"LABEL_31\": 31,\n",
      "    \"LABEL_32\": 32,\n",
      "    \"LABEL_33\": 33,\n",
      "    \"LABEL_34\": 34,\n",
      "    \"LABEL_35\": 35,\n",
      "    \"LABEL_36\": 36,\n",
      "    \"LABEL_37\": 37,\n",
      "    \"LABEL_38\": 38,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.18.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 28996\n",
      "}\n",
      "\n",
      "loading weights file ../models/mtsamples/pytorch_model.bin\n",
      "Some weights of the model checkpoint at ../models/mtsamples were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "All the weights of BertModel were initialized from the model checkpoint at ../models/mtsamples.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertModel for predictions without further training.\n",
      "Didn't find file ../models/mtsamples/added_tokens.json. We won't load it.\n",
      "loading file ../models/mtsamples/vocab.txt\n",
      "loading file ../models/mtsamples/tokenizer.json\n",
      "loading file None\n",
      "loading file ../models/mtsamples/special_tokens_map.json\n",
      "loading file ../models/mtsamples/tokenizer_config.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('headache fever', 0.8847), ('headache', 0.7987), ('fever', 0.7897)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "wandb: Network error (ConnectionError), entering retry loop.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from keybert import KeyBERT\n",
    "model = KeyBERT(model='../models/mtsamples')\n",
    "kw = model.extract_keywords('I have a headache and a fever', keyphrase_ngram_range=(1, 3), stop_words='english', \n",
    "                       use_maxsum=True, nr_candidates=20, top_n=5, use_mmr=True)\n",
    "\n",
    "kw  # [('fever', 0.0), ('headache', 0.0)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## apply Bert to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb Cell 27\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mkeywords_outcome\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39;49mapply(\u001b[39mlambda\u001b[39;49;00m x: model\u001b[39m.\u001b[39;49mextract_keywords(x[\u001b[39m'\u001b[39;49m\u001b[39mtranscription\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                                                    keyphrase_ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                                    stop_words\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m,  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                                    use_maxsum\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                                    nr_candidates\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                                                    top_n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                                    use_mmr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/frame.py:8848\u001b[0m, in \u001b[0;36mDataFrame.apply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8837\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpandas\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mapply\u001b[39;00m \u001b[39mimport\u001b[39;00m frame_apply\n\u001b[1;32m   8839\u001b[0m op \u001b[39m=\u001b[39m frame_apply(\n\u001b[1;32m   8840\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   8841\u001b[0m     func\u001b[39m=\u001b[39mfunc,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   8846\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[1;32m   8847\u001b[0m )\n\u001b[0;32m-> 8848\u001b[0m \u001b[39mreturn\u001b[39;00m op\u001b[39m.\u001b[39;49mapply()\u001b[39m.\u001b[39m__finalize__(\u001b[39mself\u001b[39m, method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mapply\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:733\u001b[0m, in \u001b[0;36mFrameApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    730\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mraw:\n\u001b[1;32m    731\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_raw()\n\u001b[0;32m--> 733\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:857\u001b[0m, in \u001b[0;36mFrameApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_standard\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 857\u001b[0m     results, res_index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_series_generator()\n\u001b[1;32m    859\u001b[0m     \u001b[39m# wrap results\u001b[39;00m\n\u001b[1;32m    860\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwrap_results(results, res_index)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:873\u001b[0m, in \u001b[0;36mFrameApply.apply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    870\u001b[0m \u001b[39mwith\u001b[39;00m option_context(\u001b[39m\"\u001b[39m\u001b[39mmode.chained_assignment\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[1;32m    871\u001b[0m     \u001b[39mfor\u001b[39;00m i, v \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(series_gen):\n\u001b[1;32m    872\u001b[0m         \u001b[39m# ignore SettingWithCopy here in case the user mutates\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         results[i] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mf(v)\n\u001b[1;32m    874\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(results[i], ABCSeries):\n\u001b[1;32m    875\u001b[0m             \u001b[39m# If we have a view on v, we need to make a copy because\u001b[39;00m\n\u001b[1;32m    876\u001b[0m             \u001b[39m#  series_generator will swap out the underlying data\u001b[39;00m\n\u001b[1;32m    877\u001b[0m             results[i] \u001b[39m=\u001b[39m results[i]\u001b[39m.\u001b[39mcopy(deep\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb Cell 27\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mkeywords_outcome\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df\u001b[39m.\u001b[39mapply(\u001b[39mlambda\u001b[39;00m x: model\u001b[39m.\u001b[39;49mextract_keywords(x[\u001b[39m'\u001b[39;49m\u001b[39mtranscription\u001b[39;49m\u001b[39m'\u001b[39;49m], \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m                                                                    keyphrase_ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m), \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                                                                    stop_words\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m'\u001b[39;49m,  \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                                                                    use_maxsum\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                                                                    nr_candidates\u001b[39m=\u001b[39;49m\u001b[39m25\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                                                                    top_n\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m                                                                    use_mmr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/v03keywordbert.ipynb#Y104sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m df\u001b[39m.\u001b[39mhead()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/keybert/_model.py:155\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[0;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m# Extract embeddings\u001b[39;00m\n\u001b[1;32m    154\u001b[0m doc_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membed(docs)\n\u001b[0;32m--> 155\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49membed(words)\n\u001b[1;32m    157\u001b[0m \u001b[39m# Find keywords\u001b[39;00m\n\u001b[1;32m    158\u001b[0m all_keywords \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/keybert/backend/_sentencetransformers.py:62\u001b[0m, in \u001b[0;36mSentenceTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39membed\u001b[39m(\u001b[39mself\u001b[39m, documents: List[\u001b[39mstr\u001b[39m], verbose: \u001b[39mbool\u001b[39m \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m     51\u001b[0m     \u001b[39m\"\"\"Embed a list of n documents/words into an n-dimensional\u001b[39;00m\n\u001b[1;32m     52\u001b[0m \u001b[39m    matrix of embeddings\u001b[39;00m\n\u001b[1;32m     53\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[39m        that each have an embeddings size of `m`\u001b[39;00m\n\u001b[1;32m     61\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 62\u001b[0m     embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49membedding_model\u001b[39m.\u001b[39;49mencode(documents, show_progress_bar\u001b[39m=\u001b[39;49mverbose)\n\u001b[1;32m     63\u001b[0m     \u001b[39mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:165\u001b[0m, in \u001b[0;36mSentenceTransformer.encode\u001b[0;34m(self, sentences, batch_size, show_progress_bar, output_value, convert_to_numpy, convert_to_tensor, device, normalize_embeddings)\u001b[0m\n\u001b[1;32m    162\u001b[0m features \u001b[39m=\u001b[39m batch_to_device(features, device)\n\u001b[1;32m    164\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39mno_grad():\n\u001b[0;32m--> 165\u001b[0m     out_features \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(features)\n\u001b[1;32m    167\u001b[0m     \u001b[39mif\u001b[39;00m output_value \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m    168\u001b[0m         embeddings \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[1;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[0;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[1;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py:66\u001b[0m, in \u001b[0;36mTransformer.forward\u001b[0;34m(self, features)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m \u001b[39min\u001b[39;00m features:\n\u001b[1;32m     64\u001b[0m     trans_features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m features[\u001b[39m'\u001b[39m\u001b[39mtoken_type_ids\u001b[39m\u001b[39m'\u001b[39m]\n\u001b[0;32m---> 66\u001b[0m output_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mauto_model(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mtrans_features, return_dict\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m     67\u001b[0m output_tokens \u001b[39m=\u001b[39m output_states[\u001b[39m0\u001b[39m]\n\u001b[1;32m     69\u001b[0m features\u001b[39m.\u001b[39mupdate({\u001b[39m'\u001b[39m\u001b[39mtoken_embeddings\u001b[39m\u001b[39m'\u001b[39m: output_tokens, \u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m: features[\u001b[39m'\u001b[39m\u001b[39mattention_mask\u001b[39m\u001b[39m'\u001b[39m]})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:996\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    987\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    989\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m    990\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m    991\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    994\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    995\u001b[0m )\n\u001b[0;32m--> 996\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m    997\u001b[0m     embedding_output,\n\u001b[1;32m    998\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m    999\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1000\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1001\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1002\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1003\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1004\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1005\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1006\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1007\u001b[0m )\n\u001b[1;32m   1008\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1009\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:585\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    576\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    577\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    578\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    582\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    583\u001b[0m     )\n\u001b[1;32m    584\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 585\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    586\u001b[0m         hidden_states,\n\u001b[1;32m    587\u001b[0m         attention_mask,\n\u001b[1;32m    588\u001b[0m         layer_head_mask,\n\u001b[1;32m    589\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    590\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    591\u001b[0m         past_key_value,\n\u001b[1;32m    592\u001b[0m         output_attentions,\n\u001b[1;32m    593\u001b[0m     )\n\u001b[1;32m    595\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    596\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:472\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    461\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    462\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    470\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    471\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 472\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    473\u001b[0m         hidden_states,\n\u001b[1;32m    474\u001b[0m         attention_mask,\n\u001b[1;32m    475\u001b[0m         head_mask,\n\u001b[1;32m    476\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    477\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    478\u001b[0m     )\n\u001b[1;32m    479\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    481\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:402\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    393\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    394\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    401\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 402\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    403\u001b[0m         hidden_states,\n\u001b[1;32m    404\u001b[0m         attention_mask,\n\u001b[1;32m    405\u001b[0m         head_mask,\n\u001b[1;32m    406\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    407\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    408\u001b[0m         past_key_value,\n\u001b[1;32m    409\u001b[0m         output_attentions,\n\u001b[1;32m    410\u001b[0m     )\n\u001b[1;32m    411\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    412\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:290\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    288\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    289\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 290\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    291\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    293\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from keybert import KeyBERT\n",
    "model = KeyBERT(model='../models/mtsamples')\n",
    "df['keywords_outcome'] = df.apply(lambda x: model.extract_keywords(x['transcription'], \n",
    "                                                                   keyphrase_ngram_range=(1, 2), \n",
    "                                                                   stop_words='english',  \n",
    "                                                                   use_maxsum=True, \n",
    "                                                                   nr_candidates=25, \n",
    "                                                                   top_n=10, \n",
    "                                                                   use_mmr=True), axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('../data/processed/mtsamples_processedBert.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
