{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from array import array\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "import imblearn\n",
    "import sklearn\n",
    "import scipy\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, cross_validate\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.pipeline import Pipeline as imbPipeline\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "from traitlets import List\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from lime.lime_text import LimeTextExplainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "df = pd.read_csv('../data/processed/mtsamples_nlp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, testing_data = train_test_split(df, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_pipeline() -> imblearn.pipeline.Pipeline:\n",
    "    \"\"\"\n",
    "    Build pipeline for model\n",
    "    \"\"\"\n",
    "    model_pipeline = imbPipeline(\n",
    "        [\n",
    "            (\"preprocessing\", CountVectorizer()),\n",
    "            (\"smote\", SMOTE(random_state=42)),\n",
    "            (\n",
    "                \"classifier\",\n",
    "                LogisticRegression(\n",
    "                    random_state=42,\n",
    "                ),\n",
    "            ),  # remainder=\"passthrough\"\n",
    "        ]\n",
    "    )\n",
    "    return model_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_pipeline = build_pipeline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hannahpetry/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:444: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "model = model_pipeline.fit(training_data.transcription_f, training_data.medical_specialty)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_probability(model: imblearn.pipeline.Pipeline, value: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    get probabilities for sample\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : imblearn.pipeline.Pipeline\n",
    "        best model from train.py\n",
    "    value : str\n",
    "        sample\n",
    "    category_list: list[str]\n",
    "        list of unique labels\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Probabilities for labels\n",
    "    \"\"\"\n",
    "\n",
    "    prob_array = model.predict_proba(value)\n",
    "    prob_df = (\n",
    "        pd.DataFrame(prob_array, index=[\"Probability\"], columns=model.classes_)\n",
    "        .transpose()\n",
    "        .sort_values(by=\"Probability\", ascending=False)\n",
    "    )\n",
    "    return prob_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                Probability\n",
      " Cardiovascular / Pulmonary        0.725316\n",
      " Radiology                         0.153696\n",
      " General Medicine                  0.088011\n",
      " Obstetrics / Gynecology           0.016509\n",
      " Consult - History and Phy.        0.011008\n",
      " Surgery                           0.004094\n",
      " Neurology                         0.000660\n",
      " Gastroenterology                  0.000326\n",
      " SOAP / Chart / Progress Notes     0.000313\n",
      " Urology                           0.000049\n",
      " Orthopedic                        0.000018\n"
     ]
    }
   ],
   "source": [
    "# Predict probability\n",
    "to_pred = \"coronary nitroglycerin muscle heart breast oxygen valve artery\"\n",
    "res_df = predict_probability(model, [to_pred])\n",
    "print(res_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_explainer(model: imblearn.pipeline.Pipeline, value: str):\n",
    "    \"\"\"\n",
    "    Get features the model used for top predicted classes\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    model : imblearn.pipeline.Pipeline\n",
    "        best model from train.py\n",
    "    value : str\n",
    "        sample\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict\n",
    "        Features from sample the model used to predict classes\n",
    "    \"\"\"\n",
    "    explainer = LimeTextExplainer(class_names=model.classes_)\n",
    "    num_features = len(value.split())\n",
    "    exp = explainer.explain_instance(\n",
    "        value, model.predict_proba, num_features=num_features, top_labels=3\n",
    "    )\n",
    "    feat_importance = exp.as_map()\n",
    "    feat_importance = {model.classes_[k]: v for k, v in feat_importance.items()}\n",
    "    feat_importance = {\n",
    "        k: [(value.split()[i], v) for i, v in v] for k, v in feat_importance.items()\n",
    "    }\n",
    "    feat_importance_pos = {\n",
    "        k: [v for v in v if v[1] > 0] for k, v in feat_importance.items()\n",
    "    }\n",
    "    return feat_importance_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' General Medicine': [('breast', 0.08538663930853976), ('muscle', 0.08129614124637603), ('heart', 0.03257568466818079), ('oxygen', 0.027563602319390052)], ' Radiology': [('artery', 0.14826870849067347), ('nitroglycerin', 0.07722799931520544), ('muscle', 0.04454801614169169)], ' Cardiovascular / Pulmonary': [('oxygen', 0.2561777240885622), ('artery', 0.23414528182539765), ('heart', 0.21199254218646277), ('coronary', 0.20022344949631096), ('valve', 0.13659521118496687)]}\n"
     ]
    }
   ],
   "source": [
    "# features the model used for top predicted classes\n",
    "feat_importance = lime_explainer(model, to_pred)\n",
    "print(feat_importance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ' General Medicine'\n",
    "\n",
    "# if key is in dict return value (first value in list)\n",
    "def get_value(x):\n",
    "    if x in feat_importance:\n",
    "        return feat_importance[x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "value = get_value(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breast', 'muscle', 'heart', 'oxygen']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# only access words in list\n",
    "words = [v[0] for v in value]\n",
    "words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine get_value and words into one function\n",
    "def get_words(x):\n",
    "    if x in feat_importance:\n",
    "        value = feat_importance[x]\n",
    "        words = [v[0] for v in value]\n",
    "        return words\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['breast', 'muscle', 'heart', 'oxygen']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_words(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_masterthesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6b1167c26af54c6340f890887cef801dce27ac37ff6fc6a99bb1eb896d088a43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
