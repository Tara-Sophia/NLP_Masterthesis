{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm, trange\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer, AutoConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "\n",
    "    MAX_LEN = 416\n",
    "    TRAIN_BATCH_SIZE = 8\n",
    "    VALID_BATCH_SIZE = 8\n",
    "    EPOCHS = 1\n",
    "    #tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    #model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    BERT_PATH = \"bert-base-uncased\" \n",
    "    MODEL_PATH = \"model.bin\"\n",
    "    \n",
    "    TOKENIZER = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "    DROPOUT = 0.2\n",
    "    MAX_GRAD_NORM = 1.0\n",
    "    LEARNING_RATE = 1e-5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class config:\n",
    "    seed = 3407,\n",
    "    num_jobs=1,\n",
    "    num_labels=2,\n",
    "    \n",
    "    # model info\n",
    "    #tokenizer_path = \"bert-base-uncased\" \n",
    "    #model_checkpoint = \"../models/model_fold1.bin\"\n",
    "    \n",
    "    tokenizer_path = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    model_checkpoint = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "    #tokenizer_path = 'allenai/biomed_roberta_base', # 'roberta-base', \n",
    "    #model_checkpoint = '../input/biomed-roberta', # 'roberta-base', \n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu',\n",
    "    \n",
    "    # training paramters\n",
    "    max_length = 514,\n",
    "    batch_size=1,\n",
    "    \n",
    "    # for this notebook\n",
    "    debug = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sample_test():\n",
    "    feats = pd.read_csv(f\"../data/features.csv\")\n",
    "    feats.loc[27, 'feature_text'] = \"Last-Pap-smear-1-year-ago\"\n",
    "    \n",
    "    notes = pd.read_csv(f\"../data/patient_notes.csv\")\n",
    "    test = pd.read_csv(f\"../data/test.csv\")\n",
    "\n",
    "    merged = test.merge(notes, how = \"left\")\n",
    "    merged = merged.merge(feats, how = \"left\")\n",
    "\n",
    "    def process_feature_text(text):\n",
    "        return text.replace(\"-OR-\", \";-\").replace(\"-\", \" \")\n",
    "    merged[\"feature_text\"] = [process_feature_text(x) for x in merged[\"feature_text\"]]\n",
    "    \n",
    "    return merged.sample(1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMETestData(torch.utils.data.Dataset):\n",
    "    def __init__(self, feature_text, pn_history, tokenizer):\n",
    "        self.feature_text = feature_text\n",
    "        self.pn_history = pn_history\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.feature_text)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        tokenized = self.tokenizer(\n",
    "            self.feature_text[idx],\n",
    "            self.pn_history[idx],\n",
    "            truncation = \"only_second\",\n",
    "            max_length = config['max_length'],\n",
    "            padding = \"max_length\",\n",
    "            return_offsets_mapping = True\n",
    "        )\n",
    "        tokenized[\"sequence_ids\"] = tokenized.sequence_ids()\n",
    "\n",
    "        input_ids = np.array(tokenized[\"input_ids\"])\n",
    "        attention_mask = np.array(tokenized[\"attention_mask\"])\n",
    "        offset_mapping = np.array(tokenized[\"offset_mapping\"])\n",
    "        sequence_ids = np.array(tokenized[\"sequence_ids\"]).astype(\"float16\")\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids, \n",
    "            'attention_mask': attention_mask, \n",
    "            'offset_mapping': offset_mapping, \n",
    "            'sequence_ids': sequence_ids,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NBMEModel(nn.Module):\n",
    "    def __init__(self, num_labels=1, path=None):\n",
    "        super().__init__()\n",
    "        \n",
    "        layer_norm_eps: float = 1e-6\n",
    "        \n",
    "        self.path = path\n",
    "        self.num_labels = num_labels\n",
    "        self.config = transformers.AutoConfig.from_pretrained(config.model_checkpoint)\n",
    "\n",
    "        self.config.update(\n",
    "            {\n",
    "                \"layer_norm_eps\": layer_norm_eps,\n",
    "            }\n",
    "        )\n",
    "        self.transformer = transformers.AutoModel.from_pretrained(config.model_checkpoint, config=self.config)\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.output = nn.Linear(self.config.hidden_size, 1)\n",
    "        \n",
    "        if self.path is not None:\n",
    "            self.load_state_dict(torch.load(self.path)['model'])\n",
    "\n",
    "    def forward(self, data):\n",
    "        \n",
    "        ids = data['input_ids']\n",
    "        mask = data['attention_mask']\n",
    "        try:\n",
    "            target = data['targets']\n",
    "        except:\n",
    "            target = None\n",
    "\n",
    "        transformer_out = self.transformer(ids, mask)\n",
    "        sequence_output = transformer_out[0]\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.output(sequence_output)\n",
    "\n",
    "        ret = {\n",
    "            \"logits\": torch.sigmoid(logits),\n",
    "        }\n",
    "        \n",
    "        if target is not None:\n",
    "            loss = self.get_loss(logits, target)\n",
    "            ret['loss'] = loss\n",
    "            ret['targets'] = target\n",
    "\n",
    "        return ret\n",
    "    \n",
    "    def get_optimizer(self, learning_rate, weigth_decay):\n",
    "        optimizer = torch.optim.AdamW(\n",
    "            self.parameters(), \n",
    "            lr=learning_rate, \n",
    "            weight_decay=weigth_decay,\n",
    "        )\n",
    "        if self.path is not None:\n",
    "            optimizer.load_state_dict(torch.load(self.path)['optimizer'])\n",
    "        \n",
    "        return optimizer\n",
    "            \n",
    "    def get_scheduler(self, optimizer, num_warmup_steps, num_training_steps):\n",
    "        scheduler = transformers.get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=num_warmup_steps,\n",
    "            num_training_steps=num_training_steps,\n",
    "        )\n",
    "        if self.path is not None:\n",
    "            scheduler.load_state_dict(torch.load(self.path)['scheduler'])\n",
    "            \n",
    "        return scheduler\n",
    "    \n",
    "    def get_loss(self, output, target):\n",
    "        loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")\n",
    "        loss = loss_fn(output.view(-1, 1), target.view(-1, 1))\n",
    "        loss = torch.masked_select(loss, target.view(-1, 1) != -100).mean()\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prediction locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at emilyalsentzer/Bio_ClinicalBERT were not used when initializing BertModel: ['cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for NBMEModel:\n\tsize mismatch for transformer.embeddings.position_ids: copying a param with shape torch.Size([1, 514]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for transformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([28996, 768]).\n\tsize mismatch for transformer.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for transformer.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([2, 768]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/02_testBert.ipynb Cell 13\u001b[0m in \u001b[0;36m<cell line: 103>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/02_testBert.ipynb#W0sZmlsZQ%3D%3D?line=100'>101</a>\u001b[0m model \u001b[39m=\u001b[39m NBMEModel()\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/02_testBert.ipynb#W0sZmlsZQ%3D%3D?line=101'>102</a>\u001b[0m \u001b[39m#model.to(device)#(torch.device)#['device'])\u001b[39;00m\n\u001b[0;32m--> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/02_testBert.ipynb#W0sZmlsZQ%3D%3D?line=102'>103</a>\u001b[0m model\u001b[39m.\u001b[39;49mload_state_dict(torch\u001b[39m.\u001b[39;49mload(path, map_location\u001b[39m=\u001b[39;49mtorch\u001b[39m.\u001b[39;49mdevice(\u001b[39m'\u001b[39;49m\u001b[39mcpu\u001b[39;49m\u001b[39m'\u001b[39;49m)))\u001b[39m#torch.device(config.device)))#['model'])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/02_testBert.ipynb#W0sZmlsZQ%3D%3D?line=103'>104</a>\u001b[0m model\u001b[39m.\u001b[39mto(device)\u001b[39m#(torch.device)#['device'])\u001b[39;00m\n\u001b[1;32m    <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/notebooks/02_testBert.ipynb#W0sZmlsZQ%3D%3D?line=104'>105</a>\u001b[0m model\u001b[39m.\u001b[39meval()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1604\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict)\u001b[0m\n\u001b[1;32m   1599\u001b[0m         error_msgs\u001b[39m.\u001b[39minsert(\n\u001b[1;32m   1600\u001b[0m             \u001b[39m0\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mMissing key(s) in state_dict: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m. \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1601\u001b[0m                 \u001b[39m'\u001b[39m\u001b[39m, \u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mjoin(\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(k) \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m missing_keys)))\n\u001b[1;32m   1603\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(error_msgs) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m-> 1604\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\u001b[39m'\u001b[39m\u001b[39mError(s) in loading state_dict for \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[1;32m   1605\u001b[0m                        \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39m\\t\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   1606\u001b[0m \u001b[39mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for NBMEModel:\n\tsize mismatch for transformer.embeddings.position_ids: copying a param with shape torch.Size([1, 514]) from checkpoint, the shape in current model is torch.Size([1, 512]).\n\tsize mismatch for transformer.embeddings.word_embeddings.weight: copying a param with shape torch.Size([50265, 768]) from checkpoint, the shape in current model is torch.Size([28996, 768]).\n\tsize mismatch for transformer.embeddings.position_embeddings.weight: copying a param with shape torch.Size([514, 768]) from checkpoint, the shape in current model is torch.Size([512, 768]).\n\tsize mismatch for transformer.embeddings.token_type_embeddings.weight: copying a param with shape torch.Size([1, 768]) from checkpoint, the shape in current model is torch.Size([2, 768])."
     ]
    }
   ],
   "source": [
    "def get_location_predictions(preds, offset_mapping, sequence_ids, test=False):\n",
    "    all_predictions = []\n",
    "    for pred, offsets, seq_ids in zip(preds, offset_mapping, sequence_ids):\n",
    "        start_idx = None\n",
    "        current_preds = []\n",
    "        for p, o, s_id in zip(pred, offsets, seq_ids):\n",
    "            if s_id is None or s_id == 0:\n",
    "                continue\n",
    "            if p > 0.5:\n",
    "                if start_idx is None:\n",
    "                    start_idx = o[0]\n",
    "                end_idx = o[1]\n",
    "            elif start_idx is not None:\n",
    "                if test:\n",
    "                    current_preds.append(f\"{start_idx} {end_idx}\")\n",
    "                else:\n",
    "                    current_preds.append((start_idx, end_idx))\n",
    "                start_idx = None\n",
    "        if test:\n",
    "            all_predictions.append(\"; \".join(current_preds))\n",
    "        else:\n",
    "            all_predictions.append(current_preds)\n",
    "    return all_predictions\n",
    "\n",
    "\n",
    "\n",
    "def predict_location_preds(tokenizer, model, feature_text, pn_history):\n",
    "\n",
    "    test_ds = NBMETestData(feature_text, pn_history, tokenizer)\n",
    "    test_dl = torch.utils.data.DataLoader(\n",
    "        test_ds, \n",
    "        batch_size=config.batch_size, \n",
    "        pin_memory=True, \n",
    "        shuffle=False, \n",
    "        drop_last=False\n",
    "    )\n",
    "\n",
    "    all_preds = None\n",
    "    offsets = []\n",
    "    seq_ids = []\n",
    "\n",
    "    preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(test_dl):\n",
    "\n",
    "            for k, v in batch.items():\n",
    "                if k not in  ['offset_mapping', 'sequence_id']:\n",
    "                    batch[k] = v.to(config.device)\n",
    "\n",
    "            logits = model(batch)['logits']\n",
    "            preds.append(logits.cpu().numpy())\n",
    "\n",
    "            offset_mapping = batch['offset_mapping']\n",
    "            sequence_ids = batch['sequence_ids']\n",
    "            offsets.append(offset_mapping.cpu().numpy())\n",
    "            seq_ids.append(sequence_ids.cpu().numpy())\n",
    "\n",
    "    preds = np.concatenate(preds, axis=0)\n",
    "    if all_preds is None:\n",
    "        all_preds = np.array(preds).astype(np.float32)\n",
    "    else:\n",
    "        all_preds += np.array(preds).astype(np.float32)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    all_preds = all_preds.squeeze()\n",
    "\n",
    "    offsets = np.concatenate(offsets, axis=0)\n",
    "    seq_ids = np.concatenate(seq_ids, axis=0)\n",
    "\n",
    "    print(all_preds.shape, offsets.shape, seq_ids.shape)\n",
    "\n",
    "    location_preds = get_location_predictions([all_preds], offsets, seq_ids, test=False)[0]\n",
    "    \n",
    "    x = []\n",
    "    \n",
    "    for location in location_preds:\n",
    "        x.append(pn_history[0][location[0]: location[1]])\n",
    "    \n",
    "    return location_preds, ', '.join(x)\n",
    "\n",
    "def get_predictions(feature_text, pn_history):\n",
    "    location_preds, pred_string = predict_location_preds(tokenizer, model, [feature_text], [pn_history])\n",
    "    print('pred string', pred_string)\n",
    "    return pred_string\n",
    "\n",
    "# def get_predictions_from_text(text):\n",
    "#     feature_text, pn_history = get_feature_text(text)\n",
    "#     return get_predictions(feature_text, pn_history)\n",
    "\n",
    "# def get_predictions_from_text_list(text_list):\n",
    "#     feature_text, pn_history = get_feature_text(text_list)\n",
    "#     return get_predictions(feature_text, pn_history)\n",
    "\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(config.tokenizer_path)#['tokenizer_path']) # config.TOKENIZER_PATH\n",
    "path ='model.pth'\n",
    "\n",
    "device = torch.device(\"cuda\")\n",
    "model = NBMEModel()\n",
    "#model.to(device)#(torch.device)#['device'])\n",
    "model.load_state_dict(torch.load(path, map_location=torch.device('cpu')))#torch.device(config.device)))#['model'])\n",
    "model.to(device)#(torch.device)#['device'])\n",
    "model.eval()\n",
    "\n",
    "input_text = create_sample_test()\n",
    "feature_text = input_text.feature_text[0].lower()\n",
    "pn_history = input_text.pn_history[0].lower()\n",
    "get_predictions(feature_text, pn_history)  \n",
    "#tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "#model = AutoModel.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Family history of thyroid disorder\n",
      "HPI: 17yo M presents with palpitations. Patient reports 3-4 months of intermittent episodes of \"heart beating/pounding out of my chest.\" 2 days ago during a soccer game had an episode, but this time had chest pressure and felt as if he were going to pass out (did not lose conciousness). Of note patient endorses abusing adderall, primarily to study (1-3 times per week). Before recent soccer game, took adderrall night before and morning of game. Denies shortness of breath, diaphoresis, fevers, chills, headache, fatigue, changes in sleep, changes in vision/hearing, abdominal paun, changes in bowel or urinary habits. \n",
      "PMHx: none\n",
      "Rx: uses friends adderrall\n",
      "FHx: mom with \"thyroid disease,\" dad with recent heart attcak\n",
      "All: none\n",
      "Immunizations: up to date\n",
      "SHx: Freshmen in college. Endorses 3-4 drinks 3 nights / week (on weekends), denies tabacco, endorses trying marijuana. Sexually active with girlfriend x 1 year, uses condoms\n"
     ]
    }
   ],
   "source": [
    "input_text = create_sample_test()\n",
    "feature_text = input_text.feature_text[0]\n",
    "print(feature_text)\n",
    "pn_history = input_text.pn_history[0]\n",
    "print(pn_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "# #similarly this can be done for all 5 models\n",
    "model1 = NBMEModel(conf=model_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
