{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# unsupervised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import multiprocessing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "from transformers import BertForMaskedLM, DistilBertForMaskedLM\n",
    "from transformers import BertTokenizer, DistilBertTokenizer\n",
    "from transformers import RobertaTokenizer, RobertaForMaskedLM\n",
    "from transformers import DataCollatorForLanguageModeling\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "import os\n",
    "\n",
    "#from constants import *\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    "    AutoConfig,\n",
    "    AutoModelForMaskedLM,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "# RAW DATA NLP\n",
    "NLP_RAW_PATH_DIR = os.path.join(\"data\", \"raw\", \"nlp\")\n",
    "MTSAMPLES_RAW_PATH_DIR = os.path.join(NLP_RAW_PATH_DIR, \"mtsamples\")\n",
    "PATIENT_NOTES_RAW_PATH_DIR = os.path.join(NLP_RAW_PATH_DIR, \"patient_notes\")\n",
    "\n",
    "# PROCESSED DATA NLP\n",
    "NLP_PROCESSED_PATH_DIR = os.path.join(\"data\", \"processed\", \"nlp\")\n",
    "MTSAMPLES_PROCESSED_PATH_DIR = os.path.join(NLP_PROCESSED_PATH_DIR, \"mtsamples\")\n",
    "PATIENT_NOTES_PROCESSED_PATH_DIR = os.path.join(NLP_PROCESSED_PATH_DIR, \"patient_notes\")\n",
    "\n",
    "# MODEL_SEMI_SUPERVISED\n",
    "MODEL_UNSUPERVISED_NAME = \"emilyalsentzer/Bio_ClinicalBERT\"\n",
    "MODEL_UNSUPERVISED_CHECKPOINTS_DIR = os.path.join(\n",
    "    \"models\", \"nlp\", \"unsupervised\", \"checkpoints\"\n",
    ")\n",
    "MODEL_UNSUPERVISED_MODEL_DIR = os.path.join(\"models\", \"nlp\", \"unsupervised\", \"model\")\n",
    "\n",
    "# MODEL_UNSUPERVISED\n",
    "\n",
    "\n",
    "# configs\n",
    "# HYPERPARAMS\n",
    "SEED_SPLIT = 0\n",
    "SEED_TRAIN = 0\n",
    "\n",
    "MAX_SEQ_LEN = 128\n",
    "TRAIN_BATCH_SIZE = 8\n",
    "EVAL_BATCH_SIZE = 8\n",
    "LEARNING_RATE = 2e-5\n",
    "LR_WARMUP_STEPS = 100\n",
    "WEIGHT_DECAY = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_path):\n",
    "\n",
    "    dtf_mlm = pd.read_csv(data_path)\n",
    "    dtf_mlm = dtf_mlm.head(100)\n",
    "    # Train/Valid Split\n",
    "    df_train, df_valid = train_test_split(\n",
    "        dtf_mlm, test_size=0.15, random_state=SEED_SPLIT\n",
    "    )\n",
    "    # Convert to Dataset object\n",
    "    dataset_train = Dataset.from_pandas(df_train[[\"transcription\"]].dropna())\n",
    "    dataset_val = Dataset.from_pandas(df_valid[[\"transcription\"]].dropna())\n",
    "    return dataset_train, dataset_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer():\n",
    "    # \"bert-base-cased\"\n",
    "    TokenizerClass = BertTokenizer\n",
    "    bert_type = \"bert-base-cased\"\n",
    "\n",
    "    tokenizer = TokenizerClass.from_pretrained(\n",
    "        bert_type,\n",
    "        model_max_length=512,  # MAX_SEQ_LEN\n",
    "        truncate=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "    )  # autotokenizer\n",
    "\n",
    "    return tokenizer\n",
    "\n",
    "def tokenize_function(batch, tokenizer):  # before row\n",
    "    return tokenizer(\n",
    "        batch[\"transcription\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,  # MAX_SEQ_LEN,\n",
    "        return_special_tokens_mask=True,\n",
    "    )\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    column_names = dataset.column_names\n",
    "\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function,\n",
    "        batched=True,\n",
    "        num_proc=multiprocessing.cpu_count(),\n",
    "        remove_columns=column_names,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer}  # ,\n",
    "        # batched=True,\n",
    "    )\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c86fa10354d4dccb0b4eca7eb3839e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb88dca59483467dabf28945af219e0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "deec73793a5f42788e2cb39b12b2b909",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b2e917d55a44aa3862fac42452e3c6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f80c5c1e27a34713bddfeb84560d65c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#0:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4bc9bbd4da6748f28f64f209fc77f554",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#1:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c8c0dc7623d54cd781e53dd6fb29728f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#2:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " "
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b267886479964a4f91b3ad62df652645",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "#3:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds, val_ds = load_datasets(\n",
    "        os.path.join(MTSAMPLES_PROCESSED_PATH_DIR, \"mtsamples_cleaned.csv\")\n",
    "    )\n",
    "\n",
    "tokenizer = load_tokenizer()\n",
    "tokenized_train_ds = tokenize_dataset(train_ds, tokenizer)\n",
    "tokenized_val_ds = tokenize_dataset(val_ds, tokenizer)\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "device = get_device()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#import metrics\n",
    "import wandb\n",
    "#initalize wandb also print in notebook\n",
    "wandb.init(project=\"nlp\", entity=\"michael\", name=\"test\", reinit=True)\n",
    "\n",
    "#from transformers import metric\n",
    "from datasets import metric, load_metric\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "def load_training_args(output_dir):\n",
    "    # steps_per_epoch = int(len(train_dataset) / TRAIN_BATCH_SIZE)\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        num_train_epochs=30,\n",
    "        do_train=True,\n",
    "        do_eval=True,\n",
    "        per_device_train_batch_size=TRAIN_BATCH_SIZE,\n",
    "        per_device_eval_batch_size=EVAL_BATCH_SIZE,\n",
    "        warmup_steps=LR_WARMUP_STEPS,\n",
    "        # save_steps=steps_per_epoch,\n",
    "        save_total_limit=3,\n",
    "        weight_decay=WEIGHT_DECAY,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"loss\",\n",
    "        greater_is_better=False,\n",
    "        seed=SEED_TRAIN,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def load_trainer(model, training_args, train_ds, val_ds, tokenizer):\n",
    "    data_collator = DataCollatorForLanguageModeling(\n",
    "        tokenizer=tokenizer, mlm=True, mlm_probability=0.15\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        data_collator=data_collator,  # masks the tokens\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics,\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def load_model(device):\n",
    "    ModelClass = BertForMaskedLM\n",
    "    bert_type = \"bert-base-cased\"\n",
    "    model = ModelClass.from_pretrained(bert_type).to(device)\n",
    "    return model\n",
    "\n",
    "\n",
    "model = load_model(device)\n",
    "training_args = load_training_args(MODEL_UNSUPERVISED_CHECKPOINTS_DIR)\n",
    "trainer = load_trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        tokenized_train_ds,\n",
    "        tokenized_val_ds,\n",
    "        tokenizer, compute_metrics\n",
    ")\n",
    "\n",
    "# last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "# if last_checkpoint is None:\n",
    "#     resume_from_checkpoint = None\n",
    "# else:\n",
    "#     resume_from_checkpoint = True\n",
    "\n",
    "trainer.train()#resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    # evaluate\n",
    "import math\n",
    "\n",
    "eval_results = trainer.evaluate()\n",
    "print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n",
    "\n",
    "    #trainer.save_model(MODEL_UNSUPERVISED_MODEL_DIR)\n",
    "    # torch.cuda.empty_cache()\n",
    "    #trainer.save_state()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = 'this text is a test, i am sick my dog is sick'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No sentence-transformers model found with name models/nlp/semi_supervised/model. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at models/nlp/semi_supervised/model were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from keybert import KeyBERT\n",
    "keybert = KeyBERT('models/nlp/semi_supervised/model')\n",
    "keywords = keybert.extract_keywords(\n",
    "    text,\n",
    "    keyphrase_ngram_range=(1, 2),\n",
    "    stop_words=\"english\",\n",
    "    use_maxsum=True,\n",
    "    nr_candidates=10,\n",
    "    top_n=5,\n",
    "    use_mmr=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/nlp/semi_supervised/model were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('test sick', 0.6507),\n",
       " ('sick dog', 0.6467),\n",
       " ('sick', 0.5757),\n",
       " ('text test', 0.5491),\n",
       " ('text', 0.3682)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import transformers\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "hf_model = pipeline('feature-extraction', model='models/nlp/semi_supervised/model')\n",
    "kw_model = KeyBERT(model=hf_model)\n",
    "keywords = kw_model.extract_keywords(\n",
    "    text,\n",
    "    keyphrase_ngram_range=(1, 2),\n",
    "    stop_words=\"english\",\n",
    "    use_maxsum=True,\n",
    "    nr_candidates=10,\n",
    "    top_n=5,\n",
    "    use_mmr=True,\n",
    ")\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##imports\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, load_metric, Dataset\n",
    "# from src/nlp/constants import (\n",
    "#     MTSAMPLES_PROCESSED_PATH_DIR,\n",
    "#     MODEL_SEMI_SUPERVISED_NAME,\n",
    "#     MODEL_SEMI_SUPERVISED_CHECKPOINTS_DIR,\n",
    "#     MODEL_SEMI_SUPERVISED_MODEL_DIR,\n",
    "# )\n",
    "from transformers import (\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoTokenizer,\n",
    ")\n",
    "\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "import wandb\n",
    "\n",
    "#wandb.init(project=\"nlp\", entity=\"nlp_masterthesis\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## sentence classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# map medical specialty to labels\n",
    "def map_medical_specialty_to_labels(path):\n",
    "    df = pd.read_csv(path)\n",
    "    dict_medical_specialty = {\n",
    "        value: idx for idx, value in enumerate(df.medical_specialty.unique())\n",
    "    }\n",
    "    df[\"labels\"] = df.medical_specialty.map(dict_medical_specialty)\n",
    "    return df\n",
    "\n",
    "\n",
    "def load_datasets(data_path):\n",
    "    dataset = Dataset.from_pandas(map_medical_specialty_to_labels(data_path))\n",
    "    dataset_train_test = dataset.train_test_split(test_size=0.1)\n",
    "    # train dataset\n",
    "    dataset_train_val = dataset_train_test[\"train\"].train_test_split(test_size=0.1)\n",
    "    dataset_train = dataset_train_val[\"train\"]\n",
    "    # validation dataset\n",
    "    dataset_val = dataset_train_val[\"test\"]\n",
    "\n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "\n",
    "def tokenize_function(batch, tokenizer):\n",
    "    return tokenizer(\n",
    "        batch[\"transcription\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "\n",
    "def tokenize_dataset(dataset, tokenizer):\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer},\n",
    "        batched=True,\n",
    "    )\n",
    "    return tokenized_datasets\n",
    "\n",
    "\n",
    "def clean_remove_column(tokenized_dataset):\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "        [\n",
    "            \"Unnamed: 0\",\n",
    "            \"description\",\n",
    "            \"medical_specialty\",\n",
    "            \"sample_name\",\n",
    "            \"transcription\",\n",
    "            \"keywords\",\n",
    "            \"keywords_list\",\n",
    "            \"location\",\n",
    "        ]\n",
    "    )\n",
    "    # tokenized_dataset = tokenized_dataset.rename_column(\"labels_val\", \"labels\")\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    metric = load_metric(\"accuracy\", average=\"macro\")\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "\n",
    "def get_device() -> torch.device:\n",
    "    return torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "def load_model(device):\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_SEMI_SUPERVISED_NAME, num_labels=39\n",
    "    ).to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_tokenizer():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(MODEL_SEMI_SUPERVISED_NAME, model_max_length=512\n",
    "                                              truncate=True, max_length=512, padding=True)\n",
    "                                        \n",
    "    return tokenizer\n",
    "\n",
    "\n",
    "def load_training_args(output_dir):\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        learning_rate=2e-5,\n",
    "        num_train_epochs=30,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        warmup_steps=500,\n",
    "        weight_decay=0.01,\n",
    "        # logging_dir=\"./logs\",\n",
    "        # logging_steps=10,\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        report_to=\"wandb\",\n",
    "    )\n",
    "    return training_args\n",
    "\n",
    "\n",
    "def load_trainer(model, training_args, train_ds, val_ds, tokenizer):\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_ds,\n",
    "        eval_dataset=val_ds,\n",
    "        compute_metrics=compute_metrics,\n",
    "        tokenizer=tokenizer,\n",
    "        # callbacks=[EarlyStoppingCallback()],\n",
    "    )\n",
    "    return trainer\n",
    "\n",
    "\n",
    "def main():\n",
    "    train_ds, val_ds = load_datasets(\n",
    "        os.path.join(MTSAMPLES_PROCESSED_PATH_DIR, \"mtsamples_cleaned.csv\")\n",
    "    )\n",
    "\n",
    "    tokenizer = load_tokenizer()\n",
    "    tokenized_train_ds = tokenize_dataset(train_ds, tokenizer)\n",
    "    tokenized_val_ds = tokenize_dataset(val_ds, tokenizer)\n",
    "\n",
    "    tokenized_train_ds = clean_remove_column(tokenized_train_ds)\n",
    "    tokenized_val_ds = clean_remove_column(tokenized_val_ds)\n",
    "\n",
    "    device = get_device()\n",
    "    model = load_model(device)\n",
    "    training_args = load_training_args(MODEL_SEMI_SUPERVISED_CHECKPOINTS_DIR)\n",
    "    trainer = load_trainer(\n",
    "        model,\n",
    "        training_args,\n",
    "        tokenized_train_ds,\n",
    "        tokenized_val_ds,\n",
    "        tokenizer,\n",
    "    )\n",
    "\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None:\n",
    "        resume_from_checkpoint = None\n",
    "    else:\n",
    "        resume_from_checkpoint = True\n",
    "\n",
    "    trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "\n",
    "    trainer.save_model(MODEL_SEMI_SUPERVISED_MODEL_DIR)\n",
    "    trainer.save_state()\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([22])\n"
     ]
    }
   ],
   "source": [
    "# load the trained model and tokenizer and test it on a sample text\n",
    "def load_model_and_tokenizer():\n",
    "    model = AutoModelForSequenceClassification.from_pretrained('models/nlp/semi_supervised/model')\n",
    "    #     MODEL_SEMI_SUPERVISED_MODEL_DIR\n",
    "    # )\n",
    "    tokenizer = AutoTokenizer.from_pretrained('models/nlp/semi_supervised/model')\n",
    "    return model, tokenizer\n",
    "\n",
    "#test the model on a sample text\n",
    "def test_model(model, tokenizer):\n",
    "    text = \"I have a headache\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model(**inputs)\n",
    "    logits = outputs.logits\n",
    "    predicted_class_idx = logits.argmax(-1)\n",
    "    print(predicted_class_idx)\n",
    "    \n",
    "model, tokenizer = load_model_and_tokenizer()\n",
    "test_model(model, tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test without keybert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"HuggingFace is a company based in New York,  he is sick , headach but is also has employees working in Paris\"\n",
    "#tokenizer = config.TOKENIZER,config.MAX_LEN\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"emilyalsentzer/Bio_ClinicalBERT\")\n",
    "tokenized = tokenizer.encode_plus(sentence, max_length=512, pad_to_max_length=True,return_offsets_mapping=True, return_token_type_ids=True) #return_tensors=\"pt\"\n",
    "input_ids = torch.tensor([tokenized[\"input_ids\"]]).to(DEVICE)\n",
    "attention_mask = torch.tensor([tokenized[\"attention_mask\"]]).to(DEVICE)\n",
    "token_type_ids = torch.tensor([tokenized[\"token_type_ids\"]]).to(DEVICE)\n",
    "offsets = torch.tensor([tokenized[\"offset_mapping\"]]).to(DEVICE)\n",
    "print(offsets)\n",
    "\n",
    "model_config = transformers.BertConfig.from_pretrained(config.BERT_PATH)\n",
    "model_config.output_hidden_states = True\n",
    "\n",
    "# #similarly this can be done for all 5 models\n",
    "model = NBMEModel(conf=model_config)\n",
    "model.load_state_dict(torch.load(\"../models/model_fold1.bin\",  map_location=torch.device('cpu')))\n",
    "\n",
    "model.to(DEVICE)\n",
    "  \n",
    "with torch.no_grad():\n",
    "    logits = model(ids=input_ids, mask=attention_mask, token_type_ids=token_type_ids) #last_hidden_state\n",
    "\n",
    "def get_predictions(logits):\n",
    "    preds = torch.sigmoid(logits).cpu().detach().numpy()\n",
    "    preds = np.where(preds > 0.5, 1, 0)\n",
    "    return preds\n",
    "\n",
    "def get_prediction_locations(preds, offsets):\n",
    "    locations = []\n",
    "    for pred, offset in zip(preds, offsets):\n",
    "        for i in range(len(pred)):\n",
    "            if pred[i] == 1:\n",
    "                locations.append(offset[i])\n",
    "    return locations\n",
    "\n",
    "def get_prediction_keywords(preds, offsets, sentence):\n",
    "    keywords = []\n",
    "    for pred, offset in zip(preds, offsets):\n",
    "        #print(len(pred))\n",
    "        for i in range(len(pred)):\n",
    "            #print(pred[i])\n",
    "            if pred[i] == 1:\n",
    "                #print('yes')\n",
    "                keywords.append(sentence[offset[i][0]:offset[i][1]])\n",
    "    return keywords\n",
    "\n",
    "\n",
    "preds = get_predictions(logits)\n",
    "#offsets = tokenized[\"offset_mapping\"]\n",
    "keyword = get_prediction_keywords(preds, offsets, sentence)\n",
    "\n",
    "def get_labels(preds):\n",
    "    labels = []\n",
    "    for pred in preds:\n",
    "        labels.append(np.where(pred == 1)[0])\n",
    "    return labels\n",
    "\n",
    "\n",
    "labels = get_labels(preds)\n",
    "print(sentence)\n",
    "print(keyword)\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keyword Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "from keybert import KeyBERT\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "def KeywordExtraction(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"models/nlp/semi_supervised/model\", model_max_lenght=512\n",
    "    )\n",
    "\n",
    "    # truncate all the text to 512 tokens\n",
    "\n",
    "    hf_model = pipeline(\n",
    "        \"feature-extraction\",\n",
    "        model=\"models/nlp/semi_supervised/model\",\n",
    "        tokenizer=tokenizer,  # \"models/nlp/semi_supervised/model\",\n",
    "    )\n",
    "\n",
    "    kw_model = KeyBERT(model=hf_model)\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,\n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        use_maxsum=True,\n",
    "        nr_candidates=20,\n",
    "        top_n=15,\n",
    "        use_mmr=True,\n",
    "        diversity=0.5,\n",
    "    )\n",
    "    return keywords\n",
    "\n",
    "\n",
    "def apply_keyword_on_Dataframe(df):\n",
    "    df[\"keywords_outcome_weights\"] = df[\"transcription\"].apply(\n",
    "        lambda x: KeywordExtraction(x)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keybert import KeyBERT\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# nltk.download(\"stopwords\")\n",
    "# nltk.download(\"punkt\")\n",
    "\n",
    "# nltk.download(\"wordnet\")\n",
    "# nltk.download(\"omw-1.4\")\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import pandas as pd\n",
    "import transformers\n",
    "from transformers import pipeline\n",
    "\n",
    "\n",
    "from keyphrase_vectorizers import KeyphraseCountVectorizer\n",
    "\n",
    "#kw_model.extract_keywords(docs=docs, vectorizer=KeyphraseCountVectorizer())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fine tune sentence transformer model\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import pandas as pd\n",
    "\n",
    "#load the model\n",
    "model = SentenceTransformer('models/nlp/semi_supervised/model')\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KeywordExtraction(text):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        \"models/nlp/semi_supervised/model\", model_max_lenght=512\n",
    "    )\n",
    "\n",
    "    # truncate all the text to 512 tokens\n",
    "\n",
    "    hf_model = pipeline(\n",
    "        \"feature-extraction\",\n",
    "        model=\"models/nlp/semi_supervised/model\",\n",
    "        tokenizer=tokenizer, # truncation# \"models/nlp/semi_supervised/model\",\n",
    "    )\n",
    "\n",
    "    kw_model = KeyBERT(model=hf_model)\n",
    "    keywords = kw_model.extract_keywords(\n",
    "        text,#vectorizer=KeyphraseCountVectorizer(), \n",
    "        keyphrase_ngram_range=(1, 2),\n",
    "        stop_words=\"english\",\n",
    "        use_maxsum=True,\n",
    "        nr_candidates=20,\n",
    "        top_n=15,\n",
    "        use_mmr=True,\n",
    "        diversity=0.5,\n",
    "    )\n",
    "    return keywords\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# make df column smaller than 512\n",
    "def small_column_df(df):\n",
    "    df = df[df[\"transcription\"].str.len() < 512]\n",
    "    df = df.head(20)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def apply_keyword_on_Dataframe(df):\n",
    "    df[\"keywords_outcome_weights\"] = df[\"transcription\"].apply(\n",
    "        lambda x: KeywordExtraction(x)\n",
    "    )\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_1 = pd.read_csv(\"data/processed/nlp/mtsamples/mtsamples_cleaned.csv\")\n",
    "df = small_column_df(df_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>description</th>\n",
       "      <th>medical_specialty</th>\n",
       "      <th>sample_name</th>\n",
       "      <th>transcription</th>\n",
       "      <th>keywords</th>\n",
       "      <th>keywords_list</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2-D M-Mode. Doppler.</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 1</td>\n",
       "      <td>mmode leave atrial enlargement leave atrial di...</td>\n",
       "      <td>cardiovascular / pulmonary, 2-d m-mode, dopple...</td>\n",
       "      <td>['cardiovascular / pulmonary', ' 2-d m-mode', ...</td>\n",
       "      <td>dict_values([[221, 233], [11, 29], [163, 181],...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>Echocardiogram and Doppler</td>\n",
       "      <td>Cardiovascular / Pulmonary</td>\n",
       "      <td>2-D Echocardiogram - 4</td>\n",
       "      <td>description normal cardiac chamber size normal...</td>\n",
       "      <td>cardiovascular / pulmonary, ejection fraction,...</td>\n",
       "      <td>['cardiovascular / pulmonary', ' ejection frac...</td>\n",
       "      <td>dict_values([[97, 114], [76, 96], [282, 295], ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  description            medical_specialty  \\\n",
       "3           3       2-D M-Mode. Doppler.     Cardiovascular / Pulmonary   \n",
       "9           9   Echocardiogram and Doppler   Cardiovascular / Pulmonary   \n",
       "\n",
       "                sample_name  \\\n",
       "3   2-D Echocardiogram - 1    \n",
       "9   2-D Echocardiogram - 4    \n",
       "\n",
       "                                       transcription  \\\n",
       "3  mmode leave atrial enlargement leave atrial di...   \n",
       "9  description normal cardiac chamber size normal...   \n",
       "\n",
       "                                            keywords  \\\n",
       "3  cardiovascular / pulmonary, 2-d m-mode, dopple...   \n",
       "9  cardiovascular / pulmonary, ejection fraction,...   \n",
       "\n",
       "                                       keywords_list  \\\n",
       "3  ['cardiovascular / pulmonary', ' 2-d m-mode', ...   \n",
       "9  ['cardiovascular / pulmonary', ' ejection frac...   \n",
       "\n",
       "                                            location  \n",
       "3  dict_values([[221, 233], [11, 29], [163, 181],...  \n",
       "9  dict_values([[97, 114], [76, 96], [282, 295], ...  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at models/nlp/semi_supervised/model were not used when initializing BertModel: ['classifier.bias', 'classifier.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m df_f \u001b[39m=\u001b[39m apply_keyword_on_Dataframe(df)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m df_f\u001b[39m.\u001b[39mhead(\u001b[39m2\u001b[39m)\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb Cell 24\u001b[0m in \u001b[0;36mapply_keyword_on_Dataframe\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_keyword_on_Dataframe\u001b[39m(df):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39;49m\u001b[39mtranscription\u001b[39;49m\u001b[39m\"\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         \u001b[39mlambda\u001b[39;49;00m x: KeywordExtraction(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/series.py:4433\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4323\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4324\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4325\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4328\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4329\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4330\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4331\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4332\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4431\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4432\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4433\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:1088\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1084\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mf, \u001b[39mstr\u001b[39m):\n\u001b[1;32m   1085\u001b[0m     \u001b[39m# if we are a string, try to dispatch\u001b[39;00m\n\u001b[1;32m   1086\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[0;32m-> 1088\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/core/apply.py:1143\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1137\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[1;32m   1138\u001b[0m         \u001b[39m# error: Argument 2 to \"map_infer\" has incompatible type\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[39m# \"Union[Callable[..., Any], str, List[Union[Callable[..., Any], str]],\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m         \u001b[39m# Dict[Hashable, Union[Union[Callable[..., Any], str],\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m         \u001b[39m# List[Union[Callable[..., Any], str]]]]]\"; expected\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m         \u001b[39m# \"Callable[[Any], Any]\"\u001b[39;00m\n\u001b[0;32m-> 1143\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1144\u001b[0m             values,\n\u001b[1;32m   1145\u001b[0m             f,  \u001b[39m# type: ignore[arg-type]\u001b[39;49;00m\n\u001b[1;32m   1146\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1147\u001b[0m         )\n\u001b[1;32m   1149\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1150\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1151\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1152\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2870\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb Cell 24\u001b[0m in \u001b[0;36mapply_keyword_on_Dataframe.<locals>.<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=38'>39</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_keyword_on_Dataframe\u001b[39m(df):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=39'>40</a>\u001b[0m     df[\u001b[39m\"\u001b[39m\u001b[39mkeywords_outcome_weights\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m\"\u001b[39m\u001b[39mtranscription\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mapply(\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=40'>41</a>\u001b[0m         \u001b[39mlambda\u001b[39;00m x: KeywordExtraction(x)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=41'>42</a>\u001b[0m     )\n",
      "\u001b[1;32m/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb Cell 24\u001b[0m in \u001b[0;36mKeywordExtraction\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m hf_model \u001b[39m=\u001b[39m pipeline(\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mfeature-extraction\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     model\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mmodels/nlp/semi_supervised/model\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     tokenizer\u001b[39m=\u001b[39mtokenizer, \u001b[39m# truncation# \"models/nlp/semi_supervised/model\",\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m kw_model \u001b[39m=\u001b[39m KeyBERT(model\u001b[39m=\u001b[39mhf_model)\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m keywords \u001b[39m=\u001b[39m kw_model\u001b[39m.\u001b[39;49mextract_keywords(\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m     text,\u001b[39m#vectorizer=KeyphraseCountVectorizer(), \u001b[39;49;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m     keyphrase_ngram_range\u001b[39m=\u001b[39;49m(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m),\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m     stop_words\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39menglish\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m     use_maxsum\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m     nr_candidates\u001b[39m=\u001b[39;49m\u001b[39m20\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     top_n\u001b[39m=\u001b[39;49m\u001b[39m15\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     use_mmr\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     diversity\u001b[39m=\u001b[39;49m\u001b[39m0.5\u001b[39;49m,\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m )\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/test.ipynb#X35sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39mreturn\u001b[39;00m keywords\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/keybert/_model.py:155\u001b[0m, in \u001b[0;36mKeyBERT.extract_keywords\u001b[0;34m(self, docs, candidates, keyphrase_ngram_range, stop_words, top_n, min_df, use_maxsum, use_mmr, diversity, nr_candidates, vectorizer, highlight, seed_keywords)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[39m# Extract embeddings\u001b[39;00m\n\u001b[1;32m    154\u001b[0m doc_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39membed(docs)\n\u001b[0;32m--> 155\u001b[0m word_embeddings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49membed(words)\n\u001b[1;32m    157\u001b[0m \u001b[39m# Find keywords\u001b[39;00m\n\u001b[1;32m    158\u001b[0m all_keywords \u001b[39m=\u001b[39m []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/keybert/backend/_hftransformers.py:59\u001b[0m, in \u001b[0;36mHFTransformerBackend.embed\u001b[0;34m(self, documents, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m dataset \u001b[39m=\u001b[39m MyDataset(documents)\n\u001b[1;32m     58\u001b[0m embeddings \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 59\u001b[0m \u001b[39mfor\u001b[39;00m document, features \u001b[39min\u001b[39;00m tqdm(\n\u001b[1;32m     60\u001b[0m     \u001b[39mzip\u001b[39m(\n\u001b[1;32m     61\u001b[0m         documents, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membedding_model(dataset, truncation\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, padding\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     62\u001b[0m     ),\n\u001b[1;32m     63\u001b[0m     total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(dataset),\n\u001b[1;32m     64\u001b[0m     disable\u001b[39m=\u001b[39m\u001b[39mnot\u001b[39;00m verbose,\n\u001b[1;32m     65\u001b[0m ):\n\u001b[1;32m     66\u001b[0m     embeddings\u001b[39m.\u001b[39mappend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_embed(document, features))\n\u001b[1;32m     68\u001b[0m \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39marray(embeddings)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/tqdm/std.py:1183\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1180\u001b[0m \u001b[39m# If the bar is disabled, then just walk the iterable\u001b[39;00m\n\u001b[1;32m   1181\u001b[0m \u001b[39m# (note: keep this check outside the loop for performance)\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdisable:\n\u001b[0;32m-> 1183\u001b[0m     \u001b[39mfor\u001b[39;00m obj \u001b[39min\u001b[39;00m iterable:\n\u001b[1;32m   1184\u001b[0m         \u001b[39myield\u001b[39;00m obj\n\u001b[1;32m   1185\u001b[0m     \u001b[39mreturn\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:111\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_item()\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miterator)\n\u001b[1;32m    112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39minfer(item, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/pipelines/pt_utils.py:112\u001b[0m, in \u001b[0;36mPipelineIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m# We're out of items within a batch\u001b[39;00m\n\u001b[1;32m    111\u001b[0m item \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39miterator)\n\u001b[0;32m--> 112\u001b[0m processed \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49minfer(item, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparams)\n\u001b[1;32m    113\u001b[0m \u001b[39m# We now have a batch of \"inferred things\".\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloader_batch_size \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    115\u001b[0m     \u001b[39m# Try to infer the size of the batch\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/pipelines/base.py:983\u001b[0m, in \u001b[0;36mPipeline.forward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m    981\u001b[0m     \u001b[39mwith\u001b[39;00m inference_context():\n\u001b[1;32m    982\u001b[0m         model_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_inputs, device\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m--> 983\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_forward(model_inputs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mforward_params)\n\u001b[1;32m    984\u001b[0m         model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_ensure_tensor_on_device(model_outputs, device\u001b[39m=\u001b[39mtorch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m    985\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/pipelines/feature_extraction.py:59\u001b[0m, in \u001b[0;36mFeatureExtractionPipeline._forward\u001b[0;34m(self, model_inputs)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_forward\u001b[39m(\u001b[39mself\u001b[39m, model_inputs):\n\u001b[0;32m---> 59\u001b[0m     model_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mmodel_inputs)\n\u001b[1;32m     60\u001b[0m     \u001b[39mreturn\u001b[39;00m model_outputs\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1018\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1009\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m   1011\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[1;32m   1012\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[1;32m   1013\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1016\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[1;32m   1017\u001b[0m )\n\u001b[0;32m-> 1018\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[1;32m   1019\u001b[0m     embedding_output,\n\u001b[1;32m   1020\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[1;32m   1021\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   1022\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   1023\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[1;32m   1024\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   1025\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   1026\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   1027\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   1028\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   1029\u001b[0m )\n\u001b[1;32m   1030\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   1031\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:607\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    598\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m    599\u001b[0m         create_custom_forward(layer_module),\n\u001b[1;32m    600\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    604\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    605\u001b[0m     )\n\u001b[1;32m    606\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 607\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[1;32m    608\u001b[0m         hidden_states,\n\u001b[1;32m    609\u001b[0m         attention_mask,\n\u001b[1;32m    610\u001b[0m         layer_head_mask,\n\u001b[1;32m    611\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    612\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    613\u001b[0m         past_key_value,\n\u001b[1;32m    614\u001b[0m         output_attentions,\n\u001b[1;32m    615\u001b[0m     )\n\u001b[1;32m    617\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    618\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:493\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    482\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    483\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[1;32m    491\u001b[0m     \u001b[39m# decoder uni-directional self-attention cached key/values tuple is at positions 1,2\u001b[39;00m\n\u001b[1;32m    492\u001b[0m     self_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[:\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     self_attention_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mattention(\n\u001b[1;32m    494\u001b[0m         hidden_states,\n\u001b[1;32m    495\u001b[0m         attention_mask,\n\u001b[1;32m    496\u001b[0m         head_mask,\n\u001b[1;32m    497\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    498\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mself_attn_past_key_value,\n\u001b[1;32m    499\u001b[0m     )\n\u001b[1;32m    500\u001b[0m     attention_output \u001b[39m=\u001b[39m self_attention_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m    502\u001b[0m     \u001b[39m# if decoder, the last output is tuple of self-attn cache\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:423\u001b[0m, in \u001b[0;36mBertAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\n\u001b[1;32m    414\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    415\u001b[0m     hidden_states: torch\u001b[39m.\u001b[39mTensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    421\u001b[0m     output_attentions: Optional[\u001b[39mbool\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m,\n\u001b[1;32m    422\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tuple[torch\u001b[39m.\u001b[39mTensor]:\n\u001b[0;32m--> 423\u001b[0m     self_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mself(\n\u001b[1;32m    424\u001b[0m         hidden_states,\n\u001b[1;32m    425\u001b[0m         attention_mask,\n\u001b[1;32m    426\u001b[0m         head_mask,\n\u001b[1;32m    427\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    428\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    429\u001b[0m         past_key_value,\n\u001b[1;32m    430\u001b[0m         output_attentions,\n\u001b[1;32m    431\u001b[0m     )\n\u001b[1;32m    432\u001b[0m     attention_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput(self_outputs[\u001b[39m0\u001b[39m], hidden_states)\n\u001b[1;32m    433\u001b[0m     outputs \u001b[39m=\u001b[39m (attention_output,) \u001b[39m+\u001b[39m self_outputs[\u001b[39m1\u001b[39m:]  \u001b[39m# add attentions if we output them\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:311\u001b[0m, in \u001b[0;36mBertSelfAttention.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    309\u001b[0m     value_layer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mcat([past_key_value[\u001b[39m1\u001b[39m], value_layer], dim\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n\u001b[1;32m    310\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 311\u001b[0m     key_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkey(hidden_states))\n\u001b[1;32m    312\u001b[0m     value_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mvalue(hidden_states))\n\u001b[1;32m    314\u001b[0m query_layer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtranspose_for_scores(mixed_query_layer)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m/opt/anaconda3/envs/nlp_masterthesis/lib/python3.10/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "df_f = apply_keyword_on_Dataframe(df)\n",
    "df_f.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://maartengr.github.io/KeyBERT/guides/countvectorizer.html#custom-backend"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
