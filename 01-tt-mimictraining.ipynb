{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TEXT</th>\n",
       "      <th>specialty</th>\n",
       "      <th>TEXT_final</th>\n",
       "      <th>TEXT_final_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Admission Date:  [**2141-9-18**]              ...</td>\n",
       "      <td>Cardiothoracic &amp; Vascular</td>\n",
       "      <td>:\\nhip pain\\n\\n:\\n24yo woman with hx SLE, CKD(...</td>\n",
       "      <td>hip pain woman hx SLE CKD currently HD PD labi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Admission Date:  [**2187-9-19**]              ...</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>:\\ns/p Motor cycle crash; left sided rib pain\\...</td>\n",
       "      <td>Motor cycle crash left sided rib pain driver h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Admission Date:  [**2190-6-5**]       Discharg...</td>\n",
       "      <td>Emergency Department</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Admission Date:  [**2101-4-30**]              ...</td>\n",
       "      <td>Infectious Disease Specialty</td>\n",
       "      <td>:\\nOSH transfer for sepsis\\n\\n:\\n75 y/o M with...</td>\n",
       "      <td>sepsis hx type DM ESRD failed renal tx HD mont...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Admission Date:  [**2146-9-15**]              ...</td>\n",
       "      <td>Cardiothoracic &amp; Vascular</td>\n",
       "      <td>:\\nChest Pain, Abdominal Pain, Nausea/Vomiting...</td>\n",
       "      <td>Chest Pain Abdominal Pain Nausea Vomiting Righ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                TEXT  \\\n",
       "0  Admission Date:  [**2141-9-18**]              ...   \n",
       "1  Admission Date:  [**2187-9-19**]              ...   \n",
       "2  Admission Date:  [**2190-6-5**]       Discharg...   \n",
       "3  Admission Date:  [**2101-4-30**]              ...   \n",
       "4  Admission Date:  [**2146-9-15**]              ...   \n",
       "\n",
       "                      specialty  \\\n",
       "0     Cardiothoracic & Vascular   \n",
       "1          Emergency Department   \n",
       "2          Emergency Department   \n",
       "3  Infectious Disease Specialty   \n",
       "4     Cardiothoracic & Vascular   \n",
       "\n",
       "                                          TEXT_final  \\\n",
       "0  :\\nhip pain\\n\\n:\\n24yo woman with hx SLE, CKD(...   \n",
       "1  :\\ns/p Motor cycle crash; left sided rib pain\\...   \n",
       "2                                                NaN   \n",
       "3  :\\nOSH transfer for sepsis\\n\\n:\\n75 y/o M with...   \n",
       "4  :\\nChest Pain, Abdominal Pain, Nausea/Vomiting...   \n",
       "\n",
       "                                  TEXT_final_cleaned  \n",
       "0  hip pain woman hx SLE CKD currently HD PD labi...  \n",
       "1  Motor cycle crash left sided rib pain driver h...  \n",
       "2                                                NaN  \n",
       "3  sepsis hx type DM ESRD failed renal tx HD mont...  \n",
       "4  Chest Pain Abdominal Pain Nausea Vomiting Righ...  "
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('data/processed/mimic_iii/diagnoses_noteevents_cleaned.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.specialty.nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7p/lw9128713_9433llvvhnlv680000gn/T/ipykernel_1359/2299056610.py:2: FutureWarning: Passing a negative integer is deprecated in version 1.0 and will not be supported in future version. Instead, use None to not limit the column width.\n",
      "  pd.set_option('display.max_colwidth', -1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>version https://git-lfs.github.com/spec/v1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>oid sha256:e58cfd03306f03486a5966363015ccc74c876450818e43a86192d46b978d0bf8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>size 619203815</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    version https://git-lfs.github.com/spec/v1\n",
       "0  oid sha256:e58cfd03306f03486a5966363015ccc74c876450818e43a86192d46b978d0bf8\n",
       "1  size 619203815                                                             "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#show full text\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-11-20 15:40:32.076165: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtaratumbraegel\u001b[0m (\u001b[33mnlp_masterthesis\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.13.5 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.13.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/tara-sophiatumbraegel/UNI/masterthesis/NLP_Masterthesis/wandb/run-20221120_154100-1c34n7dg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/nlp_masterthesis/nlp/runs/1c34n7dg\" target=\"_blank\">super-pyramid-48</a></strong> to <a href=\"https://wandb.ai/nlp_masterthesis/nlp\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import torch\n",
    "import wandb\n",
    "from datasets import Dataset\n",
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "\n",
    "from src.nlp.constants import (\n",
    "    MODEL_BASE_NAME,\n",
    "    MODEL_TC_CHECKPOINTS_DIR,\n",
    "    MODEL_TC_DIR,\n",
    "    MTSAMPLES_PROCESSED_PATH_DIR,\n",
    ")\n",
    "from src.nlp.utils import (\n",
    "    get_device,\n",
    "    load_tokenizer,\n",
    "    load_trainer,\n",
    "    load_training_args,\n",
    "    tokenize_function,\n",
    ")\n",
    "from src.nlp.text_classification_model_training import load_datasets, tokenize_dataset\n",
    "\n",
    "# wandb.init(\n",
    "#     project=\"nlp\",\n",
    "#     entity=\"nlp_masterthesis\",\n",
    "#     tags=[\"textclassification mimic\"],\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_medical_specialty_to_labels(path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Read the csv file\n",
    "    Map the medical specialty to labels\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    path : str\n",
    "        Path to the csv file\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    pd.DataFrame\n",
    "        Dataframe with the mapped labels\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(path)\n",
    "    #drop the rows with missing values\n",
    "    df = df.dropna()\n",
    "    dict_medical_specialty = {\n",
    "        value: idx for idx, value in enumerate(df.specialty.unique())\n",
    "    }\n",
    "    df[\"labels\"] = df.specialty.map(dict_medical_specialty)\n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_datasets(data_path: str) -> tuple[Dataset, Dataset]:\n",
    "    \"\"\"\n",
    "    Load the train and validation datasets\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    data_path : str\n",
    "        Path to the dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    tuple[Dataset, Dataset]\n",
    "        train and validation datasets\n",
    "    \"\"\"\n",
    "    dataset = Dataset.from_pandas(map_medical_specialty_to_labels(data_path))\n",
    "    dataset_train_test = dataset.train_test_split(test_size=0.1)\n",
    "    # train dataset\n",
    "    dataset_train_val = dataset_train_test[\"train\"].train_test_split(test_size=0.1)\n",
    "    dataset_train = dataset_train_val[\"train\"]\n",
    "    # validation dataset\n",
    "    dataset_val = dataset_train_val[\"test\"]\n",
    "\n",
    "    return dataset_train, dataset_val\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds, val_ds = load_datasets(\n",
    "        os.path.join(\"data/processed/mimic_iii/diagnoses_noteevents_cleaned.csv\")\n",
    "    )\n",
    "\n",
    "#use small sample for testing\n",
    "train_ds = train_ds.select(range(10))\n",
    "val_ds = val_ds.select(range(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tokenizer() -> AutoTokenizer:\n",
    "    \"\"\"\n",
    "    Load tokenizer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AutoTokenizer\n",
    "        Tokenizer\n",
    "    \"\"\"\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        pretrained_model_name_or_path=MODEL_BASE_NAME,\n",
    "        model_max_length=512,\n",
    "        truncate=True,\n",
    "        max_length=512,\n",
    "        padding=True,\n",
    "    )\n",
    "\n",
    "    return tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets.arrow_dataset import Batch\n",
    "def tokenize_function(\n",
    "    batch: Batch, tokenizer: AutoTokenizer, special_token: bool\n",
    ") -> Batch:\n",
    "    \"\"\"\n",
    "    Tokenize the input batch\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    batch : Batch\n",
    "        Batch to tokenize\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer to use\n",
    "    special_token : bool\n",
    "        Whether to add special tokens\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Batch\n",
    "        Tokenized batch\n",
    "    \"\"\"\n",
    "    # spcial_token = false for Text classification\n",
    "    return tokenizer(\n",
    "        batch[\"TEXT_final_cleaned\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_special_tokens_mask=False,\n",
    "        #special_token,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_dataset(dataset: Dataset, tokenizer: AutoTokenizer) -> Dataset:\n",
    "    \"\"\"\n",
    "    Tokenize the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    dataset : Dataset\n",
    "        Dataset to tokenize\n",
    "    tokenizer : AutoTokenizer\n",
    "        Tokenizer\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataset\n",
    "        Tokenized dataset\n",
    "    \"\"\"\n",
    "    # tokenize_dataset = dataset.map(\n",
    "    #     lambda batch: tokenize_function(batch, tokenizer, False),\n",
    "    #     batched=True,\n",
    "    #     remove_columns=['TEXT', 'specialty', 'TEXT_final'],\n",
    "    # )\n",
    "    tokenized_datasets = dataset.map(\n",
    "        tokenize_function,\n",
    "        fn_kwargs={\"tokenizer\": tokenizer, \"special_token\": False},\n",
    "        batched=True,\n",
    "    )\n",
    "    return tokenized_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "tokenizer = load_tokenizer()\n",
    "#tokenized_train_ds = tokenize_dataset(train_ds, tokenizer)\n",
    "#tokenized_val_ds = tokenize_dataset(val_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['TEXT', 'specialty', 'TEXT_final', 'TEXT_final_cleaned', 'labels'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "639f727498964ebc90da5f8332b77fe3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcc416ede51345319baba15850d2e72a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_train_ds = tokenize_dataset(train_ds, tokenizer)\n",
    "tokenized_val_ds = tokenize_dataset(val_ds, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_remove_column(tokenized_dataset: Dataset) -> Dataset:\n",
    "    \"\"\"\n",
    "    Remove all unneded columns from the dataset\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    tokenized_dataset : Dataset\n",
    "        Tokenized dataset\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    Dataset\n",
    "        Dataset with only the needed columns\n",
    "    \"\"\"\n",
    "    print(tokenized_dataset.column_names)\n",
    "    tokenized_dataset = tokenized_dataset.remove_columns(\n",
    "        [\n",
    "  'TEXT', 'specialty', 'TEXT_final'\n",
    "        ]\n",
    "    )\n",
    "    # tokenized_dataset = tokenized_dataset.rename_column(\n",
    "    #     \"labels_val\", \"labels\"\n",
    "    # )\n",
    "    tokenized_dataset.set_format(\"torch\")\n",
    "    return tokenized_dataset\n",
    "\n",
    "\n",
    "def load_model(\n",
    "    device: torch.device,\n",
    ") -> AutoModelForSequenceClassification:\n",
    "    \"\"\"\n",
    "    Load sequence classification model\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    device : torch.device\n",
    "        Device\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    AutoModelForSequenceClassification\n",
    "        Sequence classification model\n",
    "    \"\"\"\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        MODEL_BASE_NAME, num_labels=16\n",
    "    ).to(device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def main() -> None:\n",
    "    \"\"\"\n",
    "    Main function\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds, val_ds = load_datasets(\n",
    "        os.path.join(\"../data/processed/mimic_iii/diagnoses_noteevents_cleaned.csv\")\n",
    "    )\n",
    "\n",
    "    tokenizer = load_tokenizer()\n",
    "    tokenized_train_ds = tokenize_dataset(train_ds, tokenizer)\n",
    "    tokenized_val_ds = tokenize_dataset(val_ds, tokenizer)\n",
    "    # only use sample size for testing\n",
    "    tokenized_train_ds = tokenized_train_ds.select(range(10))\n",
    "    tokenized_val_ds = tokenized_val_ds.select(range(10))\n",
    "\n",
    "    tokenized_train_ds = clean_remove_column(tokenized_train_ds)\n",
    "    tokenized_val_ds = clean_remove_column(tokenized_val_ds)\n",
    "\n",
    "    # device = get_device()\n",
    "    # model = load_model(device)\n",
    "    # training_args = load_training_args(MODEL_TC_CHECKPOINTS_DIR)\n",
    "    # trainer = load_trainer(\n",
    "    #     model,\n",
    "    #     training_args,\n",
    "    #     tokenized_train_ds,\n",
    "    #     tokenized_val_ds,\n",
    "    #     tokenizer,\n",
    "    #     modeltype=\"sequence_classification\",\n",
    "    # )\n",
    "\n",
    "    # last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    # if last_checkpoint is None:\n",
    "    #     resume_from_checkpoint = None\n",
    "    # else:\n",
    "    #     resume_from_checkpoint = True\n",
    "\n",
    "    # trainer.train(resume_from_checkpoint=resume_from_checkpoint)\n",
    "    # trainer.save_model(MODEL_TC_DIR)\n",
    "    # trainer.save_state()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e008ba518fc94ce1407a9eb93057d27eeafd2dad53a3852a13fd11c85bd39236"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
