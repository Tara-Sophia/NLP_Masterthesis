{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is the wav2vec 2.0 model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The wav2vec 2.0 model was released by Facebook AI Research in February 2021. It is a self-supervised model for speech recognition that pre-trains a convolutional neural network on unlabeled audio data. The model is trained to predict the next token in a sequence of feature vectors extracted from the audio. The feature vectors are obtained from a CNN-based feature extractor. The model architecture is based on the Transformer, a popular sequence-to-sequence model for NLP tasks. Citations: [1](https://arxiv.org/abs/2006.11477), [2](https://arxiv.org/abs/2010.11474)\n",
    "\n",
    "The wav2vec 2.0 model has the following key features:\n",
    "1. Self-supervised pre-training\n",
    "2. No forced alignment of transcripts required\n",
    "3. No speaker information required\n",
    "4. Can be fine-tuned for downstream tasks with limited data\n",
    "citation: [1](https://arxiv.org/abs/2006.11477)\n",
    "\n",
    "The model is trained on unlabeled audio data and can be fine-tuned for downstream tasks with limited data. The model can be fine-tuned for ASR, speaker recognition, and other speech recognition tasks. The model can be fine-tuned for ASR using the Connectionist Temporal Classification (CTC) loss. The model can be fine-tuned for speaker recognition using the ArcFace loss. The model can be fine-tuned for other speech recognition tasks using the cross entropy loss.\n",
    "\n",
    "\n",
    "The wav2vec 2.0 model has the following layers:\n",
    "1. Feature encoder\n",
    "2. Contextualized projection layer\n",
    "3. Quantization layer\n",
    "\n",
    "\n",
    "The feature encoder is a CNN-based feature extractor. The feature vectors are obtained from the output of the feature encoder. In more detail the feature encoder's task is to extract features from the audio. The feature encoder consists of a stack of convolutional layers. The feature encoder extracts features from the audio by sliding a window over the audio. The window size is 0.02 seconds and the window stride is 0.01 seconds. The feature encoder uses 32-dimensional feature vectors. The feature encoder uses 10 convolutional layers. The feature encoder uses a stride of 2 for the first 5 convolutional layers and a stride of 1 for the last 5 convolutional layers. The feature encoder uses a kernel size of 2 for the first 5 convolutional layers and a kernel size of 3 for the last 5 convolutional layers. The feature encoder uses a ReLU activation function. \n",
    "Citation: [1](https://arxiv.org/abs/2006.11477) \n",
    "\n",
    "\n",
    "The contextualized projection layer is a linear layer that projects the feature vectors to a vector of the same size as the vocabulary size. The quantization layer is a vector quantization layer that maps the feature vectors to the vocabulary. The model is trained to predict the next token in a sequence of feature vectors extracted from the audio. The feature vectors are obtained from a CNN-based feature extractor. The model architecture is based on the Transformer, a popular sequence-to-sequence model for NLP tasks. Citations: [1](https://arxiv.org/abs/2006.11477), [2](https://arxiv.org/abs/2010.11474)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CNN-based feature extractor consists of the following layers:\n",
    "1. Convolutional layer\n",
    "2. Group normalisation\n",
    "3. Weighted layer-tied gated activation unit (GLU)\n",
    "4. Residual connection\n",
    "5. Dropout\n",
    "6. Layer normalisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the architecture of the Wav2vec 2.0 model and how it works. This notebook is based on the paper \"wav2vec 2.0: A Framework for Self-Supervised Learning of Speech Representations\" by Alexei Baevski, Henry Zhou, Abdelrahman Mohamed, Michael Auli, and Sergey Edunov. The paper can be found here: https://arxiv.org/abs/2006.11477\n",
    "\n",
    "The architecture of the Wav2vec 2.0 model is based on the Transformer, a popular sequence-to-sequence model for NLP tasks. The model architecture is shown below. The model consists of the following layers:\n",
    "1. Feature extractor - a CNN-based feature extractor that extracts feature vectors from raw audio\n",
    "2. Feature projection - a linear layer that projects the feature vectors to a lower dimension\n",
    "3. Wav2VecEncoder - a TransformerEncoder that converts the feature vectors to a sequence of embeddings\n",
    "\n",
    "//\n",
    " \n",
    "4. Wav2Vec2ForCTC - a CTC head that predicts the output sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In detail the model architecture is as follows:\n",
    "1. Feature extractor - a CNN-based feature extractor that extracts feature vectors from raw audio\n",
    "This means that the model extracts feature vectors from raw audio. The feature vectors are obtained from a CNN-based feature extractor. The CNN-based feature extractor consists of the following layers:\n",
    "\n",
    "2. Feature projection - a linear layer that projects the feature vectors to a lower dimension\n",
    "3. Wav2VecEncoder - a TransformerEncoder that converts the feature vectors to a sequence of embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the model work?\n",
    "1. The model extracts feature vectors from raw audio\n",
    "2. The model projects the feature vectors to a lower dimension. The model uses a linear layer to project the feature vectors to a lower dimension.\n",
    "3. The model converts the feature vectors to a sequence of embeddings. The model uses a TransformerEncoder to convert the feature vectors to a sequence of embeddings.\n",
    "4. The model predicts the output sequence. The model uses a CTC head to predict the output sequence.\n",
    "\n",
    "Scientific References:\n",
    "1. https://arxiv.org/abs/2006.11477\n",
    "4. https://arxiv.org/abs/1706.03762\n",
    "5. https://arxiv.org/abs/1801.06146\n",
    "6. https://arxiv.org/abs/1904.11660\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('nlp_masterthesis')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "40268d9a6ead8a84520c08d0b901e046bd2176336d854cc732bdbdbf7245879e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
